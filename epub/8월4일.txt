로지스틱 회귀분석

자연어 처리 프로젝트(2차 프로젝트)




probs=np.arange(0.0, 1.0, 0.01) 
		start stop step
print(len(probs))
print(probs)
odds=[p/(1-p) for p in probs]
print(odds)
plt.plot(probs, odds)
plt.xlabel('p')
plt.ylabel('p/(1-p)')
plt.show()



import mglearn 
import matplotlib.pyplot as plt 
X,y=mglearn.datasets.make_forge() 
print(X)
print(y)
mglearn.discrete_scatter(X[:,0],X[:,1],y) #산점도
plt.legend(['class 0', 'class 1'],loc=4) #범례
plt.xlabel('class 0')
plt.ylabel('class 1')
print(X.shape)




from sklearn.linear_model import LogisticRegression
fig,axes=plt.subplots(1,1,figsize=(5,3))
model=LogisticRegression()
model=model.fit(X,y)
mglearn.plots.plot_2d_separator(model, X, fill=True, eps=0.5, ax=axes, alpha=0.7)
mglearn.discrete_scatter(X[:,0],X[:,1],y,ax=axes)
axes.set_title('logit')
axes.set_xlabel('class 0')
axes.set_ylabel('class 1')
axes.legend()




from sklearn.metrics import confusion_matrix 
pred=model.predict(X)
print(pred) #출력값
print(y) #실제값
confusion_matrix(y,pred)



from sklearn.linear_model import LogisticRegression
model=LogisticRegression(C=1000, random_state=0)
			오차에 대한 패널티
model.fit(X_train,y_train)
print(model.score(X_train,y_train))
print(model.score(X_test,y_test))
pred=model.predict(X_test) 
print((y_test != pred).sum()) #오분류 샘플수





from sklearn.metrics import precision_score, recall_score, f1_score 
from sklearn.metrics import accuracy_score 
print(accuracy_score(y_test,pred)) #정확도
print(precision_score(y_test,pred)) #모형의 정확도
print(recall_score(y_test,pred)) #실제현상 설명
print(f1_score(y_test,pred))




# TN(True Negative) : 0을 0으로 맞춘 경우
# FP(False Positive) : 0인데 1로 틀린 경우
# FN(False Negative) : 1인데 0으로 틀린 경우
# TP(True Positive) : 1을 1로 맞춘 경우
# ravel() 다차원배열을 1차원배열로 변환
tn,fp,fn,tp=confusion_matrix(y, pred).ravel()
print(tn,fp,fn,tp)

          0  1
0 array([[2, 0],
1        [1, 3]], dtype=int64)


y=   [0,1,0,0]
pred=[0,1,1,1]
precision_score(y,pred) 모형의 정확도
#모형이 1로 출력한 3개 중 맞는 것 1개 1/3 





y=   [0,1,1,0]
pred=[0,1,0,0]
recall_score(y,pred)
#실제값 1 2개 중 1개만 찾음 50%

              precision    recall  f1-score   support

           0       0.33      1.00      0.50         1
           1       1.00      0.33      0.50         3

    accuracy                           0.50         4
   macro avg       0.67      0.67      0.50         4
weighted avg       0.83      0.50      0.50         4








costs=[0.001, 0.01, 0.1, 1, 10, 100, 1000]
results=[]
for c in costs:
    params={'C':c, 'max_iter':1000}
    model=LogisticRegression(**params).fit(X_train,y_train) 
    score=model.score(X_test,y_test)
    results.append(score)

print(results)    












import matplotlib.pyplot as plt 

m=max(results) 
for i,a in enumerate(results):
   인덱스,값
    if a==m:
        n=i 
        break 
print('최고 정확도:',m)    
print('최적의 C:',costs[n])
plt.rcParams['font.size']=15 
plt.plot(range(len(costs)),results)
plt.xlabel('Cost')
plt.ylabel('Accuracy')




df2=df.drop(df.index[range(40)])
df2.columns 

	0	1	2
	50	50	50
	10	50	50



from sklearn.linear_model import LogisticRegression
logit1=LogisticRegression(random_state=0)
	0 1 2
	10 50 50
logit2=LogisticRegression(random_state=0, class_weight='balanced')
						샘플비율 고려
logit3=LogisticRegression(random_state=0, class_weight={0:3, 1:1, 2:2})
	10 50 50
	30 50 100
						직접 지정


	L1	오차의 절대값
	L2	오차의 제곱











from sklearn.linear_model import LogisticRegression
models=[
    LogisticRegression(random_state=0, penalty='l1', solver='liblinear'),
    LogisticRegression(random_state=0, penalty='l2', max_iter=1000),
    LogisticRegression(random_state=0, penalty='elasticnet', solver='saga', l1_ratio=1, max_iter=5000),
    LogisticRegression(random_state=0, penalty=None)
]
for logit in models:
    print(logit)
    logit.fit(X_train, y_train)
    print(logit.score(X_train,y_train))
    print(logit.score(X_test,y_test))
    print()

LogisticRegression(penalty='l1', random_state=0, solver='liblinear')
0.9583333333333334
0.9666666666666667

LogisticRegression(max_iter=1000, random_state=0)
0.9666666666666667
1.0


X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)




import pandas as pd 
from sklearn.datasets import make_classification
X,y=make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)

		0 99%
		1 1%

dfX=pd.DataFrame(X, columns=['a','b'])
dfy=pd.DataFrame(y, columns=['y'])
df=pd.concat([dfX,dfy],axis=1)
df 



              precision    recall  f1-score   support

           0       0.88      0.91      0.89      1000
           1       0.90      0.87      0.89      1000

    accuracy                           0.89      2000
   macro avg       0.89      0.89      0.89      2000
weighted avg       0.89      0.89      0.89      2000




              precision    recall  f1-score   support

           0       1.00      1.00      1.00      1975
           1       1.00      0.60      0.75        20

    accuracy                           1.00      1995
   macro avg       1.00      0.80      0.87      1995
weighted avg       1.00      1.00      1.00      1995










	0	1
	9900	100

	100	100


토멕링크(Tomek's link) : 서로 다른 클래스에 속하는 한쌍의 데이터


		o
		 x


              precision    recall  f1-score   support

           0       0.95      0.90      0.92        20
           1       0.90      0.95      0.93        20

    accuracy                           0.93        40
   macro avg       0.93      0.93      0.92        40
weighted avg       0.93      0.93      0.92        40




from imblearn.under_sampling import TomekLinks 
X_sample,y_sample=TomekLinks(sampling_strategy='majority').fit_resample(X,y)
X_samp=pd.DataFrame(data=X_sample, columns=['a','b'])
y_samp=pd.DataFrame(data=y_sample, columns=['y'])
y_samp.y.value_counts()


y
0    9874
1     100












	[model1, model2, model3]

	model1

	model2

	model3

	
             precision    recall  f1-score   support

           0       0.89      0.91      0.90      1980
           1       0.91      0.88      0.90      1980

    accuracy                           0.90      3960
   macro avg       0.90      0.90      0.90      3960
weighted avg       0.90      0.90      0.90      3960


              precision    recall  f1-score   support

           0       0.90      0.92      0.91      1980
           1       0.92      0.89      0.91      1980

    accuracy                           0.91      3960
   macro avg       0.91      0.91      0.91      3960
weighted avg       0.91      0.91      0.91      3960

RandomOverSampler
SMOTE


numeric_features = ['age', 'fare']

numeric_transformer = Pipeline(steps=[

imputer = SimpleImputer(strategy="mean")
				mean / median / most_frequent
				평균    중위수     최빈수

    ('imputer', SimpleImputer(strategy='median')),
				mean	정규분포
				median	비정규분포
				최빈수	범주형

    ('scaler', StandardScaler())])


categorical_features = ['embarked', 'sex', 'pclass']
						1/2/3

categorical_transformer = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='most_frequent')),
					범주형 - 최빈수

    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
					원핫인코딩
preprocessor = ColumnTransformer(

    transformers=[

        ('num', numeric_transformer, numeric_features),

        ('cat', categorical_transformer, categorical_features)])

# 전처리 후 로지스틱 회귀분석 분류모형

clf = Pipeline(steps=[('preprocessor', preprocessor),

                      ('classifier', LogisticRegression())])


from sklearn.model_selection import GridSearchCV
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean', 'median'],

변수명__num__imputer__strategy


    'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100],
}
grid_search = GridSearchCV(clf, param_grid, cv=10)
						교차검증횟수
grid_search.fit(X, y)
grid_search.cv_results_["params"]













