SVM

자연어 처리 프로젝트(2차 프로젝트)
	주제 선정
	데이터 수집, 전처리
	기계학습 모형, 평가
	응용프로그램에 배포
	발표 준비

	알고리즘(아이디어)
	컴퓨팅 파워
	데이터


from sklearn.svm import SVC

model = SVC(random_state=10)
		C
		gamma
		kernel='linear'		선형
		kernel='rbf'		비선형(기본)

model.fit(X_train, y_train)







from sklearn.preprocessing import StandardScaler
ss = StandardScaler()	평균 0, 표준편차 1
ss.fit(X_train)
X_train_scaled = pd.DataFrame(ss.transform(X_train))
X_test_scaled = pd.DataFrame(ss.transform(X_test))











import numpy as np
train_rate = []
test_rate = []
c_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]
g_values = [0.0001, 0.001, 0.01, 0.1]
for n in c_values:
    for g in g_values:
        # 모델 생성
        model = SVC(C=n,gamma=g,random_state=10)
        model.fit(X_train_scaled, y_train)
        # 학습용 데이터셋의 정확도
        train_rate.append([n,g,model.score(X_train_scaled, y_train)])
        # 검증용 데이터셋의 정확도
        test_rate.append([n,g,model.score(X_test_scaled, y_test)])
        
train_arr=np.array(train_rate) #검증용 데이터셋의 정확도
test_arr=np.array(test_rate) #검증용 데이터셋의 정확도
max_rate=np.max(test_arr[:,2]) #가장 좋은 정확도
idx=np.where(test_arr[:,2] == max_rate)[0][0] #가장 성능이 좋은 인덱스
print("최적의 c:",test_rate[idx][0])
print("최적의 gamma:",test_rate[idx][1])
print("최적의 정확도:",test_rate[idx][2])   

최적의 c: 1
최적의 gamma: 0.1
최적의 정확도: 0.8620689655172413

model = SVC(C=1,gamma=0.1,random_state=10)
model.fit(X_train_scaled, y_train)
print("학습용:",model.score(X_train_scaled, y_train))
print("검증용:",model.score(X_test_scaled, y_test))




	  0    1
0 array([[13,  1],
1       [ 3, 12]], dtype=int64)


svm = SVC()

n_estimators = 10

model = BaggingClassifier(base_estimator=svm,
					모형
                          n_estimators=n_estimators,
					분류기수
                          max_samples=1/n_estimators)
				bootstrap 부트스트랩



X=df[df.columns[:-1]]
y=df['Churn']
y.value_counts()

c:\python\lib\site-packages\sklearn\ensemble\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.




import warnings
warnings.filterwarnings("ignore")

import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")

costs = np.logspace(-10, 3, 14) # 10의 -10승부터 10의 3승까지 14개
scores = []
for c in costs:
    model.base_estimator.C = c
    #교차검증
    cv=StratifiedKFold(n_splits=5, random_state=10, shuffle=True)
    this_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')
    print(this_scores)
    score=np.max(this_scores)
    print(c,'==>',score)
    scores.append(score)

print('max:',max(scores))    
print('idx:',np.argmax(scores))
print('C:',costs[np.argmax(scores)])    

max: 0.8911917098445595
idx: 13
C: 1000.0




	100

	50	50
0	30	20
1	20	30


	50	50
0	25	25
1	25	25







import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=10)
			fold

	150

	30	30	30	30	30
0	10
1	10
2	10

i=1
train_scores=[]
test_scores=[]
for train_index, test_index in skf.split(X,y):
    print(i,'fold')
    X_train, X_test = X.iloc[train_index,], X.iloc[test_index,]
			숫자 인덱스
			[행, 열]
    y_train, y_test = y[train_index], y[test_index]
    model = SVC(random_state=10)
    model.fit(X_train, y_train)
    score = model.score(X_train, y_train)
    print(score)
    train_scores.append(score)
    score = model.score(X_test, y_test)
    print(score)
    test_scores.append(score)
    i+=1
    
print('\naverage')    
print(np.mean(train_scores))
print(np.mean(test_scores))
print('\nmax')   
print(np.max(test_scores))






import warnings
warnings.filterwarnings("ignore")
from sklearn.neighbors import KNeighborsClassifier

kf = StratifiedKFold(5, shuffle=True, random_state=0)
k_range = list(range(1, 11))
means_tr = []
means_te = []
for k in k_range:
    train_scores = []
    test_scores = []
    knn = KNeighborsClassifier(n_neighbors=k)
    for train, test in kf.split(X,y):
        X_train, X_test, y_train, y_test = X.iloc[train,], X.iloc[test,], y[train], y[test]
        knn.fit(X_train, y_train)
        score = knn.score(X_train, y_train)
        train_scores.append(score)
        score = knn.score(X_test, y_test)
        test_scores.append(score)
        #print('k: %d, test score : %f' % (k, score))
    mean_tr=np.mean(train_scores)
    mean_te=np.mean(test_scores)
    print("K : %d, train score's mean %f" % (k, mean_tr))
    print("K : %d, test score's mean %f" % (k, mean_te))
    means_tr.append(mean_tr)
    means_te.append(mean_te)
    
max_score=max(means_te)    
print('최고 정확도:',max_score)
idx=np.where(means_te==max_score)[0][0]
print('최적의 k:',k_range[idx])


from sklearn.model_selection import cross_val_score
svm = SVC(random_state=0)
skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=10)
#교차검증을 자동으로 수행하는 함수
result=cross_val_score(svm, X, y, cv=skf)
print(result)
print(np.mean(result)*100)
print(np.max(result)*100)


svm = SVC(random_state=0).fit(X, y)

#모형을 만드는데 사용된 변수들의 특성 중요도 - 트리 알고리즘에 더욱 적합한 방법

result = permutation_importance(svm, X, y, n_repeats=10, random_state=0,n_jobs=-1)

#rf = RandomForestClassifier(random_state=0).fit(X, y)

#result = permutation_importance(rf, X, y, n_repeats=10, random_state=0,n_jobs=-1)

importances=np.array(list(zip(X.columns, result.importances_mean)))





from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from sklearn.svm import SVC
import numpy as np
svm = SVC(random_state=0).fit(X, y)
result1 = permutation_importance(svm, X, y, n_repeats=10, random_state=0,n_jobs=-1)
	  순서대로 나열  중요도      모형         횟수                          코어수 100%


importances1=np.array(list(zip(X.columns, result1.importances_mean)))

rf = RandomForestClassifier(random_state=0).fit(X, y)
result2 = permutation_importance(rf, X, y, n_repeats=10, random_state=0,n_jobs=-1)
importances2=np.array(list(zip(X.columns, result2.importances_mean)))



from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
model = DecisionTreeClassifier(random_state=0).fit(X, y)
# Recursive Feature Elimination with Cross Validation
# 재귀적     특성      제거             교차  검증
# step=1 각 단계에서 제거할 변수의 개수
rfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(5),
                  분류기         변수제거           교차검증 5회
              scoring='accuracy',min_features_to_select=1)
		성능측정기준
rfecv.fit(X, y)



import numpy as np
from sklearn.svm import SVC
from sklearn.model_selection import validation_curve
#10의 -6승부터 3승까지 10개의 값
param_range = np.logspace(-6, 3, 10)
train_scores, test_scores = validation_curve(
    SVC(), X, y, param_name="C", param_range=param_range,scoring="accuracy", n_jobs=-1)
train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
#최고 정확도
print(np.max(test_scores_mean))
#최적의 cost
print(param_range[np.argmax(test_scores_mean)])


plt.figure(figsize=(13, 13))
plt.xlabel('C')
plt.ylabel("score")
plt.ylim(0.4, 1.1)
plt.semilogx(param_range, train_scores_mean, label="Train")
plt.semilogx(param_range, test_scores_mean, label="Test")
# plt.plot(param_range, train_scores_mean, label="Train")
# plt.plot(param_range, test_scores_mean, label="Test")
plt.legend(loc="best")
plt.show()


