SVM

자연어 처리 프로젝트(2차 프로젝트)










	kernel='linear'	선형모델
	       'rbf'	비선형모델



from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score 
import numpy as np 
import mglearn
learn_data=np.array([[0,0],[1,0],[0,1],[1,1]])
learn_label=np.array([0,0,0,1])
mglearn.discrete_scatter(learn_data[:,0],learn_data[:,1],learn_label)



svm=LinearSVC()
svm.fit(learn_data, learn_label)
X_test=np.array([[0,0],[1,0],[0,1],[1,1]])
pred=svm.predict(X_test)
print(pred)
print(accuracy_score([0,0,0,1],pred))




from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import numpy as np 
import mglearn
X_train = np.array([[0,0], [1,0], [0,1], [1,1]])
y_train = [0, 1, 1, 0]  
mglearn.discrete_scatter(X_train[:,0],X_train[:,1], y_train)


	xor exclusive or
		배타적 
	
	0	0	0
	1	0	1
	0	1	1
	1	1	0




svm = SVC(kernel='linear')
#svm = SVC(kernel='rbf')
svm.fit(X_train, y_train)

X_test = np.array([[0,0], [1,0], [0,1], [1,1]])
pred = svm.predict(X_test)

print(X_test , "의 예측 결과: " ,  pred)
print("정답률 = " , accuracy_score([0, 1, 1, 0], pred))  




[[0 0]
 [1 0]
 [0 1]
 [1 1]] 의 예측 결과:  [1 1 1 1]
정답률 =  0.5

from sklearn.svm import SVC
#선형 svm 모형
#model = SVC(kernel = 'linear')
model = SVC() # rbf
	SVC(kernel='rbf')
model.fit(X, y)





import matplotlib.pyplot as plt
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
# X 데이터는 어느 클래스로 분류될까요?
plt.plot([4.5], [3.5], 'x', color='red', markersize=20)
plt.colorbar()
print(model.predict([[4.5, 3.5]]))
#서포트 벡터 샘플
print(model.support_vectors_)
# 서포트 벡터의 인덱스
model.support_



params = {'C': [0.1, 1, 10, 100], 'gamma': [0.00001, 0.001, 0.01, 0.1, 1, 10]}

	오차에 대한 패널티			샘플의 영향력
	
grid = GridSearchCV(SVC(), params)

grid.fit(X_train, y_train)


	SVC()





from sklearn.datasets import make_blobs 
X,y=make_blobs(n_samples=100, centers=2, cluster_std=0.6, random_state=0)
		샘플수		군집수		표준편차		





from sklearn.svm import SVC
model=SVC(kernel='linear').fit(X_train,y_train) #선형
#model=SVC(kernel='rbf').fit(X_train,y_train) #비선형(기본값)


from sklearn.model_selection import GridSearchCV
params={'C':[0.1, 1, 10, 100], 'gamma':[0.00001, 0.001, 0.01, 0.1, 1, 10]}
		패널티			샘플의 영향력
grid=GridSearchCV(SVC(kernel='linear'), params)
			


#grid=GridSearchCV(SVC(kernel='rbf'), params)
grid.fit(X_train,y_train)
print(grid.best_params_) #최적의 파라미터
print(grid.best_estimator_) #최적의 모형


{'C': 0.1, 'gamma': 1e-05}
		       
SVC(C=0.1, gamma=1e-05, kernel='linear')



40명의 흑백 얼굴 사진 400장, 4096 (64x64) 특성

		0 0 0 0 0
		0 0 0 0 0 



	0~255
	black	white

	0.0 ~ 1.0











import matplotlib.pyplot as plt
import numpy as np
N = 2 #2행
M = 5 #5열
#np.random.seed(0)
fig = plt.figure(figsize=(9, 5))
#컬러맵: https://matplotlib.org/tutorials/colors/colormaps.html
klist = np.random.choice(range(len(faces.data)), N * M)
				range(400) => 0~399
		choice(range(400), 10)
for i in range(N):
    for j in range(M):
        k = klist[i * M + j]
        ax = fig.add_subplot(N, M, i * M + j + 1)
        ax.imshow(faces.images[k], cmap=plt.cm.gray)
        ax.xaxis.set_ticks([]) #x축 눈금 제거
        ax.yaxis.set_ticks([]) #y축 눈금 제거
        plt.title(faces.target[k])
plt.tight_layout() #여백 조절
plt.show()




N = 2
M = 5
#np.random.seed(4)
fig = plt.figure(figsize=(9, 5))
klist = np.random.choice(range(len(y_test)), N * M)
for i in range(N):
    for j in range(M):
        k = klist[i * M + j]
        ax = fig.add_subplot(N, M, i * M + j + 1)
        ax.imshow(X_test[k:(k + 1), :].reshape(64, 64), cmap=plt.cm.gray)
        ax.xaxis.set_ticks([])
        ax.yaxis.set_ticks([])
        pred=svc.predict(X_test[k:(k + 1), :])[0]
        plt.title(f"{y_test[k]} => {pred}")
        
plt.tight_layout()
plt.show()




400
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39}
(4096,)
(400, 4096)

5243008
5.0001220703125

학습용: 1.0
검증용: 0.95

              precision    recall  f1-score   support

           0       0.67      1.00      0.80         2
           1       1.00      1.00      1.00         2
           2       0.67      1.00      0.80         2
           3       1.00      1.00      1.00         2
           4       1.00      0.50      0.67         2
           5       1.00      1.00      1.00         2
           6       1.00      1.00      1.00         2
           7       1.00      0.50      0.67         2



from sklearn.decomposition import PCA 
import matplotlib.pyplot as plt 

pca=PCA(n_components=2, random_state=10)
	주성분의 수
components=pca.fit_transform(faces.data) 
#print(components)
df=pd.DataFrame(data=components, columns=['component 1','component 2'])
df.index=faces.target
#plt.figure(figsize=(20,15))
for i in range(3,6):
#for i in range(40):
    plt.scatter(df.iloc[faces.target == i, 0], df.iloc[faces.target == i, 1],  label=f'face {i}')
    plt.legend()
plt.show()


for i in range(40):
    plt.scatter(df.iloc[faces.target == i, 0], df.iloc[faces.target == i, 1],  label=f'face {i}')
    plt.legend()
plt.show()















