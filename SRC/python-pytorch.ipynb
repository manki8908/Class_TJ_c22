{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch ( facebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Tensor\n",
    "* 다차원 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 학습시킬때는 tensor로 사용 ( cpu, gpu 사용 )\n",
    "# predict할때는 다시 numpy \n",
    "\n",
    "li = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "#넘파이배열을 텐서로 변환\n",
    "\n",
    "tensor1 = torch.tensor(li)\n",
    "\n",
    "tensor2 = torch.as_tensor(li)\n",
    "\n",
    "tensor3 = torch.from_numpy(li)\n",
    "\n",
    "print(tensor1)\n",
    "\n",
    "print(tensor2)\n",
    "\n",
    "print(tensor3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "#텐서를 넘파이배열로 변환\n",
    "\n",
    "print(tensor1.numpy())\n",
    "\n",
    "print(tensor2.numpy())\n",
    "\n",
    "print(tensor3.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4963, 0.7682, 0.0885, 0.1320, 0.3074])\n",
      "tensor([ 0.5507,  0.2704,  0.6472,  0.2490, -0.3354])\n",
      "tensor([8, 4, 3, 6, 9])\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# torch 난수 생성\n",
    "\n",
    "torch.manual_seed(0) #랜덤시드 고정\n",
    "\n",
    "a = torch.rand(5) # 0 ~ 1 사이의 5개의 난수\n",
    "\n",
    "b = torch.randn(5) # 평균 0, 표준편차 1인 5개의 난수\n",
    "\n",
    "c = torch.randint(10, size=(5,)) # 0~9 사이의 5개의 난수\n",
    "\n",
    "print(a)\n",
    "\n",
    "print(b)\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "print(torch.arange(1, 10)) # 1~9\n",
    "\n",
    "print(torch.ones((2, 5))) # 2행 5열, 1로 채움\n",
    "\n",
    "print(torch.zeros((3, 5))) #3행 5열, 0으로 채움\n",
    "\n",
    "print(torch.linspace(0, 10, 5)) # 0~10, 5등분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "#텐서의 형상 변환(reshape)\n",
    "\n",
    "t1 = torch.ones(4, 3)\n",
    "\n",
    "t2 = t1.view(3, 4) #3행 4열로 변환\n",
    "\n",
    "t3 = t1.view(12) #1차원 배열로 변환\n",
    "\n",
    "print(t1)\n",
    "\n",
    "print(t2)\n",
    "\n",
    "print(t3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "t1.view(1, 3, 4) #3차원으로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) iris 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# iris 데이터셋 로딩\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "print(X[:5])\n",
    "\n",
    "print(y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "<class 'numpy.ndarray'>\n",
      "(120, 4)\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split( X, y, random_state=10, test_size=0.2, stratify=y, shuffle=True )\n",
    "\n",
    "print(len(X_train))    \n",
    "print(type(X_train))\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#신경망 모형\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        #input layer, Linear 선형함수(1차함수)\n",
    "\n",
    "        #input nodes, output nodes 50\n",
    "\n",
    "        self.layer1 = nn.Linear(input_dim,50) \n",
    "\n",
    "        self.layer2 = nn.Linear(50, 20)\n",
    "\n",
    "        self.layer3 = nn.Linear(20, 3)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.layer1(x))\n",
    "\n",
    "        x = F.relu(self.layer2(x))\n",
    "\n",
    "        # 출력층의 활성화함수 - 소프트맥스\n",
    "\n",
    "        x = F.softmax(self.layer3(x), dim=0)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "import torch\n",
    "\n",
    "model = Model(X_train.shape[1]) # 초기화함수의 input_dim으로 변수개수가 전달됨\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # 최적화함수 정의\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() #손실함수 정의\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "loss: 1.0980700254440308\n",
      "Epoch 2\n",
      "loss: 1.097053050994873\n",
      "Epoch 3\n",
      "loss: 1.096076488494873\n",
      "Epoch 4\n",
      "loss: 1.0948431491851807\n",
      "Epoch 5\n",
      "loss: 1.0936310291290283\n",
      "Epoch 6\n",
      "loss: 1.092368245124817\n",
      "Epoch 7\n",
      "loss: 1.0912740230560303\n",
      "Epoch 8\n",
      "loss: 1.0903372764587402\n",
      "Epoch 9\n",
      "loss: 1.0895863771438599\n",
      "Epoch 10\n",
      "loss: 1.0889297723770142\n",
      "Epoch 11\n",
      "loss: 1.0883090496063232\n",
      "Epoch 12\n",
      "loss: 1.0878071784973145\n",
      "Epoch 13\n",
      "loss: 1.087348461151123\n",
      "Epoch 14\n",
      "loss: 1.086920142173767\n",
      "Epoch 15\n",
      "loss: 1.0865323543548584\n",
      "Epoch 16\n",
      "loss: 1.086174488067627\n",
      "Epoch 17\n",
      "loss: 1.0858628749847412\n",
      "Epoch 18\n",
      "loss: 1.0855845212936401\n",
      "Epoch 19\n",
      "loss: 1.0854016542434692\n",
      "Epoch 20\n",
      "loss: 1.085187315940857\n",
      "Epoch 21\n",
      "loss: 1.0849668979644775\n",
      "Epoch 22\n",
      "loss: 1.0847266912460327\n",
      "Epoch 23\n",
      "loss: 1.0845065116882324\n",
      "Epoch 24\n",
      "loss: 1.0842607021331787\n",
      "Epoch 25\n",
      "loss: 1.0840308666229248\n",
      "Epoch 26\n",
      "loss: 1.0838323831558228\n",
      "Epoch 27\n",
      "loss: 1.0836454629898071\n",
      "Epoch 28\n",
      "loss: 1.0834791660308838\n",
      "Epoch 29\n",
      "loss: 1.0833290815353394\n",
      "Epoch 30\n",
      "loss: 1.0831936597824097\n",
      "Epoch 31\n",
      "loss: 1.0830743312835693\n",
      "Epoch 32\n",
      "loss: 1.082971453666687\n",
      "Epoch 33\n",
      "loss: 1.0828834772109985\n",
      "Epoch 34\n",
      "loss: 1.082808017730713\n",
      "Epoch 35\n",
      "loss: 1.0827444791793823\n",
      "Epoch 36\n",
      "loss: 1.082690954208374\n",
      "Epoch 37\n",
      "loss: 1.0826458930969238\n",
      "Epoch 38\n",
      "loss: 1.0826071500778198\n",
      "Epoch 39\n",
      "loss: 1.0825737714767456\n",
      "Epoch 40\n",
      "loss: 1.0825445652008057\n",
      "Epoch 41\n",
      "loss: 1.0825188159942627\n",
      "Epoch 42\n",
      "loss: 1.082495927810669\n",
      "Epoch 43\n",
      "loss: 1.0824750661849976\n",
      "Epoch 44\n",
      "loss: 1.0824557542800903\n",
      "Epoch 45\n",
      "loss: 1.0824378728866577\n",
      "Epoch 46\n",
      "loss: 1.0824209451675415\n",
      "Epoch 47\n",
      "loss: 1.0824052095413208\n",
      "Epoch 48\n",
      "loss: 1.082390308380127\n",
      "Epoch 49\n",
      "loss: 1.0823763608932495\n",
      "Epoch 50\n",
      "loss: 1.082363486289978\n",
      "Epoch 51\n",
      "loss: 1.0823514461517334\n",
      "Epoch 52\n",
      "loss: 1.0823402404785156\n",
      "Epoch 53\n",
      "loss: 1.0823301076889038\n",
      "Epoch 54\n",
      "loss: 1.0823200941085815\n",
      "Epoch 55\n",
      "loss: 1.0823109149932861\n",
      "Epoch 56\n",
      "loss: 1.082302451133728\n",
      "Epoch 57\n",
      "loss: 1.0822951793670654\n",
      "Epoch 58\n",
      "loss: 1.0822888612747192\n",
      "Epoch 59\n",
      "loss: 1.0822845697402954\n",
      "Epoch 60\n",
      "loss: 1.0822815895080566\n",
      "Epoch 61\n",
      "loss: 1.0822787284851074\n",
      "Epoch 62\n",
      "loss: 1.0822752714157104\n",
      "Epoch 63\n",
      "loss: 1.0822712182998657\n",
      "Epoch 64\n",
      "loss: 1.0822677612304688\n",
      "Epoch 65\n",
      "loss: 1.0822646617889404\n",
      "Epoch 66\n",
      "loss: 1.0822631120681763\n",
      "Epoch 67\n",
      "loss: 1.0822616815567017\n",
      "Epoch 68\n",
      "loss: 1.082260012626648\n",
      "Epoch 69\n",
      "loss: 1.0822581052780151\n",
      "Epoch 70\n",
      "loss: 1.0822558403015137\n",
      "Epoch 71\n",
      "loss: 1.0822532176971436\n",
      "Epoch 72\n",
      "loss: 1.0822510719299316\n",
      "Epoch 73\n",
      "loss: 1.082249402999878\n",
      "Epoch 74\n",
      "loss: 1.0822476148605347\n",
      "Epoch 75\n",
      "loss: 1.082245945930481\n",
      "Epoch 76\n",
      "loss: 1.0822436809539795\n",
      "Epoch 77\n",
      "loss: 1.0822416543960571\n",
      "Epoch 78\n",
      "loss: 1.0822392702102661\n",
      "Epoch 79\n",
      "loss: 1.0822371244430542\n",
      "Epoch 80\n",
      "loss: 1.082235336303711\n",
      "Epoch 81\n",
      "loss: 1.0822333097457886\n",
      "Epoch 82\n",
      "loss: 1.0822312831878662\n",
      "Epoch 83\n",
      "loss: 1.0822291374206543\n",
      "Epoch 84\n",
      "loss: 1.0822268724441528\n",
      "Epoch 85\n",
      "loss: 1.08222496509552\n",
      "Epoch 86\n",
      "loss: 1.0822230577468872\n",
      "Epoch 87\n",
      "loss: 1.0822213888168335\n",
      "Epoch 88\n",
      "loss: 1.0822194814682007\n",
      "Epoch 89\n",
      "loss: 1.082217812538147\n",
      "Epoch 90\n",
      "loss: 1.0822159051895142\n",
      "Epoch 91\n",
      "loss: 1.082214117050171\n",
      "Epoch 92\n",
      "loss: 1.0822126865386963\n",
      "Epoch 93\n",
      "loss: 1.0822111368179321\n",
      "Epoch 94\n",
      "loss: 1.0822093486785889\n",
      "Epoch 95\n",
      "loss: 1.0822077989578247\n",
      "Epoch 96\n",
      "loss: 1.0822062492370605\n",
      "Epoch 97\n",
      "loss: 1.082204818725586\n",
      "Epoch 98\n",
      "loss: 1.0822032690048218\n",
      "Epoch 99\n",
      "loss: 1.0822020769119263\n",
      "Epoch 100\n",
      "loss: 1.0822006464004517\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#넘파이배열로부터 텐서를 만들고\n",
    "X_train = Variable(torch.from_numpy(np.array(X_train))).float()  # 메모리 => cpu,gpu\n",
    "y_train = Variable(torch.from_numpy(np.array(y_train))).long()\n",
    "#label = Variable(torch.from_numpy(np.array(label)).float()).to(device)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "\n",
    "    print(\"Epoch\",epoch)\n",
    "\n",
    "    #예측값\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    #손실함수에 예측값과 실제값을 입력\n",
    "\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    print('loss:',loss.item())\n",
    "\n",
    "    \n",
    "\n",
    "    # 경사 초기화\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward() # 역전파\n",
    "\n",
    "    optimizer.step() # 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1036e-08, 4.9576e-03, 6.6222e-05],\n",
       "        [1.9384e-02, 6.9058e-07, 1.6350e-10],\n",
       "        [1.5941e-08, 1.9797e-03, 1.2581e-04],\n",
       "        [2.5040e-09, 3.7730e-04, 5.8909e-04],\n",
       "        [2.5334e-06, 3.6634e-01, 7.0266e-07]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "# Prediction\n",
    "\n",
    "X_test = Variable(torch.from_numpy(X_test)).float()\n",
    "\n",
    "pred = model(X_test)\n",
    "\n",
    "pred[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2, 1, 2, 0, 2, 2, 0, 0, 1, 2, 2, 1, 0, 0, 1, 2, 0, 2, 2,\n",
       "       2, 0, 0, 1, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.argmax(pred.data.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 모형의 정확도 측정\n",
    "\n",
    "accuracy_score(y_test, np.argmax(pred.data.numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "torch.save(model, \"../MODL/iris-torch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "model2 = torch.load(\"../MODL/iris-torch.h5\")\n",
    "\n",
    "np.argmax(model2(X_test[0]).data.numpy())\n",
    "#np.argmax(model2(Variable(torch.from_numpy(X_test[0])).float()).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [32, 3]                   --\n",
       "├─Linear: 1-1                            [32, 50]                  250\n",
       "├─Linear: 1-2                            [32, 20]                  1,020\n",
       "├─Linear: 1-3                            [32, 3]                   63\n",
       "==========================================================================================\n",
       "Total params: 1,333\n",
       "Trainable params: 1,333\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.02\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "#pip install torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model)\n",
    "\n",
    "# input_size=(batch size, input nodes)\n",
    "\n",
    "summary(model, input_size=(32, 4)) # 미니배치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) gpu 사용법\n",
    "* tensorflow 처럼 cuda 버전 맞춰서 torch 설치하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!C:\\Users\\tjoeun\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "1\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#gpu 사용 가능 여부\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "#  True\n",
    "\n",
    "#gpu 장치의 인덱스\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "#  0\n",
    "\n",
    "# 사용가능한 gpu 장치수\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#  1\n",
    "\n",
    "#  gpu의 이름\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2], device='cuda:0')\n",
      "tensor([1, 2], device='cuda:0')\n",
      "tensor([1, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "import torch\n",
    "\n",
    "# Default CUDA device\n",
    "\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "# 텐서 자료를 gpu에 저장 방법 3가지\n",
    "\n",
    "a = torch.tensor([1, 2], device=cuda)\n",
    "\n",
    "b = torch.tensor([1, 2]).cuda()\n",
    "\n",
    "c = torch.tensor([1, 2]).to(device=cuda)\n",
    "\n",
    "print(a)\n",
    "\n",
    "print(b)\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "#사용하지 않는 텐서 정리\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) wine 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  \n",
       "0                            3.92   1065.0  \n",
       "1                            3.40   1050.0  \n",
       "2                            3.17   1185.0  \n",
       "3                            3.45   1480.0  \n",
       "4                            2.93    735.0  \n",
       "..                            ...      ...  \n",
       "173                          1.74    740.0  \n",
       "174                          1.56    750.0  \n",
       "175                          1.56    835.0  \n",
       "176                          1.62    840.0  \n",
       "177                          1.60    560.0  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "\n",
    "##############################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(wine.data, columns=wine.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "wine.target\n",
    "\n",
    "##############################\n",
    "\n",
    "#독립변수\n",
    "\n",
    "X = wine.data\n",
    "\n",
    "#종속변수\n",
    "\n",
    "y = wine.target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "##############################\n",
    "\n",
    "import torch\n",
    "\n",
    "# 학습용 데이터를 텐서로 변환\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "\n",
    "# 검증용 데이터를 텐서로 변환\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "##############################\n",
    "\n",
    "#텐서를 gpu로 옮기고\n",
    "\n",
    "X_train=X_train.cuda()\n",
    "\n",
    "y_train=y_train.cuda()\n",
    "\n",
    "X_test=X_test.cuda()\n",
    "\n",
    "y_test=y_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1.3480e+01, 1.6700e+00, 2.6400e+00, 2.2500e+01, 8.9000e+01, 2.6000e+00,\n",
      "        1.1000e+00, 5.2000e-01, 2.2900e+00, 1.1750e+01, 5.7000e-01, 1.7800e+00,\n",
      "        6.2000e+02], device='cuda:0'), tensor(2, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 독립변수와 종속변수의 텐서를 합침\n",
    "\n",
    "train = TensorDataset(X_train, y_train)\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "# 미니배치로 분할\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 신경망 구성\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(13, 96) # input 13, output 96\n",
    "\n",
    "        self.fc2 = nn.Linear(96, 72)\n",
    "\n",
    "        self.fc3 = nn.Linear(72, 64)\n",
    "\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "\n",
    "        self.fc6 = nn.Linear(16, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        # softmax 함수의 결과값에 log 적용\n",
    "\n",
    "        # dim=0 행방향, 1 열방향\n",
    "\n",
    "        # dim=0, row에서 최대값을 구하는 방식\n",
    "\n",
    "        return F.log_softmax(x, dim=0)\n",
    "\n",
    "# 인스턴스 생성\n",
    "\n",
    "model = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 tensor(5.8417, device='cuda:0')\n",
      "100 tensor(6.2587, device='cuda:0')\n",
      "150 tensor(5.6116, device='cuda:0')\n",
      "200 tensor(5.8158, device='cuda:0')\n",
      "250 tensor(5.6393, device='cuda:0')\n",
      "300 tensor(5.5924, device='cuda:0')\n",
      "350 tensor(5.5390, device='cuda:0')\n",
      "400 tensor(5.7564, device='cuda:0')\n",
      "450 tensor(5.7482, device='cuda:0')\n",
      "500 tensor(5.7077, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "# 손실함수 객체\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화함수\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 시작\n",
    "\n",
    "for epoch in range(500):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for train_x, train_y in train_loader:\n",
    "\n",
    "        # 계산 그래프 구성\n",
    "\n",
    "        train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "\n",
    "        #텐서를 gpu로 이동시킴\n",
    "\n",
    "        train_x=train_x.cuda()\n",
    "\n",
    "        train_y=train_y.cuda()        \n",
    "\n",
    "        # 경사 초기화\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 계산\n",
    "\n",
    "        output = model(train_x)\n",
    "\n",
    "        # 오차계산\n",
    "\n",
    "        loss = criterion(output, train_y)\n",
    "\n",
    "        # 역전파 계산\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # 가중치 업데이트\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # 누적 오차 계산\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "    # 50회 반복마다 누적오차 출력\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "\n",
    "        print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6388888888888888"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "# 계산 그래프 구성\n",
    "\n",
    "X_test, y_test = Variable(X_test), Variable(y_test)\n",
    "\n",
    "# 출력값 계산\n",
    "\n",
    "result = torch.max(model(X_test).data, 1)[1]\n",
    "\n",
    "# 모형의 정확도 측정\n",
    "\n",
    "# gpu에 저장된 텐서를 cpu로 이동시킴\n",
    "\n",
    "y_test=y_test.cpu()\n",
    "\n",
    "result=result.cpu()\n",
    "\n",
    "accuracy = sum(y_test.data.numpy() == result.numpy()) / len(y_test.data.numpy())\n",
    "\n",
    "# 모형의 정확도 출력\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) mnist 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n",
      "torch.Size([60000])\n",
      "10 tensor(297.1051, device='cuda:0')\n",
      "20 tensor(79.6446, device='cuda:0')\n",
      "30 tensor(35.6095, device='cuda:0')\n",
      "40 tensor(17.4472, device='cuda:0')\n",
      "50 tensor(8.4787, device='cuda:0')\n",
      "60 tensor(3.5049, device='cuda:0')\n",
      "70 tensor(2.0258, device='cuda:0')\n",
      "80 tensor(1.5283, device='cuda:0')\n",
      "90 tensor(0.9067, device='cuda:0')\n",
      "100 tensor(0.7611, device='cuda:0')\n",
      "tensor([7, 2, 1, 0, 4])\n",
      "[7 2 1 0 4]\n",
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc5): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc6): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Net                                      [100, 10]                 --\n",
       "├─Linear: 1-1                            [100, 256]                200,960\n",
       "├─Linear: 1-2                            [100, 256]                65,792\n",
       "├─Linear: 1-3                            [100, 256]                65,792\n",
       "├─Linear: 1-4                            [100, 128]                32,896\n",
       "├─Linear: 1-5                            [100, 128]                16,512\n",
       "├─Linear: 1-6                            [100, 10]                 1,290\n",
       "==========================================================================================\n",
       "Total params: 383,242\n",
       "Trainable params: 383,242\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 38.32\n",
       "==========================================================================================\n",
       "Input size (MB): 0.31\n",
       "Forward/backward pass size (MB): 0.83\n",
       "Params size (MB): 1.53\n",
       "Estimated Total Size (MB): 2.67\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "##############################\n",
    "\n",
    "X_train= X_train.reshape(-1,784)\n",
    "\n",
    "X_test= X_test.reshape(-1,784)\n",
    "\n",
    "X_train=X_train/255.\n",
    "\n",
    "X_test=X_test/255.\n",
    "\n",
    "##############################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "plt.imshow(X_train[0].reshape(28,28),cmap=cm.gray_r)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##############################\n",
    "\n",
    "import torch\n",
    "\n",
    "# 학습용 데이터 텐서 변환\n",
    "\n",
    "# from_numpy() 넘파이배열을 텐서로 변환\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "\n",
    "y_train = torch.from_numpy(y_train.astype('int32')).long()\n",
    "\n",
    "# 검증용 데이터 텐서 변환\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "y_test = torch.from_numpy(y_test.astype('int32')).long()\n",
    "\n",
    "# 변환된 텐서의 샘플수 확인\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "##############################\n",
    "\n",
    "X_train=X_train.cuda()\n",
    "\n",
    "y_train=y_train.cuda()\n",
    "\n",
    "X_test=X_test.cuda()\n",
    "\n",
    "y_test=y_test.cuda()\n",
    "\n",
    "##############################\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 독립변수와 종속변수 텐서를 합침\n",
    "\n",
    "train = TensorDataset(X_train, y_train)\n",
    "\n",
    "# 미니배치 분할\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)\n",
    "\n",
    "##############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "\n",
    "        self.fc6 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        x = F.relu(self.fc4(x))\n",
    "\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return F.log_softmax(x,dim=0)\n",
    "\n",
    "model = Net().cuda()\n",
    "\n",
    "##############################\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# 손실함수\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화함수\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_train, y_train in train_loader:\n",
    "\n",
    "        # 계산 그래프 구성\n",
    "\n",
    "        X_train, y_train = Variable(X_train), Variable(y_train)\n",
    "\n",
    "        X_train=X_train.cuda()\n",
    "\n",
    "        y_train=y_train.cuda()  \n",
    "\n",
    "        # 경사 초기화\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 계산\n",
    "\n",
    "        output = model(X_train)\n",
    "\n",
    "        # 오차계산\n",
    "\n",
    "        loss = criterion(output, y_train)\n",
    "\n",
    "        # 역전파 계산\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # 가중치 업데이트\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # 누적 오차 계산\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "    # 10회 반복마다 누적오차 출력\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "\n",
    "        print(epoch+1, total_loss)\n",
    "\n",
    "##############################\n",
    "\n",
    "# 계산 그래프 구성\n",
    "\n",
    "X_test, y_test = Variable(X_test), Variable(y_test)\n",
    "\n",
    "result = torch.max(model(X_test).data, 1)[1]\n",
    "\n",
    "y_test=y_test.cpu()\n",
    "\n",
    "result=result.cpu()\n",
    "\n",
    "print(result[:5]) # 출력값\n",
    "\n",
    "print(y_test.data.numpy()[:5]) #실제값\n",
    "\n",
    "# 모형의 정확도 측정\n",
    "\n",
    "accuracy = sum(y_test.data.numpy() == result.numpy()) / len(y_test.data.numpy())\n",
    "\n",
    "# 모형의 정확도 출력\n",
    "\n",
    "accuracy\n",
    "\n",
    "##############################\n",
    "\n",
    "print(model)\n",
    "\n",
    "##############################\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "# batch_size, input nodes\n",
    "\n",
    "summary(model, input_size=(batch_size, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) mnist 데이터셋(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "##############################\n",
    "\n",
    "# 샘플수, 채널(흑백1/컬러3), 가로, 세로\n",
    "\n",
    "X_train= X_train.reshape(-1,1,28,28)\n",
    "\n",
    "X_test= X_test.reshape(-1,1,28,28)\n",
    "\n",
    "X_train=X_train/255.\n",
    "\n",
    "X_test=X_test/255.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "import torch\n",
    "\n",
    "# 넘파이배열을 텐서로 이동\n",
    "\n",
    "X_train=torch.from_numpy(X_train).float()\n",
    "\n",
    "y_train=torch.from_numpy(y_train.astype('int32')).long()\n",
    "\n",
    "X_test=torch.from_numpy(X_test).float()\n",
    "\n",
    "y_test=torch.from_numpy(y_test.astype('int32')).long()\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "##############################\n",
    "\n",
    "#텐서를 gpu로 옮기고\n",
    "\n",
    "X_train=X_train.cuda()\n",
    "\n",
    "y_train=y_train.cuda()\n",
    "\n",
    "X_test=X_test.cuda()\n",
    "\n",
    "y_test=y_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]], device='cuda:0'), tensor(5, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 독립변수와 종속변수 텐서를 합침\n",
    "\n",
    "train = TensorDataset(X_train, y_train)\n",
    "\n",
    "#print(train[0])\n",
    "\n",
    "# 미니배치 분할\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 신경망 구성\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # input shape: 60000, 1, 28, 28\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 입력 채널 수(흑백1,컬러3), 출력 채널 수, 필터 크기\n",
    "        # 6 * ((28-5)/1)+1) * ((28-5)/1)+1) = 6*24*24\n",
    "\n",
    "        # max_pool(2) --> 6*12*12\n",
    "\n",
    "        # input shape: 6*12*12\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 16 * ((12-5)/1)+1) * ((12-5)/1)+1) = 16*8*8\n",
    "\n",
    "        # max_pool(2) --> 16*4*4\n",
    "\n",
    "        # input shape: 16*4*4\n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2) # 풀링 영역 크기\n",
    "\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        x = x.view(-1, 256)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=0)\n",
    "\n",
    "    \n",
    "\n",
    "# 인스턴스 생성\n",
    "\n",
    "model = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 tensor(19.2938, device='cuda:0')\n",
      "100 tensor(10.2149, device='cuda:0')\n",
      "150 tensor(6.3706, device='cuda:0')\n",
      "200 tensor(3.9375, device='cuda:0')\n",
      "250 tensor(2.4561, device='cuda:0')\n",
      "300 tensor(1.3874, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(300):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_train, y_train in train_loader:\n",
    "\n",
    "        X_train, y_train = Variable(X_train), Variable(y_train)     # 계산 그래프 구성\n",
    "\n",
    "        #텐서를 gpu로 이동시킴\n",
    "\n",
    "        X_train=X_train.cuda()\n",
    "\n",
    "        y_train=y_train.cuda()\n",
    "\n",
    "        # 경사 초기화\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 계산\n",
    "\n",
    "        output = model(X_train)\n",
    "\n",
    "        # 오차계산\n",
    "\n",
    "        loss = criterion(output, y_train)\n",
    "\n",
    "        # 역전파 계산\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # 가중치 업데이트\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # 누적 오차 계산\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "    # 50회 반복마다 누적 오차 출력\n",
    "\n",
    "    if (epoch+1) % 50 == 0:\n",
    "\n",
    "        print(epoch+1, total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "X_test, y_test = Variable(X_test), Variable(y_test)\n",
    "\n",
    "# [0] values, [1] indices\n",
    "\n",
    "# 모형이 분류한 값들(10개) 중 가장 큰 값과 인덱스\n",
    "\n",
    "# 출력이 0 또는 1이 되게 함\n",
    "\n",
    "result = torch.max(model(X_test).data, 1)[1]\n",
    "\n",
    "#print(result)\n",
    "\n",
    "# 모형의 정확도 측정\n",
    "\n",
    "# gpu에 저장된 텐서를 cpu로 이동시킴\n",
    "\n",
    "y_test=y_test.cpu()\n",
    "\n",
    "result=result.cpu()\n",
    "\n",
    "accuracy = sum(y_test.data.numpy() == result.numpy()) / len(y_test.data.numpy())\n",
    "\n",
    "accuracy\n",
    "\n",
    "##############################\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "# batch_size, channels, height, width\n",
    "\n",
    "summary(model, input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) mnist 데이터셋(skorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(60000, 784)\n",
      "<class 'numpy.ndarray'>\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# skorch : pytorch를 쉽게 구현할 수 있는 라이브러리\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "##############################\n",
    "\n",
    "X_train= X_train.reshape(-1,784)\n",
    "\n",
    "X_test= X_test.reshape(-1,784)\n",
    "\n",
    "X_train=X_train/255.\n",
    "\n",
    "X_test=X_test/255.\n",
    "\n",
    "print(X_train.shape)\n",
    "print(type(X_train))\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.1)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp39-cp39-win_amd64.whl (4.9 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\tjoeun\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#!C:\\Users\\tjoeun\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n",
      "torch.Size([10000, 784])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(X_train\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(X_test\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 17\u001b[0m X_train\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     19\u001b[0m y_train\u001b[39m=\u001b[39my_train\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     21\u001b[0m X_test\u001b[39m=\u001b[39mX_test\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\tjoeun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "import torch\n",
    "\n",
    "X_train=torch.from_numpy(X_train).float()\n",
    "\n",
    "y_train=torch.from_numpy(y_train.astype('int32')).long()\n",
    "\n",
    "X_test=torch.from_numpy(X_test).float()\n",
    "\n",
    "y_test=torch.from_numpy(y_test.astype('int32')).long()\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train=X_train.cuda()\n",
    "\n",
    "y_train=y_train.cuda()\n",
    "\n",
    "X_test=X_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 신경망 구성\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "model=Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skorch\n",
      "  Obtaining dependency information for skorch from https://files.pythonhosted.org/packages/b7/fa/55cad8c7cb104fe04341945a05e21da3b04c0a0fc0b3644ee2175b8ad8b8/skorch-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading skorch-0.14.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\tjoeun\\appdata\\roaming\\python\\python39\\site-packages (from skorch) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from skorch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from skorch) (1.11.1)\n",
      "Collecting tabulate>=0.7.7 (from skorch)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting tqdm>=4.14.0 (from skorch)\n",
      "  Obtaining dependency information for tqdm>=4.14.0 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.22.0->skorch) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.22.0->skorch) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjoeun\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.14.0->skorch) (0.4.6)\n",
      "Downloading skorch-0.14.0-py3-none-any.whl (221 kB)\n",
      "   ---------------------------------------- 0.0/221.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 221.3/221.3 kB ? eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Installing collected packages: tqdm, tabulate, skorch\n",
      "Successfully installed skorch-0.14.0 tabulate-0.9.0 tqdm-4.66.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\tjoeun\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "# pip install skorch\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "net = NeuralNetClassifier(Net,max_epochs=20,lr=0.1)\n",
    "\n",
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 모형의 정확도 계산\n",
    "\n",
    "pred = net.predict(X_test)\n",
    "\n",
    "y_test = y_test.cpu().numpy()\n",
    "\n",
    "accuracy = np.mean(pred == y_test)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(100,784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) 단순회귀분석(random data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "         0.7778,  1.0000])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import os    \n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # matplot 커널 겹침에러 방지\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "################################\n",
    "\n",
    "a=torch.linspace(-1, 1, 10)\n",
    "\n",
    "print(a)\n",
    "\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000],\n",
      "        [-0.7778],\n",
      "        [-0.5556],\n",
      "        [-0.3333],\n",
      "        [-0.1111],\n",
      "        [ 0.1111],\n",
      "        [ 0.3333],\n",
      "        [ 0.5556],\n",
      "        [ 0.7778],\n",
      "        [ 1.0000]])\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "# squeeze 차원축소\n",
    "# un squeeze 차원증가\n",
    "\n",
    "# size가 1인 dimension 추가\n",
    "\n",
    "# dim=추가할인덱스, (0, 추가할 인덱스)\n",
    "\n",
    "b=torch.unsqueeze(torch.linspace(-1, 1, 10), dim=1)\n",
    "\n",
    "print(b)\n",
    "\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
      "         0.7778,  1.0000])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "# size가 1인 dimension 제거\n",
    "\n",
    "# dim=추가할인덱스, (0, 추가할 인덱스)\n",
    "\n",
    "print(b.shape)\n",
    "\n",
    "print(torch.squeeze(b))\n",
    "\n",
    "print(torch.squeeze(b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f8877a1070>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "X = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1).to(device)\n",
    "\n",
    "y = X.pow(3) + 0.3 * torch.rand(X.size()).to(device)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "plt.scatter(X.cpu().numpy(), y.cpu().numpy()) # 학습외에는 RAM에서 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=100, bias=True)\n",
       "  (1): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "################################\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = nn.Sequential(\n",
    "\n",
    "    nn.Linear(1, 100),\n",
    "\n",
    "    nn.Linear(100, 10),\n",
    "\n",
    "    nn.Linear(10, 1)\n",
    "\n",
    ")\n",
    "\n",
    "model.to(device) # Convert to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.156\n",
      "epoch: 1, loss: 0.152\n",
      "epoch: 2, loss: 0.148\n",
      "epoch: 3, loss: 0.145\n",
      "epoch: 4, loss: 0.141\n",
      "epoch: 5, loss: 0.138\n",
      "epoch: 6, loss: 0.135\n",
      "epoch: 7, loss: 0.131\n",
      "epoch: 8, loss: 0.128\n",
      "epoch: 9, loss: 0.126\n",
      "epoch: 10, loss: 0.123\n",
      "epoch: 11, loss: 0.120\n",
      "epoch: 12, loss: 0.117\n",
      "epoch: 13, loss: 0.115\n",
      "epoch: 14, loss: 0.112\n",
      "epoch: 15, loss: 0.110\n",
      "epoch: 16, loss: 0.108\n",
      "epoch: 17, loss: 0.105\n",
      "epoch: 18, loss: 0.103\n",
      "epoch: 19, loss: 0.101\n",
      "epoch: 20, loss: 0.099\n",
      "epoch: 21, loss: 0.097\n",
      "epoch: 22, loss: 0.095\n",
      "epoch: 23, loss: 0.094\n",
      "epoch: 24, loss: 0.092\n",
      "epoch: 25, loss: 0.090\n",
      "epoch: 26, loss: 0.089\n",
      "epoch: 27, loss: 0.087\n",
      "epoch: 28, loss: 0.086\n",
      "epoch: 29, loss: 0.084\n",
      "epoch: 30, loss: 0.083\n",
      "epoch: 31, loss: 0.081\n",
      "epoch: 32, loss: 0.080\n",
      "epoch: 33, loss: 0.079\n",
      "epoch: 34, loss: 0.077\n",
      "epoch: 35, loss: 0.076\n",
      "epoch: 36, loss: 0.075\n",
      "epoch: 37, loss: 0.074\n",
      "epoch: 38, loss: 0.073\n",
      "epoch: 39, loss: 0.072\n",
      "epoch: 40, loss: 0.071\n",
      "epoch: 41, loss: 0.070\n",
      "epoch: 42, loss: 0.069\n",
      "epoch: 43, loss: 0.068\n",
      "epoch: 44, loss: 0.067\n",
      "epoch: 45, loss: 0.066\n",
      "epoch: 46, loss: 0.065\n",
      "epoch: 47, loss: 0.064\n",
      "epoch: 48, loss: 0.063\n",
      "epoch: 49, loss: 0.063\n",
      "epoch: 50, loss: 0.062\n",
      "epoch: 51, loss: 0.061\n",
      "epoch: 52, loss: 0.060\n",
      "epoch: 53, loss: 0.060\n",
      "epoch: 54, loss: 0.059\n",
      "epoch: 55, loss: 0.058\n",
      "epoch: 56, loss: 0.058\n",
      "epoch: 57, loss: 0.057\n",
      "epoch: 58, loss: 0.056\n",
      "epoch: 59, loss: 0.056\n",
      "epoch: 60, loss: 0.055\n",
      "epoch: 61, loss: 0.055\n",
      "epoch: 62, loss: 0.054\n",
      "epoch: 63, loss: 0.054\n",
      "epoch: 64, loss: 0.053\n",
      "epoch: 65, loss: 0.053\n",
      "epoch: 66, loss: 0.052\n",
      "epoch: 67, loss: 0.052\n",
      "epoch: 68, loss: 0.051\n",
      "epoch: 69, loss: 0.051\n",
      "epoch: 70, loss: 0.050\n",
      "epoch: 71, loss: 0.050\n",
      "epoch: 72, loss: 0.049\n",
      "epoch: 73, loss: 0.049\n",
      "epoch: 74, loss: 0.049\n",
      "epoch: 75, loss: 0.048\n",
      "epoch: 76, loss: 0.048\n",
      "epoch: 77, loss: 0.047\n",
      "epoch: 78, loss: 0.047\n",
      "epoch: 79, loss: 0.047\n",
      "epoch: 80, loss: 0.046\n",
      "epoch: 81, loss: 0.046\n",
      "epoch: 82, loss: 0.046\n",
      "epoch: 83, loss: 0.045\n",
      "epoch: 84, loss: 0.045\n",
      "epoch: 85, loss: 0.045\n",
      "epoch: 86, loss: 0.045\n",
      "epoch: 87, loss: 0.044\n",
      "epoch: 88, loss: 0.044\n",
      "epoch: 89, loss: 0.044\n",
      "epoch: 90, loss: 0.043\n",
      "epoch: 91, loss: 0.043\n",
      "epoch: 92, loss: 0.043\n",
      "epoch: 93, loss: 0.043\n",
      "epoch: 94, loss: 0.042\n",
      "epoch: 95, loss: 0.042\n",
      "epoch: 96, loss: 0.042\n",
      "epoch: 97, loss: 0.042\n",
      "epoch: 98, loss: 0.041\n",
      "epoch: 99, loss: 0.041\n",
      "epoch: 100, loss: 0.041\n",
      "epoch: 101, loss: 0.041\n",
      "epoch: 102, loss: 0.040\n",
      "epoch: 103, loss: 0.040\n",
      "epoch: 104, loss: 0.040\n",
      "epoch: 105, loss: 0.040\n",
      "epoch: 106, loss: 0.040\n",
      "epoch: 107, loss: 0.039\n",
      "epoch: 108, loss: 0.039\n",
      "epoch: 109, loss: 0.039\n",
      "epoch: 110, loss: 0.039\n",
      "epoch: 111, loss: 0.039\n",
      "epoch: 112, loss: 0.038\n",
      "epoch: 113, loss: 0.038\n",
      "epoch: 114, loss: 0.038\n",
      "epoch: 115, loss: 0.038\n",
      "epoch: 116, loss: 0.038\n",
      "epoch: 117, loss: 0.038\n",
      "epoch: 118, loss: 0.037\n",
      "epoch: 119, loss: 0.037\n",
      "epoch: 120, loss: 0.037\n",
      "epoch: 121, loss: 0.037\n",
      "epoch: 122, loss: 0.037\n",
      "epoch: 123, loss: 0.037\n",
      "epoch: 124, loss: 0.037\n",
      "epoch: 125, loss: 0.036\n",
      "epoch: 126, loss: 0.036\n",
      "epoch: 127, loss: 0.036\n",
      "epoch: 128, loss: 0.036\n",
      "epoch: 129, loss: 0.036\n",
      "epoch: 130, loss: 0.036\n",
      "epoch: 131, loss: 0.036\n",
      "epoch: 132, loss: 0.035\n",
      "epoch: 133, loss: 0.035\n",
      "epoch: 134, loss: 0.035\n",
      "epoch: 135, loss: 0.035\n",
      "epoch: 136, loss: 0.035\n",
      "epoch: 137, loss: 0.035\n",
      "epoch: 138, loss: 0.035\n",
      "epoch: 139, loss: 0.035\n",
      "epoch: 140, loss: 0.035\n",
      "epoch: 141, loss: 0.034\n",
      "epoch: 142, loss: 0.034\n",
      "epoch: 143, loss: 0.034\n",
      "epoch: 144, loss: 0.034\n",
      "epoch: 145, loss: 0.034\n",
      "epoch: 146, loss: 0.034\n",
      "epoch: 147, loss: 0.034\n",
      "epoch: 148, loss: 0.034\n",
      "epoch: 149, loss: 0.034\n",
      "epoch: 150, loss: 0.034\n",
      "epoch: 151, loss: 0.033\n",
      "epoch: 152, loss: 0.033\n",
      "epoch: 153, loss: 0.033\n",
      "epoch: 154, loss: 0.033\n",
      "epoch: 155, loss: 0.033\n",
      "epoch: 156, loss: 0.033\n",
      "epoch: 157, loss: 0.033\n",
      "epoch: 158, loss: 0.033\n",
      "epoch: 159, loss: 0.033\n",
      "epoch: 160, loss: 0.033\n",
      "epoch: 161, loss: 0.033\n",
      "epoch: 162, loss: 0.032\n",
      "epoch: 163, loss: 0.032\n",
      "epoch: 164, loss: 0.032\n",
      "epoch: 165, loss: 0.032\n",
      "epoch: 166, loss: 0.032\n",
      "epoch: 167, loss: 0.032\n",
      "epoch: 168, loss: 0.032\n",
      "epoch: 169, loss: 0.032\n",
      "epoch: 170, loss: 0.032\n",
      "epoch: 171, loss: 0.032\n",
      "epoch: 172, loss: 0.032\n",
      "epoch: 173, loss: 0.032\n",
      "epoch: 174, loss: 0.032\n",
      "epoch: 175, loss: 0.032\n",
      "epoch: 176, loss: 0.031\n",
      "epoch: 177, loss: 0.031\n",
      "epoch: 178, loss: 0.031\n",
      "epoch: 179, loss: 0.031\n",
      "epoch: 180, loss: 0.031\n",
      "epoch: 181, loss: 0.031\n",
      "epoch: 182, loss: 0.031\n",
      "epoch: 183, loss: 0.031\n",
      "epoch: 184, loss: 0.031\n",
      "epoch: 185, loss: 0.031\n",
      "epoch: 186, loss: 0.031\n",
      "epoch: 187, loss: 0.031\n",
      "epoch: 188, loss: 0.031\n",
      "epoch: 189, loss: 0.031\n",
      "epoch: 190, loss: 0.031\n",
      "epoch: 191, loss: 0.031\n",
      "epoch: 192, loss: 0.031\n",
      "epoch: 193, loss: 0.031\n",
      "epoch: 194, loss: 0.030\n",
      "epoch: 195, loss: 0.030\n",
      "epoch: 196, loss: 0.030\n",
      "epoch: 197, loss: 0.030\n",
      "epoch: 198, loss: 0.030\n",
      "epoch: 199, loss: 0.030\n",
      "epoch: 200, loss: 0.030\n",
      "epoch: 201, loss: 0.030\n",
      "epoch: 202, loss: 0.030\n",
      "epoch: 203, loss: 0.030\n",
      "epoch: 204, loss: 0.030\n",
      "epoch: 205, loss: 0.030\n",
      "epoch: 206, loss: 0.030\n",
      "epoch: 207, loss: 0.030\n",
      "epoch: 208, loss: 0.030\n",
      "epoch: 209, loss: 0.030\n",
      "epoch: 210, loss: 0.030\n",
      "epoch: 211, loss: 0.030\n",
      "epoch: 212, loss: 0.030\n",
      "epoch: 213, loss: 0.030\n",
      "epoch: 214, loss: 0.030\n",
      "epoch: 215, loss: 0.030\n",
      "epoch: 216, loss: 0.030\n",
      "epoch: 217, loss: 0.030\n",
      "epoch: 218, loss: 0.030\n",
      "epoch: 219, loss: 0.029\n",
      "epoch: 220, loss: 0.029\n",
      "epoch: 221, loss: 0.029\n",
      "epoch: 222, loss: 0.029\n",
      "epoch: 223, loss: 0.029\n",
      "epoch: 224, loss: 0.029\n",
      "epoch: 225, loss: 0.029\n",
      "epoch: 226, loss: 0.029\n",
      "epoch: 227, loss: 0.029\n",
      "epoch: 228, loss: 0.029\n",
      "epoch: 229, loss: 0.029\n",
      "epoch: 230, loss: 0.029\n",
      "epoch: 231, loss: 0.029\n",
      "epoch: 232, loss: 0.029\n",
      "epoch: 233, loss: 0.029\n",
      "epoch: 234, loss: 0.029\n",
      "epoch: 235, loss: 0.029\n",
      "epoch: 236, loss: 0.029\n",
      "epoch: 237, loss: 0.029\n",
      "epoch: 238, loss: 0.029\n",
      "epoch: 239, loss: 0.029\n",
      "epoch: 240, loss: 0.029\n",
      "epoch: 241, loss: 0.029\n",
      "epoch: 242, loss: 0.029\n",
      "epoch: 243, loss: 0.029\n",
      "epoch: 244, loss: 0.029\n",
      "epoch: 245, loss: 0.029\n",
      "epoch: 246, loss: 0.029\n",
      "epoch: 247, loss: 0.029\n",
      "epoch: 248, loss: 0.029\n",
      "epoch: 249, loss: 0.029\n",
      "epoch: 250, loss: 0.029\n",
      "epoch: 251, loss: 0.029\n",
      "epoch: 252, loss: 0.029\n",
      "epoch: 253, loss: 0.029\n",
      "epoch: 254, loss: 0.029\n",
      "epoch: 255, loss: 0.029\n",
      "epoch: 256, loss: 0.029\n",
      "epoch: 257, loss: 0.029\n",
      "epoch: 258, loss: 0.028\n",
      "epoch: 259, loss: 0.028\n",
      "epoch: 260, loss: 0.028\n",
      "epoch: 261, loss: 0.028\n",
      "epoch: 262, loss: 0.028\n",
      "epoch: 263, loss: 0.028\n",
      "epoch: 264, loss: 0.028\n",
      "epoch: 265, loss: 0.028\n",
      "epoch: 266, loss: 0.028\n",
      "epoch: 267, loss: 0.028\n",
      "epoch: 268, loss: 0.028\n",
      "epoch: 269, loss: 0.028\n",
      "epoch: 270, loss: 0.028\n",
      "epoch: 271, loss: 0.028\n",
      "epoch: 272, loss: 0.028\n",
      "epoch: 273, loss: 0.028\n",
      "epoch: 274, loss: 0.028\n",
      "epoch: 275, loss: 0.028\n",
      "epoch: 276, loss: 0.028\n",
      "epoch: 277, loss: 0.028\n",
      "epoch: 278, loss: 0.028\n",
      "epoch: 279, loss: 0.028\n",
      "epoch: 280, loss: 0.028\n",
      "epoch: 281, loss: 0.028\n",
      "epoch: 282, loss: 0.028\n",
      "epoch: 283, loss: 0.028\n",
      "epoch: 284, loss: 0.028\n",
      "epoch: 285, loss: 0.028\n",
      "epoch: 286, loss: 0.028\n",
      "epoch: 287, loss: 0.028\n",
      "epoch: 288, loss: 0.028\n",
      "epoch: 289, loss: 0.028\n",
      "epoch: 290, loss: 0.028\n",
      "epoch: 291, loss: 0.028\n",
      "epoch: 292, loss: 0.028\n",
      "epoch: 293, loss: 0.028\n",
      "epoch: 294, loss: 0.028\n",
      "epoch: 295, loss: 0.028\n",
      "epoch: 296, loss: 0.028\n",
      "epoch: 297, loss: 0.028\n",
      "epoch: 298, loss: 0.028\n",
      "epoch: 299, loss: 0.028\n",
      "epoch: 300, loss: 0.028\n",
      "epoch: 301, loss: 0.028\n",
      "epoch: 302, loss: 0.028\n",
      "epoch: 303, loss: 0.028\n",
      "epoch: 304, loss: 0.028\n",
      "epoch: 305, loss: 0.028\n",
      "epoch: 306, loss: 0.028\n",
      "epoch: 307, loss: 0.028\n",
      "epoch: 308, loss: 0.028\n",
      "epoch: 309, loss: 0.028\n",
      "epoch: 310, loss: 0.028\n",
      "epoch: 311, loss: 0.028\n",
      "epoch: 312, loss: 0.028\n",
      "epoch: 313, loss: 0.028\n",
      "epoch: 314, loss: 0.028\n",
      "epoch: 315, loss: 0.028\n",
      "epoch: 316, loss: 0.028\n",
      "epoch: 317, loss: 0.028\n",
      "epoch: 318, loss: 0.028\n",
      "epoch: 319, loss: 0.028\n",
      "epoch: 320, loss: 0.028\n",
      "epoch: 321, loss: 0.028\n",
      "epoch: 322, loss: 0.028\n",
      "epoch: 323, loss: 0.028\n",
      "epoch: 324, loss: 0.028\n",
      "epoch: 325, loss: 0.028\n",
      "epoch: 326, loss: 0.028\n",
      "epoch: 327, loss: 0.028\n",
      "epoch: 328, loss: 0.028\n",
      "epoch: 329, loss: 0.028\n",
      "epoch: 330, loss: 0.028\n",
      "epoch: 331, loss: 0.028\n",
      "epoch: 332, loss: 0.028\n",
      "epoch: 333, loss: 0.028\n",
      "epoch: 334, loss: 0.028\n",
      "epoch: 335, loss: 0.028\n",
      "epoch: 336, loss: 0.028\n",
      "epoch: 337, loss: 0.028\n",
      "epoch: 338, loss: 0.028\n",
      "epoch: 339, loss: 0.028\n",
      "epoch: 340, loss: 0.028\n",
      "epoch: 341, loss: 0.028\n",
      "epoch: 342, loss: 0.028\n",
      "epoch: 343, loss: 0.028\n",
      "epoch: 344, loss: 0.028\n",
      "epoch: 345, loss: 0.028\n",
      "epoch: 346, loss: 0.028\n",
      "epoch: 347, loss: 0.028\n",
      "epoch: 348, loss: 0.028\n",
      "epoch: 349, loss: 0.028\n",
      "epoch: 350, loss: 0.028\n",
      "epoch: 351, loss: 0.028\n",
      "epoch: 352, loss: 0.028\n",
      "epoch: 353, loss: 0.028\n",
      "epoch: 354, loss: 0.028\n",
      "epoch: 355, loss: 0.028\n",
      "epoch: 356, loss: 0.028\n",
      "epoch: 357, loss: 0.028\n",
      "epoch: 358, loss: 0.028\n",
      "epoch: 359, loss: 0.028\n",
      "epoch: 360, loss: 0.028\n",
      "epoch: 361, loss: 0.028\n",
      "epoch: 362, loss: 0.028\n",
      "epoch: 363, loss: 0.028\n",
      "epoch: 364, loss: 0.027\n",
      "epoch: 365, loss: 0.027\n",
      "epoch: 366, loss: 0.027\n",
      "epoch: 367, loss: 0.027\n",
      "epoch: 368, loss: 0.027\n",
      "epoch: 369, loss: 0.027\n",
      "epoch: 370, loss: 0.027\n",
      "epoch: 371, loss: 0.027\n",
      "epoch: 372, loss: 0.027\n",
      "epoch: 373, loss: 0.027\n",
      "epoch: 374, loss: 0.027\n",
      "epoch: 375, loss: 0.027\n",
      "epoch: 376, loss: 0.027\n",
      "epoch: 377, loss: 0.027\n",
      "epoch: 378, loss: 0.027\n",
      "epoch: 379, loss: 0.027\n",
      "epoch: 380, loss: 0.027\n",
      "epoch: 381, loss: 0.027\n",
      "epoch: 382, loss: 0.027\n",
      "epoch: 383, loss: 0.027\n",
      "epoch: 384, loss: 0.027\n",
      "epoch: 385, loss: 0.027\n",
      "epoch: 386, loss: 0.027\n",
      "epoch: 387, loss: 0.027\n",
      "epoch: 388, loss: 0.027\n",
      "epoch: 389, loss: 0.027\n",
      "epoch: 390, loss: 0.027\n",
      "epoch: 391, loss: 0.027\n",
      "epoch: 392, loss: 0.027\n",
      "epoch: 393, loss: 0.027\n",
      "epoch: 394, loss: 0.027\n",
      "epoch: 395, loss: 0.027\n",
      "epoch: 396, loss: 0.027\n",
      "epoch: 397, loss: 0.027\n",
      "epoch: 398, loss: 0.027\n",
      "epoch: 399, loss: 0.027\n",
      "epoch: 400, loss: 0.027\n",
      "epoch: 401, loss: 0.027\n",
      "epoch: 402, loss: 0.027\n",
      "epoch: 403, loss: 0.027\n",
      "epoch: 404, loss: 0.027\n",
      "epoch: 405, loss: 0.027\n",
      "epoch: 406, loss: 0.027\n",
      "epoch: 407, loss: 0.027\n",
      "epoch: 408, loss: 0.027\n",
      "epoch: 409, loss: 0.027\n",
      "epoch: 410, loss: 0.027\n",
      "epoch: 411, loss: 0.027\n",
      "epoch: 412, loss: 0.027\n",
      "epoch: 413, loss: 0.027\n",
      "epoch: 414, loss: 0.027\n",
      "epoch: 415, loss: 0.027\n",
      "epoch: 416, loss: 0.027\n",
      "epoch: 417, loss: 0.027\n",
      "epoch: 418, loss: 0.027\n",
      "epoch: 419, loss: 0.027\n",
      "epoch: 420, loss: 0.027\n",
      "epoch: 421, loss: 0.027\n",
      "epoch: 422, loss: 0.027\n",
      "epoch: 423, loss: 0.027\n",
      "epoch: 424, loss: 0.027\n",
      "epoch: 425, loss: 0.027\n",
      "epoch: 426, loss: 0.027\n",
      "epoch: 427, loss: 0.027\n",
      "epoch: 428, loss: 0.027\n",
      "epoch: 429, loss: 0.027\n",
      "epoch: 430, loss: 0.027\n",
      "epoch: 431, loss: 0.027\n",
      "epoch: 432, loss: 0.027\n",
      "epoch: 433, loss: 0.027\n",
      "epoch: 434, loss: 0.027\n",
      "epoch: 435, loss: 0.027\n",
      "epoch: 436, loss: 0.027\n",
      "epoch: 437, loss: 0.027\n",
      "epoch: 438, loss: 0.027\n",
      "epoch: 439, loss: 0.027\n",
      "epoch: 440, loss: 0.027\n",
      "epoch: 441, loss: 0.027\n",
      "epoch: 442, loss: 0.027\n",
      "epoch: 443, loss: 0.027\n",
      "epoch: 444, loss: 0.027\n",
      "epoch: 445, loss: 0.027\n",
      "epoch: 446, loss: 0.027\n",
      "epoch: 447, loss: 0.027\n",
      "epoch: 448, loss: 0.027\n",
      "epoch: 449, loss: 0.027\n",
      "epoch: 450, loss: 0.027\n",
      "epoch: 451, loss: 0.027\n",
      "epoch: 452, loss: 0.027\n",
      "epoch: 453, loss: 0.027\n",
      "epoch: 454, loss: 0.027\n",
      "epoch: 455, loss: 0.027\n",
      "epoch: 456, loss: 0.027\n",
      "epoch: 457, loss: 0.027\n",
      "epoch: 458, loss: 0.027\n",
      "epoch: 459, loss: 0.027\n",
      "epoch: 460, loss: 0.027\n",
      "epoch: 461, loss: 0.027\n",
      "epoch: 462, loss: 0.027\n",
      "epoch: 463, loss: 0.027\n",
      "epoch: 464, loss: 0.027\n",
      "epoch: 465, loss: 0.027\n",
      "epoch: 466, loss: 0.027\n",
      "epoch: 467, loss: 0.027\n",
      "epoch: 468, loss: 0.027\n",
      "epoch: 469, loss: 0.027\n",
      "epoch: 470, loss: 0.027\n",
      "epoch: 471, loss: 0.027\n",
      "epoch: 472, loss: 0.027\n",
      "epoch: 473, loss: 0.027\n",
      "epoch: 474, loss: 0.027\n",
      "epoch: 475, loss: 0.027\n",
      "epoch: 476, loss: 0.027\n",
      "epoch: 477, loss: 0.027\n",
      "epoch: 478, loss: 0.027\n",
      "epoch: 479, loss: 0.027\n",
      "epoch: 480, loss: 0.027\n",
      "epoch: 481, loss: 0.027\n",
      "epoch: 482, loss: 0.027\n",
      "epoch: 483, loss: 0.027\n",
      "epoch: 484, loss: 0.027\n",
      "epoch: 485, loss: 0.027\n",
      "epoch: 486, loss: 0.027\n",
      "epoch: 487, loss: 0.027\n",
      "epoch: 488, loss: 0.027\n",
      "epoch: 489, loss: 0.027\n",
      "epoch: 490, loss: 0.027\n",
      "epoch: 491, loss: 0.027\n",
      "epoch: 492, loss: 0.027\n",
      "epoch: 493, loss: 0.027\n",
      "epoch: 494, loss: 0.027\n",
      "epoch: 495, loss: 0.027\n",
      "epoch: 496, loss: 0.027\n",
      "epoch: 497, loss: 0.027\n",
      "epoch: 498, loss: 0.027\n",
      "epoch: 499, loss: 0.027\n",
      "epoch: 500, loss: 0.027\n",
      "epoch: 501, loss: 0.027\n",
      "epoch: 502, loss: 0.027\n",
      "epoch: 503, loss: 0.027\n",
      "epoch: 504, loss: 0.027\n",
      "epoch: 505, loss: 0.027\n",
      "epoch: 506, loss: 0.027\n",
      "epoch: 507, loss: 0.027\n",
      "epoch: 508, loss: 0.027\n",
      "epoch: 509, loss: 0.027\n",
      "epoch: 510, loss: 0.027\n",
      "epoch: 511, loss: 0.027\n",
      "epoch: 512, loss: 0.027\n",
      "epoch: 513, loss: 0.027\n",
      "epoch: 514, loss: 0.027\n",
      "epoch: 515, loss: 0.027\n",
      "epoch: 516, loss: 0.027\n",
      "epoch: 517, loss: 0.027\n",
      "epoch: 518, loss: 0.027\n",
      "epoch: 519, loss: 0.027\n",
      "epoch: 520, loss: 0.027\n",
      "epoch: 521, loss: 0.027\n",
      "epoch: 522, loss: 0.027\n",
      "epoch: 523, loss: 0.027\n",
      "epoch: 524, loss: 0.027\n",
      "epoch: 525, loss: 0.027\n",
      "epoch: 526, loss: 0.027\n",
      "epoch: 527, loss: 0.027\n",
      "epoch: 528, loss: 0.027\n",
      "epoch: 529, loss: 0.027\n",
      "epoch: 530, loss: 0.027\n",
      "epoch: 531, loss: 0.027\n",
      "epoch: 532, loss: 0.027\n",
      "epoch: 533, loss: 0.027\n",
      "epoch: 534, loss: 0.027\n",
      "epoch: 535, loss: 0.027\n",
      "epoch: 536, loss: 0.027\n",
      "epoch: 537, loss: 0.027\n",
      "epoch: 538, loss: 0.027\n",
      "epoch: 539, loss: 0.027\n",
      "epoch: 540, loss: 0.027\n",
      "epoch: 541, loss: 0.027\n",
      "epoch: 542, loss: 0.027\n",
      "epoch: 543, loss: 0.027\n",
      "epoch: 544, loss: 0.027\n",
      "epoch: 545, loss: 0.027\n",
      "epoch: 546, loss: 0.027\n",
      "epoch: 547, loss: 0.027\n",
      "epoch: 548, loss: 0.027\n",
      "epoch: 549, loss: 0.027\n",
      "epoch: 550, loss: 0.027\n",
      "epoch: 551, loss: 0.027\n",
      "epoch: 552, loss: 0.027\n",
      "epoch: 553, loss: 0.027\n",
      "epoch: 554, loss: 0.027\n",
      "epoch: 555, loss: 0.027\n",
      "epoch: 556, loss: 0.027\n",
      "epoch: 557, loss: 0.027\n",
      "epoch: 558, loss: 0.027\n",
      "epoch: 559, loss: 0.027\n",
      "epoch: 560, loss: 0.027\n",
      "epoch: 561, loss: 0.027\n",
      "epoch: 562, loss: 0.027\n",
      "epoch: 563, loss: 0.027\n",
      "epoch: 564, loss: 0.027\n",
      "epoch: 565, loss: 0.027\n",
      "epoch: 566, loss: 0.027\n",
      "epoch: 567, loss: 0.027\n",
      "epoch: 568, loss: 0.027\n",
      "epoch: 569, loss: 0.027\n",
      "epoch: 570, loss: 0.027\n",
      "epoch: 571, loss: 0.027\n",
      "epoch: 572, loss: 0.027\n",
      "epoch: 573, loss: 0.027\n",
      "epoch: 574, loss: 0.027\n",
      "epoch: 575, loss: 0.027\n",
      "epoch: 576, loss: 0.027\n",
      "epoch: 577, loss: 0.027\n",
      "epoch: 578, loss: 0.027\n",
      "epoch: 579, loss: 0.027\n",
      "epoch: 580, loss: 0.027\n",
      "epoch: 581, loss: 0.027\n",
      "epoch: 582, loss: 0.027\n",
      "epoch: 583, loss: 0.027\n",
      "epoch: 584, loss: 0.027\n",
      "epoch: 585, loss: 0.027\n",
      "epoch: 586, loss: 0.027\n",
      "epoch: 587, loss: 0.027\n",
      "epoch: 588, loss: 0.027\n",
      "epoch: 589, loss: 0.027\n",
      "epoch: 590, loss: 0.027\n",
      "epoch: 591, loss: 0.027\n",
      "epoch: 592, loss: 0.027\n",
      "epoch: 593, loss: 0.027\n",
      "epoch: 594, loss: 0.027\n",
      "epoch: 595, loss: 0.027\n",
      "epoch: 596, loss: 0.027\n",
      "epoch: 597, loss: 0.027\n",
      "epoch: 598, loss: 0.027\n",
      "epoch: 599, loss: 0.027\n",
      "epoch: 600, loss: 0.027\n",
      "epoch: 601, loss: 0.027\n",
      "epoch: 602, loss: 0.027\n",
      "epoch: 603, loss: 0.027\n",
      "epoch: 604, loss: 0.027\n",
      "epoch: 605, loss: 0.027\n",
      "epoch: 606, loss: 0.027\n",
      "epoch: 607, loss: 0.027\n",
      "epoch: 608, loss: 0.027\n",
      "epoch: 609, loss: 0.027\n",
      "epoch: 610, loss: 0.027\n",
      "epoch: 611, loss: 0.027\n",
      "epoch: 612, loss: 0.027\n",
      "epoch: 613, loss: 0.027\n",
      "epoch: 614, loss: 0.027\n",
      "epoch: 615, loss: 0.027\n",
      "epoch: 616, loss: 0.027\n",
      "epoch: 617, loss: 0.027\n",
      "epoch: 618, loss: 0.027\n",
      "epoch: 619, loss: 0.027\n",
      "epoch: 620, loss: 0.027\n",
      "epoch: 621, loss: 0.027\n",
      "epoch: 622, loss: 0.027\n",
      "epoch: 623, loss: 0.027\n",
      "epoch: 624, loss: 0.027\n",
      "epoch: 625, loss: 0.027\n",
      "epoch: 626, loss: 0.027\n",
      "epoch: 627, loss: 0.027\n",
      "epoch: 628, loss: 0.027\n",
      "epoch: 629, loss: 0.027\n",
      "epoch: 630, loss: 0.027\n",
      "epoch: 631, loss: 0.027\n",
      "epoch: 632, loss: 0.027\n",
      "epoch: 633, loss: 0.027\n",
      "epoch: 634, loss: 0.027\n",
      "epoch: 635, loss: 0.027\n",
      "epoch: 636, loss: 0.027\n",
      "epoch: 637, loss: 0.027\n",
      "epoch: 638, loss: 0.027\n",
      "epoch: 639, loss: 0.027\n",
      "epoch: 640, loss: 0.027\n",
      "epoch: 641, loss: 0.027\n",
      "epoch: 642, loss: 0.027\n",
      "epoch: 643, loss: 0.027\n",
      "epoch: 644, loss: 0.027\n",
      "epoch: 645, loss: 0.027\n",
      "epoch: 646, loss: 0.027\n",
      "epoch: 647, loss: 0.027\n",
      "epoch: 648, loss: 0.027\n",
      "epoch: 649, loss: 0.027\n",
      "epoch: 650, loss: 0.027\n",
      "epoch: 651, loss: 0.027\n",
      "epoch: 652, loss: 0.027\n",
      "epoch: 653, loss: 0.027\n",
      "epoch: 654, loss: 0.027\n",
      "epoch: 655, loss: 0.027\n",
      "epoch: 656, loss: 0.027\n",
      "epoch: 657, loss: 0.027\n",
      "epoch: 658, loss: 0.027\n",
      "epoch: 659, loss: 0.027\n",
      "epoch: 660, loss: 0.027\n",
      "epoch: 661, loss: 0.027\n",
      "epoch: 662, loss: 0.027\n",
      "epoch: 663, loss: 0.027\n",
      "epoch: 664, loss: 0.027\n",
      "epoch: 665, loss: 0.027\n",
      "epoch: 666, loss: 0.027\n",
      "epoch: 667, loss: 0.027\n",
      "epoch: 668, loss: 0.027\n",
      "epoch: 669, loss: 0.027\n",
      "epoch: 670, loss: 0.027\n",
      "epoch: 671, loss: 0.027\n",
      "epoch: 672, loss: 0.027\n",
      "epoch: 673, loss: 0.027\n",
      "epoch: 674, loss: 0.027\n",
      "epoch: 675, loss: 0.027\n",
      "epoch: 676, loss: 0.027\n",
      "epoch: 677, loss: 0.027\n",
      "epoch: 678, loss: 0.027\n",
      "epoch: 679, loss: 0.027\n",
      "epoch: 680, loss: 0.027\n",
      "epoch: 681, loss: 0.027\n",
      "epoch: 682, loss: 0.027\n",
      "epoch: 683, loss: 0.027\n",
      "epoch: 684, loss: 0.027\n",
      "epoch: 685, loss: 0.027\n",
      "epoch: 686, loss: 0.027\n",
      "epoch: 687, loss: 0.027\n",
      "epoch: 688, loss: 0.027\n",
      "epoch: 689, loss: 0.027\n",
      "epoch: 690, loss: 0.027\n",
      "epoch: 691, loss: 0.027\n",
      "epoch: 692, loss: 0.027\n",
      "epoch: 693, loss: 0.027\n",
      "epoch: 694, loss: 0.027\n",
      "epoch: 695, loss: 0.027\n",
      "epoch: 696, loss: 0.027\n",
      "epoch: 697, loss: 0.027\n",
      "epoch: 698, loss: 0.027\n",
      "epoch: 699, loss: 0.027\n",
      "epoch: 700, loss: 0.027\n",
      "epoch: 701, loss: 0.027\n",
      "epoch: 702, loss: 0.027\n",
      "epoch: 703, loss: 0.027\n",
      "epoch: 704, loss: 0.027\n",
      "epoch: 705, loss: 0.027\n",
      "epoch: 706, loss: 0.027\n",
      "epoch: 707, loss: 0.027\n",
      "epoch: 708, loss: 0.027\n",
      "epoch: 709, loss: 0.027\n",
      "epoch: 710, loss: 0.027\n",
      "epoch: 711, loss: 0.027\n",
      "epoch: 712, loss: 0.027\n",
      "epoch: 713, loss: 0.027\n",
      "epoch: 714, loss: 0.027\n",
      "epoch: 715, loss: 0.027\n",
      "epoch: 716, loss: 0.027\n",
      "epoch: 717, loss: 0.027\n",
      "epoch: 718, loss: 0.027\n",
      "epoch: 719, loss: 0.027\n",
      "epoch: 720, loss: 0.027\n",
      "epoch: 721, loss: 0.027\n",
      "epoch: 722, loss: 0.027\n",
      "epoch: 723, loss: 0.027\n",
      "epoch: 724, loss: 0.027\n",
      "epoch: 725, loss: 0.027\n",
      "epoch: 726, loss: 0.027\n",
      "epoch: 727, loss: 0.027\n",
      "epoch: 728, loss: 0.027\n",
      "epoch: 729, loss: 0.027\n",
      "epoch: 730, loss: 0.027\n",
      "epoch: 731, loss: 0.027\n",
      "epoch: 732, loss: 0.027\n",
      "epoch: 733, loss: 0.027\n",
      "epoch: 734, loss: 0.027\n",
      "epoch: 735, loss: 0.027\n",
      "epoch: 736, loss: 0.027\n",
      "epoch: 737, loss: 0.027\n",
      "epoch: 738, loss: 0.027\n",
      "epoch: 739, loss: 0.027\n",
      "epoch: 740, loss: 0.027\n",
      "epoch: 741, loss: 0.027\n",
      "epoch: 742, loss: 0.027\n",
      "epoch: 743, loss: 0.027\n",
      "epoch: 744, loss: 0.027\n",
      "epoch: 745, loss: 0.027\n",
      "epoch: 746, loss: 0.027\n",
      "epoch: 747, loss: 0.027\n",
      "epoch: 748, loss: 0.027\n",
      "epoch: 749, loss: 0.027\n",
      "epoch: 750, loss: 0.027\n",
      "epoch: 751, loss: 0.027\n",
      "epoch: 752, loss: 0.027\n",
      "epoch: 753, loss: 0.027\n",
      "epoch: 754, loss: 0.027\n",
      "epoch: 755, loss: 0.027\n",
      "epoch: 756, loss: 0.027\n",
      "epoch: 757, loss: 0.027\n",
      "epoch: 758, loss: 0.027\n",
      "epoch: 759, loss: 0.027\n",
      "epoch: 760, loss: 0.027\n",
      "epoch: 761, loss: 0.027\n",
      "epoch: 762, loss: 0.027\n",
      "epoch: 763, loss: 0.027\n",
      "epoch: 764, loss: 0.027\n",
      "epoch: 765, loss: 0.027\n",
      "epoch: 766, loss: 0.027\n",
      "epoch: 767, loss: 0.027\n",
      "epoch: 768, loss: 0.027\n",
      "epoch: 769, loss: 0.027\n",
      "epoch: 770, loss: 0.027\n",
      "epoch: 771, loss: 0.027\n",
      "epoch: 772, loss: 0.027\n",
      "epoch: 773, loss: 0.027\n",
      "epoch: 774, loss: 0.027\n",
      "epoch: 775, loss: 0.027\n",
      "epoch: 776, loss: 0.027\n",
      "epoch: 777, loss: 0.027\n",
      "epoch: 778, loss: 0.027\n",
      "epoch: 779, loss: 0.027\n",
      "epoch: 780, loss: 0.027\n",
      "epoch: 781, loss: 0.027\n",
      "epoch: 782, loss: 0.027\n",
      "epoch: 783, loss: 0.027\n",
      "epoch: 784, loss: 0.027\n",
      "epoch: 785, loss: 0.027\n",
      "epoch: 786, loss: 0.027\n",
      "epoch: 787, loss: 0.027\n",
      "epoch: 788, loss: 0.027\n",
      "epoch: 789, loss: 0.027\n",
      "epoch: 790, loss: 0.027\n",
      "epoch: 791, loss: 0.027\n",
      "epoch: 792, loss: 0.027\n",
      "epoch: 793, loss: 0.027\n",
      "epoch: 794, loss: 0.027\n",
      "epoch: 795, loss: 0.027\n",
      "epoch: 796, loss: 0.027\n",
      "epoch: 797, loss: 0.027\n",
      "epoch: 798, loss: 0.027\n",
      "epoch: 799, loss: 0.027\n",
      "epoch: 800, loss: 0.027\n",
      "epoch: 801, loss: 0.027\n",
      "epoch: 802, loss: 0.027\n",
      "epoch: 803, loss: 0.027\n",
      "epoch: 804, loss: 0.027\n",
      "epoch: 805, loss: 0.027\n",
      "epoch: 806, loss: 0.027\n",
      "epoch: 807, loss: 0.027\n",
      "epoch: 808, loss: 0.027\n",
      "epoch: 809, loss: 0.027\n",
      "epoch: 810, loss: 0.027\n",
      "epoch: 811, loss: 0.027\n",
      "epoch: 812, loss: 0.027\n",
      "epoch: 813, loss: 0.027\n",
      "epoch: 814, loss: 0.027\n",
      "epoch: 815, loss: 0.027\n",
      "epoch: 816, loss: 0.027\n",
      "epoch: 817, loss: 0.027\n",
      "epoch: 818, loss: 0.027\n",
      "epoch: 819, loss: 0.027\n",
      "epoch: 820, loss: 0.027\n",
      "epoch: 821, loss: 0.027\n",
      "epoch: 822, loss: 0.027\n",
      "epoch: 823, loss: 0.027\n",
      "epoch: 824, loss: 0.027\n",
      "epoch: 825, loss: 0.027\n",
      "epoch: 826, loss: 0.027\n",
      "epoch: 827, loss: 0.027\n",
      "epoch: 828, loss: 0.027\n",
      "epoch: 829, loss: 0.027\n",
      "epoch: 830, loss: 0.027\n",
      "epoch: 831, loss: 0.027\n",
      "epoch: 832, loss: 0.027\n",
      "epoch: 833, loss: 0.027\n",
      "epoch: 834, loss: 0.027\n",
      "epoch: 835, loss: 0.027\n",
      "epoch: 836, loss: 0.027\n",
      "epoch: 837, loss: 0.027\n",
      "epoch: 838, loss: 0.027\n",
      "epoch: 839, loss: 0.027\n",
      "epoch: 840, loss: 0.027\n",
      "epoch: 841, loss: 0.027\n",
      "epoch: 842, loss: 0.027\n",
      "epoch: 843, loss: 0.027\n",
      "epoch: 844, loss: 0.027\n",
      "epoch: 845, loss: 0.027\n",
      "epoch: 846, loss: 0.027\n",
      "epoch: 847, loss: 0.027\n",
      "epoch: 848, loss: 0.027\n",
      "epoch: 849, loss: 0.027\n",
      "epoch: 850, loss: 0.027\n",
      "epoch: 851, loss: 0.027\n",
      "epoch: 852, loss: 0.027\n",
      "epoch: 853, loss: 0.027\n",
      "epoch: 854, loss: 0.027\n",
      "epoch: 855, loss: 0.027\n",
      "epoch: 856, loss: 0.027\n",
      "epoch: 857, loss: 0.027\n",
      "epoch: 858, loss: 0.027\n",
      "epoch: 859, loss: 0.027\n",
      "epoch: 860, loss: 0.027\n",
      "epoch: 861, loss: 0.027\n",
      "epoch: 862, loss: 0.027\n",
      "epoch: 863, loss: 0.027\n",
      "epoch: 864, loss: 0.027\n",
      "epoch: 865, loss: 0.027\n",
      "epoch: 866, loss: 0.027\n",
      "epoch: 867, loss: 0.027\n",
      "epoch: 868, loss: 0.027\n",
      "epoch: 869, loss: 0.027\n",
      "epoch: 870, loss: 0.027\n",
      "epoch: 871, loss: 0.027\n",
      "epoch: 872, loss: 0.027\n",
      "epoch: 873, loss: 0.027\n",
      "epoch: 874, loss: 0.027\n",
      "epoch: 875, loss: 0.027\n",
      "epoch: 876, loss: 0.027\n",
      "epoch: 877, loss: 0.027\n",
      "epoch: 878, loss: 0.027\n",
      "epoch: 879, loss: 0.027\n",
      "epoch: 880, loss: 0.027\n",
      "epoch: 881, loss: 0.027\n",
      "epoch: 882, loss: 0.027\n",
      "epoch: 883, loss: 0.027\n",
      "epoch: 884, loss: 0.027\n",
      "epoch: 885, loss: 0.027\n",
      "epoch: 886, loss: 0.027\n",
      "epoch: 887, loss: 0.027\n",
      "epoch: 888, loss: 0.027\n",
      "epoch: 889, loss: 0.027\n",
      "epoch: 890, loss: 0.027\n",
      "epoch: 891, loss: 0.027\n",
      "epoch: 892, loss: 0.027\n",
      "epoch: 893, loss: 0.027\n",
      "epoch: 894, loss: 0.027\n",
      "epoch: 895, loss: 0.027\n",
      "epoch: 896, loss: 0.027\n",
      "epoch: 897, loss: 0.027\n",
      "epoch: 898, loss: 0.027\n",
      "epoch: 899, loss: 0.027\n",
      "epoch: 900, loss: 0.027\n",
      "epoch: 901, loss: 0.027\n",
      "epoch: 902, loss: 0.027\n",
      "epoch: 903, loss: 0.027\n",
      "epoch: 904, loss: 0.027\n",
      "epoch: 905, loss: 0.027\n",
      "epoch: 906, loss: 0.027\n",
      "epoch: 907, loss: 0.027\n",
      "epoch: 908, loss: 0.027\n",
      "epoch: 909, loss: 0.027\n",
      "epoch: 910, loss: 0.027\n",
      "epoch: 911, loss: 0.027\n",
      "epoch: 912, loss: 0.027\n",
      "epoch: 913, loss: 0.027\n",
      "epoch: 914, loss: 0.027\n",
      "epoch: 915, loss: 0.027\n",
      "epoch: 916, loss: 0.027\n",
      "epoch: 917, loss: 0.027\n",
      "epoch: 918, loss: 0.027\n",
      "epoch: 919, loss: 0.027\n",
      "epoch: 920, loss: 0.027\n",
      "epoch: 921, loss: 0.027\n",
      "epoch: 922, loss: 0.027\n",
      "epoch: 923, loss: 0.027\n",
      "epoch: 924, loss: 0.027\n",
      "epoch: 925, loss: 0.027\n",
      "epoch: 926, loss: 0.027\n",
      "epoch: 927, loss: 0.027\n",
      "epoch: 928, loss: 0.027\n",
      "epoch: 929, loss: 0.027\n",
      "epoch: 930, loss: 0.027\n",
      "epoch: 931, loss: 0.027\n",
      "epoch: 932, loss: 0.027\n",
      "epoch: 933, loss: 0.027\n",
      "epoch: 934, loss: 0.027\n",
      "epoch: 935, loss: 0.027\n",
      "epoch: 936, loss: 0.027\n",
      "epoch: 937, loss: 0.027\n",
      "epoch: 938, loss: 0.027\n",
      "epoch: 939, loss: 0.027\n",
      "epoch: 940, loss: 0.027\n",
      "epoch: 941, loss: 0.027\n",
      "epoch: 942, loss: 0.027\n",
      "epoch: 943, loss: 0.027\n",
      "epoch: 944, loss: 0.027\n",
      "epoch: 945, loss: 0.027\n",
      "epoch: 946, loss: 0.027\n",
      "epoch: 947, loss: 0.027\n",
      "epoch: 948, loss: 0.027\n",
      "epoch: 949, loss: 0.027\n",
      "epoch: 950, loss: 0.027\n",
      "epoch: 951, loss: 0.027\n",
      "epoch: 952, loss: 0.027\n",
      "epoch: 953, loss: 0.027\n",
      "epoch: 954, loss: 0.027\n",
      "epoch: 955, loss: 0.027\n",
      "epoch: 956, loss: 0.027\n",
      "epoch: 957, loss: 0.027\n",
      "epoch: 958, loss: 0.027\n",
      "epoch: 959, loss: 0.027\n",
      "epoch: 960, loss: 0.027\n",
      "epoch: 961, loss: 0.027\n",
      "epoch: 962, loss: 0.027\n",
      "epoch: 963, loss: 0.027\n",
      "epoch: 964, loss: 0.027\n",
      "epoch: 965, loss: 0.027\n",
      "epoch: 966, loss: 0.027\n",
      "epoch: 967, loss: 0.027\n",
      "epoch: 968, loss: 0.027\n",
      "epoch: 969, loss: 0.027\n",
      "epoch: 970, loss: 0.027\n",
      "epoch: 971, loss: 0.027\n",
      "epoch: 972, loss: 0.027\n",
      "epoch: 973, loss: 0.027\n",
      "epoch: 974, loss: 0.027\n",
      "epoch: 975, loss: 0.027\n",
      "epoch: 976, loss: 0.027\n",
      "epoch: 977, loss: 0.027\n",
      "epoch: 978, loss: 0.027\n",
      "epoch: 979, loss: 0.027\n",
      "epoch: 980, loss: 0.027\n",
      "epoch: 981, loss: 0.027\n",
      "epoch: 982, loss: 0.027\n",
      "epoch: 983, loss: 0.027\n",
      "epoch: 984, loss: 0.027\n",
      "epoch: 985, loss: 0.027\n",
      "epoch: 986, loss: 0.027\n",
      "epoch: 987, loss: 0.027\n",
      "epoch: 988, loss: 0.027\n",
      "epoch: 989, loss: 0.027\n",
      "epoch: 990, loss: 0.027\n",
      "epoch: 991, loss: 0.027\n",
      "epoch: 992, loss: 0.027\n",
      "epoch: 993, loss: 0.027\n",
      "epoch: 994, loss: 0.027\n",
      "epoch: 995, loss: 0.027\n",
      "epoch: 996, loss: 0.027\n",
      "epoch: 997, loss: 0.027\n",
      "epoch: 998, loss: 0.027\n",
      "epoch: 999, loss: 0.027\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(1000):\n",
    "\n",
    "    \n",
    "\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    \n",
    "\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    print(f\"epoch: {t}, loss: {loss:.3f}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 1, epoch: 0, loss: 0.177\n",
      "model: 1, epoch: 1, loss: 0.160\n",
      "model: 1, epoch: 2, loss: 0.144\n",
      "model: 1, epoch: 3, loss: 0.129\n",
      "model: 1, epoch: 4, loss: 0.116\n",
      "model: 1, epoch: 5, loss: 0.103\n",
      "model: 1, epoch: 6, loss: 0.092\n",
      "model: 1, epoch: 7, loss: 0.082\n",
      "model: 1, epoch: 8, loss: 0.073\n",
      "model: 1, epoch: 9, loss: 0.066\n",
      "model: 1, epoch: 10, loss: 0.059\n",
      "model: 1, epoch: 11, loss: 0.053\n",
      "model: 1, epoch: 12, loss: 0.048\n",
      "model: 1, epoch: 13, loss: 0.044\n",
      "model: 1, epoch: 14, loss: 0.040\n",
      "model: 1, epoch: 15, loss: 0.037\n",
      "model: 1, epoch: 16, loss: 0.035\n",
      "model: 1, epoch: 17, loss: 0.033\n",
      "model: 1, epoch: 18, loss: 0.032\n",
      "model: 1, epoch: 19, loss: 0.030\n",
      "model: 1, epoch: 20, loss: 0.030\n",
      "model: 1, epoch: 21, loss: 0.029\n",
      "model: 1, epoch: 22, loss: 0.028\n",
      "model: 1, epoch: 23, loss: 0.028\n",
      "model: 1, epoch: 24, loss: 0.028\n",
      "model: 1, epoch: 25, loss: 0.028\n",
      "model: 1, epoch: 26, loss: 0.027\n",
      "model: 1, epoch: 27, loss: 0.027\n",
      "model: 1, epoch: 28, loss: 0.028\n",
      "model: 1, epoch: 29, loss: 0.028\n",
      "model: 1, epoch: 30, loss: 0.028\n",
      "model: 1, epoch: 31, loss: 0.028\n",
      "model: 1, epoch: 32, loss: 0.028\n",
      "model: 1, epoch: 33, loss: 0.028\n",
      "model: 1, epoch: 34, loss: 0.028\n",
      "model: 1, epoch: 35, loss: 0.028\n",
      "model: 1, epoch: 36, loss: 0.027\n",
      "model: 1, epoch: 37, loss: 0.027\n",
      "model: 1, epoch: 38, loss: 0.027\n",
      "model: 1, epoch: 39, loss: 0.027\n",
      "model: 1, epoch: 40, loss: 0.026\n",
      "model: 1, epoch: 41, loss: 0.026\n",
      "model: 1, epoch: 42, loss: 0.026\n",
      "model: 1, epoch: 43, loss: 0.025\n",
      "model: 1, epoch: 44, loss: 0.025\n",
      "model: 1, epoch: 45, loss: 0.025\n",
      "model: 1, epoch: 46, loss: 0.024\n",
      "model: 1, epoch: 47, loss: 0.024\n",
      "model: 1, epoch: 48, loss: 0.024\n",
      "model: 1, epoch: 49, loss: 0.024\n",
      "model: 1, epoch: 50, loss: 0.023\n",
      "model: 1, epoch: 51, loss: 0.023\n",
      "model: 1, epoch: 52, loss: 0.023\n",
      "model: 1, epoch: 53, loss: 0.023\n",
      "model: 1, epoch: 54, loss: 0.023\n",
      "model: 1, epoch: 55, loss: 0.023\n",
      "model: 1, epoch: 56, loss: 0.022\n",
      "model: 1, epoch: 57, loss: 0.022\n",
      "model: 1, epoch: 58, loss: 0.022\n",
      "model: 1, epoch: 59, loss: 0.022\n",
      "model: 1, epoch: 60, loss: 0.022\n",
      "model: 1, epoch: 61, loss: 0.022\n",
      "model: 1, epoch: 62, loss: 0.021\n",
      "model: 1, epoch: 63, loss: 0.021\n",
      "model: 1, epoch: 64, loss: 0.021\n",
      "model: 1, epoch: 65, loss: 0.021\n",
      "model: 1, epoch: 66, loss: 0.021\n",
      "model: 1, epoch: 67, loss: 0.021\n",
      "model: 1, epoch: 68, loss: 0.021\n",
      "model: 1, epoch: 69, loss: 0.020\n",
      "model: 1, epoch: 70, loss: 0.020\n",
      "model: 1, epoch: 71, loss: 0.020\n",
      "model: 1, epoch: 72, loss: 0.020\n",
      "model: 1, epoch: 73, loss: 0.020\n",
      "model: 1, epoch: 74, loss: 0.020\n",
      "model: 1, epoch: 75, loss: 0.019\n",
      "model: 1, epoch: 76, loss: 0.019\n",
      "model: 1, epoch: 77, loss: 0.019\n",
      "model: 1, epoch: 78, loss: 0.019\n",
      "model: 1, epoch: 79, loss: 0.019\n",
      "model: 1, epoch: 80, loss: 0.019\n",
      "model: 1, epoch: 81, loss: 0.019\n",
      "model: 1, epoch: 82, loss: 0.018\n",
      "model: 1, epoch: 83, loss: 0.018\n",
      "model: 1, epoch: 84, loss: 0.018\n",
      "model: 1, epoch: 85, loss: 0.018\n",
      "model: 1, epoch: 86, loss: 0.018\n",
      "model: 1, epoch: 87, loss: 0.018\n",
      "model: 1, epoch: 88, loss: 0.018\n",
      "model: 1, epoch: 89, loss: 0.017\n",
      "model: 1, epoch: 90, loss: 0.017\n",
      "model: 1, epoch: 91, loss: 0.017\n",
      "model: 1, epoch: 92, loss: 0.017\n",
      "model: 1, epoch: 93, loss: 0.017\n",
      "model: 1, epoch: 94, loss: 0.017\n",
      "model: 1, epoch: 95, loss: 0.017\n",
      "model: 1, epoch: 96, loss: 0.017\n",
      "model: 1, epoch: 97, loss: 0.016\n",
      "model: 1, epoch: 98, loss: 0.016\n",
      "model: 1, epoch: 99, loss: 0.016\n",
      "model: 1, epoch: 100, loss: 0.016\n",
      "model: 1, epoch: 101, loss: 0.016\n",
      "model: 1, epoch: 102, loss: 0.016\n",
      "model: 1, epoch: 103, loss: 0.016\n",
      "model: 1, epoch: 104, loss: 0.016\n",
      "model: 1, epoch: 105, loss: 0.015\n",
      "model: 1, epoch: 106, loss: 0.015\n",
      "model: 1, epoch: 107, loss: 0.015\n",
      "model: 1, epoch: 108, loss: 0.015\n",
      "model: 1, epoch: 109, loss: 0.015\n",
      "model: 1, epoch: 110, loss: 0.015\n",
      "model: 1, epoch: 111, loss: 0.015\n",
      "model: 1, epoch: 112, loss: 0.015\n",
      "model: 1, epoch: 113, loss: 0.015\n",
      "model: 1, epoch: 114, loss: 0.014\n",
      "model: 1, epoch: 115, loss: 0.014\n",
      "model: 1, epoch: 116, loss: 0.014\n",
      "model: 1, epoch: 117, loss: 0.014\n",
      "model: 1, epoch: 118, loss: 0.014\n",
      "model: 1, epoch: 119, loss: 0.014\n",
      "model: 1, epoch: 120, loss: 0.014\n",
      "model: 1, epoch: 121, loss: 0.014\n",
      "model: 1, epoch: 122, loss: 0.014\n",
      "model: 1, epoch: 123, loss: 0.014\n",
      "model: 1, epoch: 124, loss: 0.013\n",
      "model: 1, epoch: 125, loss: 0.013\n",
      "model: 1, epoch: 126, loss: 0.013\n",
      "model: 1, epoch: 127, loss: 0.013\n",
      "model: 1, epoch: 128, loss: 0.013\n",
      "model: 1, epoch: 129, loss: 0.013\n",
      "model: 1, epoch: 130, loss: 0.013\n",
      "model: 1, epoch: 131, loss: 0.013\n",
      "model: 1, epoch: 132, loss: 0.013\n",
      "model: 1, epoch: 133, loss: 0.013\n",
      "model: 1, epoch: 134, loss: 0.013\n",
      "model: 1, epoch: 135, loss: 0.012\n",
      "model: 1, epoch: 136, loss: 0.012\n",
      "model: 1, epoch: 137, loss: 0.012\n",
      "model: 1, epoch: 138, loss: 0.012\n",
      "model: 1, epoch: 139, loss: 0.012\n",
      "model: 1, epoch: 140, loss: 0.012\n",
      "model: 1, epoch: 141, loss: 0.012\n",
      "model: 1, epoch: 142, loss: 0.012\n",
      "model: 1, epoch: 143, loss: 0.012\n",
      "model: 1, epoch: 144, loss: 0.012\n",
      "model: 1, epoch: 145, loss: 0.012\n",
      "model: 1, epoch: 146, loss: 0.012\n",
      "model: 1, epoch: 147, loss: 0.012\n",
      "model: 1, epoch: 148, loss: 0.012\n",
      "model: 1, epoch: 149, loss: 0.011\n",
      "model: 1, epoch: 150, loss: 0.011\n",
      "model: 1, epoch: 151, loss: 0.011\n",
      "model: 1, epoch: 152, loss: 0.011\n",
      "model: 1, epoch: 153, loss: 0.011\n",
      "model: 1, epoch: 154, loss: 0.011\n",
      "model: 1, epoch: 155, loss: 0.011\n",
      "model: 1, epoch: 156, loss: 0.011\n",
      "model: 1, epoch: 157, loss: 0.011\n",
      "model: 1, epoch: 158, loss: 0.011\n",
      "model: 1, epoch: 159, loss: 0.011\n",
      "model: 1, epoch: 160, loss: 0.011\n",
      "model: 1, epoch: 161, loss: 0.011\n",
      "model: 1, epoch: 162, loss: 0.011\n",
      "model: 1, epoch: 163, loss: 0.011\n",
      "model: 1, epoch: 164, loss: 0.011\n",
      "model: 1, epoch: 165, loss: 0.010\n",
      "model: 1, epoch: 166, loss: 0.010\n",
      "model: 1, epoch: 167, loss: 0.010\n",
      "model: 1, epoch: 168, loss: 0.010\n",
      "model: 1, epoch: 169, loss: 0.010\n",
      "model: 1, epoch: 170, loss: 0.010\n",
      "model: 1, epoch: 171, loss: 0.010\n",
      "model: 1, epoch: 172, loss: 0.010\n",
      "model: 1, epoch: 173, loss: 0.010\n",
      "model: 1, epoch: 174, loss: 0.010\n",
      "model: 1, epoch: 175, loss: 0.010\n",
      "model: 1, epoch: 176, loss: 0.010\n",
      "model: 1, epoch: 177, loss: 0.010\n",
      "model: 1, epoch: 178, loss: 0.010\n",
      "model: 1, epoch: 179, loss: 0.010\n",
      "model: 1, epoch: 180, loss: 0.010\n",
      "model: 1, epoch: 181, loss: 0.010\n",
      "model: 1, epoch: 182, loss: 0.010\n",
      "model: 1, epoch: 183, loss: 0.010\n",
      "model: 1, epoch: 184, loss: 0.010\n",
      "model: 1, epoch: 185, loss: 0.010\n",
      "model: 1, epoch: 186, loss: 0.010\n",
      "model: 1, epoch: 187, loss: 0.009\n",
      "model: 1, epoch: 188, loss: 0.009\n",
      "model: 1, epoch: 189, loss: 0.009\n",
      "model: 1, epoch: 190, loss: 0.009\n",
      "model: 1, epoch: 191, loss: 0.009\n",
      "model: 1, epoch: 192, loss: 0.009\n",
      "model: 1, epoch: 193, loss: 0.009\n",
      "model: 1, epoch: 194, loss: 0.009\n",
      "model: 1, epoch: 195, loss: 0.009\n",
      "model: 1, epoch: 196, loss: 0.009\n",
      "model: 1, epoch: 197, loss: 0.009\n",
      "model: 1, epoch: 198, loss: 0.009\n",
      "model: 1, epoch: 199, loss: 0.009\n",
      "model: 1, epoch: 200, loss: 0.009\n",
      "model: 1, epoch: 201, loss: 0.009\n",
      "model: 1, epoch: 202, loss: 0.009\n",
      "model: 1, epoch: 203, loss: 0.009\n",
      "model: 1, epoch: 204, loss: 0.009\n",
      "model: 1, epoch: 205, loss: 0.009\n",
      "model: 1, epoch: 206, loss: 0.009\n",
      "model: 1, epoch: 207, loss: 0.009\n",
      "model: 1, epoch: 208, loss: 0.009\n",
      "model: 1, epoch: 209, loss: 0.009\n",
      "model: 1, epoch: 210, loss: 0.009\n",
      "model: 1, epoch: 211, loss: 0.009\n",
      "model: 1, epoch: 212, loss: 0.009\n",
      "model: 1, epoch: 213, loss: 0.009\n",
      "model: 1, epoch: 214, loss: 0.009\n",
      "model: 1, epoch: 215, loss: 0.009\n",
      "model: 1, epoch: 216, loss: 0.009\n",
      "model: 1, epoch: 217, loss: 0.009\n",
      "model: 1, epoch: 218, loss: 0.009\n",
      "model: 1, epoch: 219, loss: 0.009\n",
      "model: 1, epoch: 220, loss: 0.009\n",
      "model: 1, epoch: 221, loss: 0.009\n",
      "model: 1, epoch: 222, loss: 0.009\n",
      "model: 1, epoch: 223, loss: 0.008\n",
      "model: 1, epoch: 224, loss: 0.008\n",
      "model: 1, epoch: 225, loss: 0.008\n",
      "model: 1, epoch: 226, loss: 0.008\n",
      "model: 1, epoch: 227, loss: 0.008\n",
      "model: 1, epoch: 228, loss: 0.008\n",
      "model: 1, epoch: 229, loss: 0.008\n",
      "model: 1, epoch: 230, loss: 0.008\n",
      "model: 1, epoch: 231, loss: 0.008\n",
      "model: 1, epoch: 232, loss: 0.008\n",
      "model: 1, epoch: 233, loss: 0.008\n",
      "model: 1, epoch: 234, loss: 0.008\n",
      "model: 1, epoch: 235, loss: 0.008\n",
      "model: 1, epoch: 236, loss: 0.008\n",
      "model: 1, epoch: 237, loss: 0.008\n",
      "model: 1, epoch: 238, loss: 0.008\n",
      "model: 1, epoch: 239, loss: 0.008\n",
      "model: 1, epoch: 240, loss: 0.008\n",
      "model: 1, epoch: 241, loss: 0.008\n",
      "model: 1, epoch: 242, loss: 0.008\n",
      "model: 1, epoch: 243, loss: 0.008\n",
      "model: 1, epoch: 244, loss: 0.008\n",
      "model: 1, epoch: 245, loss: 0.008\n",
      "model: 1, epoch: 246, loss: 0.008\n",
      "model: 1, epoch: 247, loss: 0.008\n",
      "model: 1, epoch: 248, loss: 0.008\n",
      "model: 1, epoch: 249, loss: 0.008\n",
      "model: 1, epoch: 250, loss: 0.008\n",
      "model: 1, epoch: 251, loss: 0.008\n",
      "model: 1, epoch: 252, loss: 0.008\n",
      "model: 1, epoch: 253, loss: 0.008\n",
      "model: 1, epoch: 254, loss: 0.008\n",
      "model: 1, epoch: 255, loss: 0.008\n",
      "model: 1, epoch: 256, loss: 0.008\n",
      "model: 1, epoch: 257, loss: 0.008\n",
      "model: 1, epoch: 258, loss: 0.008\n",
      "model: 1, epoch: 259, loss: 0.008\n",
      "model: 1, epoch: 260, loss: 0.008\n",
      "model: 1, epoch: 261, loss: 0.008\n",
      "model: 1, epoch: 262, loss: 0.008\n",
      "model: 1, epoch: 263, loss: 0.008\n",
      "model: 1, epoch: 264, loss: 0.008\n",
      "model: 1, epoch: 265, loss: 0.008\n",
      "model: 1, epoch: 266, loss: 0.008\n",
      "model: 1, epoch: 267, loss: 0.008\n",
      "model: 1, epoch: 268, loss: 0.008\n",
      "model: 1, epoch: 269, loss: 0.008\n",
      "model: 1, epoch: 270, loss: 0.008\n",
      "model: 1, epoch: 271, loss: 0.008\n",
      "model: 1, epoch: 272, loss: 0.008\n",
      "model: 1, epoch: 273, loss: 0.008\n",
      "model: 1, epoch: 274, loss: 0.008\n",
      "model: 1, epoch: 275, loss: 0.008\n",
      "model: 1, epoch: 276, loss: 0.008\n",
      "model: 1, epoch: 277, loss: 0.008\n",
      "model: 1, epoch: 278, loss: 0.008\n",
      "model: 1, epoch: 279, loss: 0.008\n",
      "model: 1, epoch: 280, loss: 0.008\n",
      "model: 1, epoch: 281, loss: 0.008\n",
      "model: 1, epoch: 282, loss: 0.008\n",
      "model: 1, epoch: 283, loss: 0.008\n",
      "model: 1, epoch: 284, loss: 0.008\n",
      "model: 1, epoch: 285, loss: 0.008\n",
      "model: 1, epoch: 286, loss: 0.008\n",
      "model: 1, epoch: 287, loss: 0.008\n",
      "model: 1, epoch: 288, loss: 0.008\n",
      "model: 1, epoch: 289, loss: 0.008\n",
      "model: 1, epoch: 290, loss: 0.008\n",
      "model: 1, epoch: 291, loss: 0.008\n",
      "model: 1, epoch: 292, loss: 0.008\n",
      "model: 1, epoch: 293, loss: 0.008\n",
      "model: 1, epoch: 294, loss: 0.008\n",
      "model: 1, epoch: 295, loss: 0.008\n",
      "model: 1, epoch: 296, loss: 0.008\n",
      "model: 1, epoch: 297, loss: 0.008\n",
      "model: 1, epoch: 298, loss: 0.008\n",
      "model: 1, epoch: 299, loss: 0.008\n",
      "model: 1, epoch: 300, loss: 0.008\n",
      "model: 1, epoch: 301, loss: 0.008\n",
      "model: 1, epoch: 302, loss: 0.008\n",
      "model: 1, epoch: 303, loss: 0.008\n",
      "model: 1, epoch: 304, loss: 0.008\n",
      "model: 1, epoch: 305, loss: 0.008\n",
      "model: 1, epoch: 306, loss: 0.008\n",
      "model: 1, epoch: 307, loss: 0.008\n",
      "model: 1, epoch: 308, loss: 0.008\n",
      "model: 1, epoch: 309, loss: 0.008\n",
      "model: 1, epoch: 310, loss: 0.008\n",
      "model: 1, epoch: 311, loss: 0.008\n",
      "model: 1, epoch: 312, loss: 0.008\n",
      "model: 1, epoch: 313, loss: 0.008\n",
      "model: 1, epoch: 314, loss: 0.008\n",
      "model: 1, epoch: 315, loss: 0.008\n",
      "model: 1, epoch: 316, loss: 0.008\n",
      "model: 1, epoch: 317, loss: 0.008\n",
      "model: 1, epoch: 318, loss: 0.008\n",
      "model: 1, epoch: 319, loss: 0.008\n",
      "model: 1, epoch: 320, loss: 0.008\n",
      "model: 1, epoch: 321, loss: 0.008\n",
      "model: 1, epoch: 322, loss: 0.008\n",
      "model: 1, epoch: 323, loss: 0.008\n",
      "model: 1, epoch: 324, loss: 0.008\n",
      "model: 1, epoch: 325, loss: 0.008\n",
      "model: 1, epoch: 326, loss: 0.008\n",
      "model: 1, epoch: 327, loss: 0.008\n",
      "model: 1, epoch: 328, loss: 0.007\n",
      "model: 1, epoch: 329, loss: 0.007\n",
      "model: 1, epoch: 330, loss: 0.007\n",
      "model: 1, epoch: 331, loss: 0.007\n",
      "model: 1, epoch: 332, loss: 0.007\n",
      "model: 1, epoch: 333, loss: 0.007\n",
      "model: 1, epoch: 334, loss: 0.007\n",
      "model: 1, epoch: 335, loss: 0.007\n",
      "model: 1, epoch: 336, loss: 0.007\n",
      "model: 1, epoch: 337, loss: 0.007\n",
      "model: 1, epoch: 338, loss: 0.007\n",
      "model: 1, epoch: 339, loss: 0.007\n",
      "model: 1, epoch: 340, loss: 0.007\n",
      "model: 1, epoch: 341, loss: 0.007\n",
      "model: 1, epoch: 342, loss: 0.007\n",
      "model: 1, epoch: 343, loss: 0.007\n",
      "model: 1, epoch: 344, loss: 0.007\n",
      "model: 1, epoch: 345, loss: 0.007\n",
      "model: 1, epoch: 346, loss: 0.007\n",
      "model: 1, epoch: 347, loss: 0.007\n",
      "model: 1, epoch: 348, loss: 0.007\n",
      "model: 1, epoch: 349, loss: 0.007\n",
      "model: 1, epoch: 350, loss: 0.007\n",
      "model: 1, epoch: 351, loss: 0.007\n",
      "model: 1, epoch: 352, loss: 0.007\n",
      "model: 1, epoch: 353, loss: 0.007\n",
      "model: 1, epoch: 354, loss: 0.007\n",
      "model: 1, epoch: 355, loss: 0.007\n",
      "model: 1, epoch: 356, loss: 0.007\n",
      "model: 1, epoch: 357, loss: 0.007\n",
      "model: 1, epoch: 358, loss: 0.007\n",
      "model: 1, epoch: 359, loss: 0.007\n",
      "model: 1, epoch: 360, loss: 0.007\n",
      "model: 1, epoch: 361, loss: 0.007\n",
      "model: 1, epoch: 362, loss: 0.007\n",
      "model: 1, epoch: 363, loss: 0.007\n",
      "model: 1, epoch: 364, loss: 0.007\n",
      "model: 1, epoch: 365, loss: 0.007\n",
      "model: 1, epoch: 366, loss: 0.007\n",
      "model: 1, epoch: 367, loss: 0.007\n",
      "model: 1, epoch: 368, loss: 0.007\n",
      "model: 1, epoch: 369, loss: 0.007\n",
      "model: 1, epoch: 370, loss: 0.007\n",
      "model: 1, epoch: 371, loss: 0.007\n",
      "model: 1, epoch: 372, loss: 0.007\n",
      "model: 1, epoch: 373, loss: 0.007\n",
      "model: 1, epoch: 374, loss: 0.007\n",
      "model: 1, epoch: 375, loss: 0.007\n",
      "model: 1, epoch: 376, loss: 0.007\n",
      "model: 1, epoch: 377, loss: 0.007\n",
      "model: 1, epoch: 378, loss: 0.007\n",
      "model: 1, epoch: 379, loss: 0.007\n",
      "model: 1, epoch: 380, loss: 0.007\n",
      "model: 1, epoch: 381, loss: 0.007\n",
      "model: 1, epoch: 382, loss: 0.007\n",
      "model: 1, epoch: 383, loss: 0.007\n",
      "model: 1, epoch: 384, loss: 0.007\n",
      "model: 1, epoch: 385, loss: 0.007\n",
      "model: 1, epoch: 386, loss: 0.007\n",
      "model: 1, epoch: 387, loss: 0.007\n",
      "model: 1, epoch: 388, loss: 0.007\n",
      "model: 1, epoch: 389, loss: 0.007\n",
      "model: 1, epoch: 390, loss: 0.007\n",
      "model: 1, epoch: 391, loss: 0.007\n",
      "model: 1, epoch: 392, loss: 0.007\n",
      "model: 1, epoch: 393, loss: 0.007\n",
      "model: 1, epoch: 394, loss: 0.007\n",
      "model: 1, epoch: 395, loss: 0.007\n",
      "model: 1, epoch: 396, loss: 0.007\n",
      "model: 1, epoch: 397, loss: 0.007\n",
      "model: 1, epoch: 398, loss: 0.007\n",
      "model: 1, epoch: 399, loss: 0.007\n",
      "model: 1, epoch: 400, loss: 0.007\n",
      "model: 1, epoch: 401, loss: 0.007\n",
      "model: 1, epoch: 402, loss: 0.007\n",
      "model: 1, epoch: 403, loss: 0.007\n",
      "model: 1, epoch: 404, loss: 0.007\n",
      "model: 1, epoch: 405, loss: 0.007\n",
      "model: 1, epoch: 406, loss: 0.007\n",
      "model: 1, epoch: 407, loss: 0.007\n",
      "model: 1, epoch: 408, loss: 0.007\n",
      "model: 1, epoch: 409, loss: 0.007\n",
      "model: 1, epoch: 410, loss: 0.007\n",
      "model: 1, epoch: 411, loss: 0.007\n",
      "model: 1, epoch: 412, loss: 0.007\n",
      "model: 1, epoch: 413, loss: 0.007\n",
      "model: 1, epoch: 414, loss: 0.007\n",
      "model: 1, epoch: 415, loss: 0.007\n",
      "model: 1, epoch: 416, loss: 0.007\n",
      "model: 1, epoch: 417, loss: 0.007\n",
      "model: 1, epoch: 418, loss: 0.007\n",
      "model: 1, epoch: 419, loss: 0.007\n",
      "model: 1, epoch: 420, loss: 0.007\n",
      "model: 1, epoch: 421, loss: 0.007\n",
      "model: 1, epoch: 422, loss: 0.007\n",
      "model: 1, epoch: 423, loss: 0.007\n",
      "model: 1, epoch: 424, loss: 0.007\n",
      "model: 1, epoch: 425, loss: 0.007\n",
      "model: 1, epoch: 426, loss: 0.007\n",
      "model: 1, epoch: 427, loss: 0.007\n",
      "model: 1, epoch: 428, loss: 0.007\n",
      "model: 1, epoch: 429, loss: 0.007\n",
      "model: 1, epoch: 430, loss: 0.007\n",
      "model: 1, epoch: 431, loss: 0.007\n",
      "model: 1, epoch: 432, loss: 0.007\n",
      "model: 1, epoch: 433, loss: 0.007\n",
      "model: 1, epoch: 434, loss: 0.007\n",
      "model: 1, epoch: 435, loss: 0.007\n",
      "model: 1, epoch: 436, loss: 0.007\n",
      "model: 1, epoch: 437, loss: 0.007\n",
      "model: 1, epoch: 438, loss: 0.007\n",
      "model: 1, epoch: 439, loss: 0.007\n",
      "model: 1, epoch: 440, loss: 0.007\n",
      "model: 1, epoch: 441, loss: 0.007\n",
      "model: 1, epoch: 442, loss: 0.007\n",
      "model: 1, epoch: 443, loss: 0.007\n",
      "model: 1, epoch: 444, loss: 0.007\n",
      "model: 1, epoch: 445, loss: 0.007\n",
      "model: 1, epoch: 446, loss: 0.007\n",
      "model: 1, epoch: 447, loss: 0.007\n",
      "model: 1, epoch: 448, loss: 0.007\n",
      "model: 1, epoch: 449, loss: 0.007\n",
      "model: 1, epoch: 450, loss: 0.007\n",
      "model: 1, epoch: 451, loss: 0.007\n",
      "model: 1, epoch: 452, loss: 0.007\n",
      "model: 1, epoch: 453, loss: 0.007\n",
      "model: 1, epoch: 454, loss: 0.007\n",
      "model: 1, epoch: 455, loss: 0.007\n",
      "model: 1, epoch: 456, loss: 0.007\n",
      "model: 1, epoch: 457, loss: 0.007\n",
      "model: 1, epoch: 458, loss: 0.007\n",
      "model: 1, epoch: 459, loss: 0.007\n",
      "model: 1, epoch: 460, loss: 0.007\n",
      "model: 1, epoch: 461, loss: 0.007\n",
      "model: 1, epoch: 462, loss: 0.007\n",
      "model: 1, epoch: 463, loss: 0.007\n",
      "model: 1, epoch: 464, loss: 0.007\n",
      "model: 1, epoch: 465, loss: 0.007\n",
      "model: 1, epoch: 466, loss: 0.007\n",
      "model: 1, epoch: 467, loss: 0.007\n",
      "model: 1, epoch: 468, loss: 0.007\n",
      "model: 1, epoch: 469, loss: 0.007\n",
      "model: 1, epoch: 470, loss: 0.007\n",
      "model: 1, epoch: 471, loss: 0.007\n",
      "model: 1, epoch: 472, loss: 0.007\n",
      "model: 1, epoch: 473, loss: 0.007\n",
      "model: 1, epoch: 474, loss: 0.007\n",
      "model: 1, epoch: 475, loss: 0.007\n",
      "model: 1, epoch: 476, loss: 0.007\n",
      "model: 1, epoch: 477, loss: 0.007\n",
      "model: 1, epoch: 478, loss: 0.007\n",
      "model: 1, epoch: 479, loss: 0.007\n",
      "model: 1, epoch: 480, loss: 0.007\n",
      "model: 1, epoch: 481, loss: 0.007\n",
      "model: 1, epoch: 482, loss: 0.007\n",
      "model: 1, epoch: 483, loss: 0.007\n",
      "model: 1, epoch: 484, loss: 0.007\n",
      "model: 1, epoch: 485, loss: 0.007\n",
      "model: 1, epoch: 486, loss: 0.007\n",
      "model: 1, epoch: 487, loss: 0.007\n",
      "model: 1, epoch: 488, loss: 0.007\n",
      "model: 1, epoch: 489, loss: 0.007\n",
      "model: 1, epoch: 490, loss: 0.007\n",
      "model: 1, epoch: 491, loss: 0.007\n",
      "model: 1, epoch: 492, loss: 0.007\n",
      "model: 1, epoch: 493, loss: 0.007\n",
      "model: 1, epoch: 494, loss: 0.007\n",
      "model: 1, epoch: 495, loss: 0.007\n",
      "model: 1, epoch: 496, loss: 0.007\n",
      "model: 1, epoch: 497, loss: 0.007\n",
      "model: 1, epoch: 498, loss: 0.007\n",
      "model: 1, epoch: 499, loss: 0.007\n",
      "model: 1, epoch: 500, loss: 0.007\n",
      "model: 1, epoch: 501, loss: 0.007\n",
      "model: 1, epoch: 502, loss: 0.007\n",
      "model: 1, epoch: 503, loss: 0.007\n",
      "model: 1, epoch: 504, loss: 0.007\n",
      "model: 1, epoch: 505, loss: 0.007\n",
      "model: 1, epoch: 506, loss: 0.007\n",
      "model: 1, epoch: 507, loss: 0.007\n",
      "model: 1, epoch: 508, loss: 0.007\n",
      "model: 1, epoch: 509, loss: 0.007\n",
      "model: 1, epoch: 510, loss: 0.007\n",
      "model: 1, epoch: 511, loss: 0.007\n",
      "model: 1, epoch: 512, loss: 0.007\n",
      "model: 1, epoch: 513, loss: 0.007\n",
      "model: 1, epoch: 514, loss: 0.007\n",
      "model: 1, epoch: 515, loss: 0.007\n",
      "model: 1, epoch: 516, loss: 0.007\n",
      "model: 1, epoch: 517, loss: 0.007\n",
      "model: 1, epoch: 518, loss: 0.007\n",
      "model: 1, epoch: 519, loss: 0.007\n",
      "model: 1, epoch: 520, loss: 0.007\n",
      "model: 1, epoch: 521, loss: 0.007\n",
      "model: 1, epoch: 522, loss: 0.007\n",
      "model: 1, epoch: 523, loss: 0.007\n",
      "model: 1, epoch: 524, loss: 0.007\n",
      "model: 1, epoch: 525, loss: 0.007\n",
      "model: 1, epoch: 526, loss: 0.007\n",
      "model: 1, epoch: 527, loss: 0.007\n",
      "model: 1, epoch: 528, loss: 0.007\n",
      "model: 1, epoch: 529, loss: 0.007\n",
      "model: 1, epoch: 530, loss: 0.007\n",
      "model: 1, epoch: 531, loss: 0.007\n",
      "model: 1, epoch: 532, loss: 0.007\n",
      "model: 1, epoch: 533, loss: 0.007\n",
      "model: 1, epoch: 534, loss: 0.007\n",
      "model: 1, epoch: 535, loss: 0.007\n",
      "model: 1, epoch: 536, loss: 0.007\n",
      "model: 1, epoch: 537, loss: 0.007\n",
      "model: 1, epoch: 538, loss: 0.007\n",
      "model: 1, epoch: 539, loss: 0.007\n",
      "model: 1, epoch: 540, loss: 0.007\n",
      "model: 1, epoch: 541, loss: 0.007\n",
      "model: 1, epoch: 542, loss: 0.007\n",
      "model: 1, epoch: 543, loss: 0.007\n",
      "model: 1, epoch: 544, loss: 0.007\n",
      "model: 1, epoch: 545, loss: 0.007\n",
      "model: 1, epoch: 546, loss: 0.007\n",
      "model: 1, epoch: 547, loss: 0.007\n",
      "model: 1, epoch: 548, loss: 0.007\n",
      "model: 1, epoch: 549, loss: 0.007\n",
      "model: 1, epoch: 550, loss: 0.007\n",
      "model: 1, epoch: 551, loss: 0.007\n",
      "model: 1, epoch: 552, loss: 0.007\n",
      "model: 1, epoch: 553, loss: 0.007\n",
      "model: 1, epoch: 554, loss: 0.007\n",
      "model: 1, epoch: 555, loss: 0.007\n",
      "model: 1, epoch: 556, loss: 0.007\n",
      "model: 1, epoch: 557, loss: 0.007\n",
      "model: 1, epoch: 558, loss: 0.007\n",
      "model: 1, epoch: 559, loss: 0.007\n",
      "model: 1, epoch: 560, loss: 0.007\n",
      "model: 1, epoch: 561, loss: 0.007\n",
      "model: 1, epoch: 562, loss: 0.007\n",
      "model: 1, epoch: 563, loss: 0.007\n",
      "model: 1, epoch: 564, loss: 0.007\n",
      "model: 1, epoch: 565, loss: 0.007\n",
      "model: 1, epoch: 566, loss: 0.007\n",
      "model: 1, epoch: 567, loss: 0.007\n",
      "model: 1, epoch: 568, loss: 0.007\n",
      "model: 1, epoch: 569, loss: 0.007\n",
      "model: 1, epoch: 570, loss: 0.007\n",
      "model: 1, epoch: 571, loss: 0.007\n",
      "model: 1, epoch: 572, loss: 0.007\n",
      "model: 1, epoch: 573, loss: 0.007\n",
      "model: 1, epoch: 574, loss: 0.007\n",
      "model: 1, epoch: 575, loss: 0.007\n",
      "model: 1, epoch: 576, loss: 0.007\n",
      "model: 1, epoch: 577, loss: 0.007\n",
      "model: 1, epoch: 578, loss: 0.007\n",
      "model: 1, epoch: 579, loss: 0.007\n",
      "model: 1, epoch: 580, loss: 0.007\n",
      "model: 1, epoch: 581, loss: 0.007\n",
      "model: 1, epoch: 582, loss: 0.007\n",
      "model: 1, epoch: 583, loss: 0.007\n",
      "model: 1, epoch: 584, loss: 0.007\n",
      "model: 1, epoch: 585, loss: 0.007\n",
      "model: 1, epoch: 586, loss: 0.007\n",
      "model: 1, epoch: 587, loss: 0.007\n",
      "model: 1, epoch: 588, loss: 0.007\n",
      "model: 1, epoch: 589, loss: 0.007\n",
      "model: 1, epoch: 590, loss: 0.007\n",
      "model: 1, epoch: 591, loss: 0.007\n",
      "model: 1, epoch: 592, loss: 0.007\n",
      "model: 1, epoch: 593, loss: 0.007\n",
      "model: 1, epoch: 594, loss: 0.007\n",
      "model: 1, epoch: 595, loss: 0.007\n",
      "model: 1, epoch: 596, loss: 0.007\n",
      "model: 1, epoch: 597, loss: 0.007\n",
      "model: 1, epoch: 598, loss: 0.007\n",
      "model: 1, epoch: 599, loss: 0.007\n",
      "model: 1, epoch: 600, loss: 0.007\n",
      "model: 1, epoch: 601, loss: 0.007\n",
      "model: 1, epoch: 602, loss: 0.007\n",
      "model: 1, epoch: 603, loss: 0.007\n",
      "model: 1, epoch: 604, loss: 0.007\n",
      "model: 1, epoch: 605, loss: 0.007\n",
      "model: 1, epoch: 606, loss: 0.007\n",
      "model: 1, epoch: 607, loss: 0.007\n",
      "model: 1, epoch: 608, loss: 0.007\n",
      "model: 1, epoch: 609, loss: 0.007\n",
      "model: 1, epoch: 610, loss: 0.007\n",
      "model: 1, epoch: 611, loss: 0.007\n",
      "model: 1, epoch: 612, loss: 0.007\n",
      "model: 1, epoch: 613, loss: 0.007\n",
      "model: 1, epoch: 614, loss: 0.007\n",
      "model: 1, epoch: 615, loss: 0.007\n",
      "model: 1, epoch: 616, loss: 0.007\n",
      "model: 1, epoch: 617, loss: 0.007\n",
      "model: 1, epoch: 618, loss: 0.007\n",
      "model: 1, epoch: 619, loss: 0.007\n",
      "model: 1, epoch: 620, loss: 0.007\n",
      "model: 1, epoch: 621, loss: 0.007\n",
      "model: 1, epoch: 622, loss: 0.007\n",
      "model: 1, epoch: 623, loss: 0.007\n",
      "model: 1, epoch: 624, loss: 0.007\n",
      "model: 1, epoch: 625, loss: 0.007\n",
      "model: 1, epoch: 626, loss: 0.007\n",
      "model: 1, epoch: 627, loss: 0.007\n",
      "model: 1, epoch: 628, loss: 0.007\n",
      "model: 1, epoch: 629, loss: 0.007\n",
      "model: 1, epoch: 630, loss: 0.007\n",
      "model: 1, epoch: 631, loss: 0.007\n",
      "model: 1, epoch: 632, loss: 0.007\n",
      "model: 1, epoch: 633, loss: 0.007\n",
      "model: 1, epoch: 634, loss: 0.007\n",
      "model: 1, epoch: 635, loss: 0.007\n",
      "model: 1, epoch: 636, loss: 0.007\n",
      "model: 1, epoch: 637, loss: 0.007\n",
      "model: 1, epoch: 638, loss: 0.007\n",
      "model: 1, epoch: 639, loss: 0.007\n",
      "model: 1, epoch: 640, loss: 0.007\n",
      "model: 1, epoch: 641, loss: 0.007\n",
      "model: 1, epoch: 642, loss: 0.007\n",
      "model: 1, epoch: 643, loss: 0.007\n",
      "model: 1, epoch: 644, loss: 0.007\n",
      "model: 1, epoch: 645, loss: 0.007\n",
      "model: 1, epoch: 646, loss: 0.007\n",
      "model: 1, epoch: 647, loss: 0.007\n",
      "model: 1, epoch: 648, loss: 0.007\n",
      "model: 1, epoch: 649, loss: 0.007\n",
      "model: 1, epoch: 650, loss: 0.007\n",
      "model: 1, epoch: 651, loss: 0.007\n",
      "model: 1, epoch: 652, loss: 0.007\n",
      "model: 1, epoch: 653, loss: 0.007\n",
      "model: 1, epoch: 654, loss: 0.007\n",
      "model: 1, epoch: 655, loss: 0.007\n",
      "model: 1, epoch: 656, loss: 0.007\n",
      "model: 1, epoch: 657, loss: 0.007\n",
      "model: 1, epoch: 658, loss: 0.007\n",
      "model: 1, epoch: 659, loss: 0.007\n",
      "model: 1, epoch: 660, loss: 0.007\n",
      "model: 1, epoch: 661, loss: 0.007\n",
      "model: 1, epoch: 662, loss: 0.007\n",
      "model: 1, epoch: 663, loss: 0.007\n",
      "model: 1, epoch: 664, loss: 0.007\n",
      "model: 1, epoch: 665, loss: 0.007\n",
      "model: 1, epoch: 666, loss: 0.007\n",
      "model: 1, epoch: 667, loss: 0.007\n",
      "model: 1, epoch: 668, loss: 0.007\n",
      "model: 1, epoch: 669, loss: 0.007\n",
      "model: 1, epoch: 670, loss: 0.007\n",
      "model: 1, epoch: 671, loss: 0.007\n",
      "model: 1, epoch: 672, loss: 0.007\n",
      "model: 1, epoch: 673, loss: 0.007\n",
      "model: 1, epoch: 674, loss: 0.007\n",
      "model: 1, epoch: 675, loss: 0.007\n",
      "model: 1, epoch: 676, loss: 0.007\n",
      "model: 1, epoch: 677, loss: 0.007\n",
      "model: 1, epoch: 678, loss: 0.007\n",
      "model: 1, epoch: 679, loss: 0.007\n",
      "model: 1, epoch: 680, loss: 0.007\n",
      "model: 1, epoch: 681, loss: 0.007\n",
      "model: 1, epoch: 682, loss: 0.007\n",
      "model: 1, epoch: 683, loss: 0.007\n",
      "model: 1, epoch: 684, loss: 0.007\n",
      "model: 1, epoch: 685, loss: 0.007\n",
      "model: 1, epoch: 686, loss: 0.007\n",
      "model: 1, epoch: 687, loss: 0.007\n",
      "model: 1, epoch: 688, loss: 0.007\n",
      "model: 1, epoch: 689, loss: 0.007\n",
      "model: 1, epoch: 690, loss: 0.007\n",
      "model: 1, epoch: 691, loss: 0.007\n",
      "model: 1, epoch: 692, loss: 0.007\n",
      "model: 1, epoch: 693, loss: 0.006\n",
      "model: 1, epoch: 694, loss: 0.006\n",
      "model: 1, epoch: 695, loss: 0.006\n",
      "model: 1, epoch: 696, loss: 0.006\n",
      "model: 1, epoch: 697, loss: 0.006\n",
      "model: 1, epoch: 698, loss: 0.006\n",
      "model: 1, epoch: 699, loss: 0.006\n",
      "model: 1, epoch: 700, loss: 0.006\n",
      "model: 1, epoch: 701, loss: 0.006\n",
      "model: 1, epoch: 702, loss: 0.006\n",
      "model: 1, epoch: 703, loss: 0.006\n",
      "model: 1, epoch: 704, loss: 0.006\n",
      "model: 1, epoch: 705, loss: 0.006\n",
      "model: 1, epoch: 706, loss: 0.006\n",
      "model: 1, epoch: 707, loss: 0.006\n",
      "model: 1, epoch: 708, loss: 0.006\n",
      "model: 1, epoch: 709, loss: 0.006\n",
      "model: 1, epoch: 710, loss: 0.006\n",
      "model: 1, epoch: 711, loss: 0.006\n",
      "model: 1, epoch: 712, loss: 0.006\n",
      "model: 1, epoch: 713, loss: 0.006\n",
      "model: 1, epoch: 714, loss: 0.006\n",
      "model: 1, epoch: 715, loss: 0.006\n",
      "model: 1, epoch: 716, loss: 0.006\n",
      "model: 1, epoch: 717, loss: 0.006\n",
      "model: 1, epoch: 718, loss: 0.006\n",
      "model: 1, epoch: 719, loss: 0.006\n",
      "model: 1, epoch: 720, loss: 0.006\n",
      "model: 1, epoch: 721, loss: 0.006\n",
      "model: 1, epoch: 722, loss: 0.006\n",
      "model: 1, epoch: 723, loss: 0.006\n",
      "model: 1, epoch: 724, loss: 0.006\n",
      "model: 1, epoch: 725, loss: 0.006\n",
      "model: 1, epoch: 726, loss: 0.006\n",
      "model: 1, epoch: 727, loss: 0.006\n",
      "model: 1, epoch: 728, loss: 0.006\n",
      "model: 1, epoch: 729, loss: 0.006\n",
      "model: 1, epoch: 730, loss: 0.006\n",
      "model: 1, epoch: 731, loss: 0.006\n",
      "model: 1, epoch: 732, loss: 0.006\n",
      "model: 1, epoch: 733, loss: 0.006\n",
      "model: 1, epoch: 734, loss: 0.006\n",
      "model: 1, epoch: 735, loss: 0.006\n",
      "model: 1, epoch: 736, loss: 0.006\n",
      "model: 1, epoch: 737, loss: 0.006\n",
      "model: 1, epoch: 738, loss: 0.006\n",
      "model: 1, epoch: 739, loss: 0.006\n",
      "model: 1, epoch: 740, loss: 0.006\n",
      "model: 1, epoch: 741, loss: 0.006\n",
      "model: 1, epoch: 742, loss: 0.006\n",
      "model: 1, epoch: 743, loss: 0.006\n",
      "model: 1, epoch: 744, loss: 0.006\n",
      "model: 1, epoch: 745, loss: 0.006\n",
      "model: 1, epoch: 746, loss: 0.006\n",
      "model: 1, epoch: 747, loss: 0.006\n",
      "model: 1, epoch: 748, loss: 0.006\n",
      "model: 1, epoch: 749, loss: 0.006\n",
      "model: 1, epoch: 750, loss: 0.006\n",
      "model: 1, epoch: 751, loss: 0.006\n",
      "model: 1, epoch: 752, loss: 0.006\n",
      "model: 1, epoch: 753, loss: 0.006\n",
      "model: 1, epoch: 754, loss: 0.006\n",
      "model: 1, epoch: 755, loss: 0.006\n",
      "model: 1, epoch: 756, loss: 0.006\n",
      "model: 1, epoch: 757, loss: 0.006\n",
      "model: 1, epoch: 758, loss: 0.006\n",
      "model: 1, epoch: 759, loss: 0.006\n",
      "model: 1, epoch: 760, loss: 0.006\n",
      "model: 1, epoch: 761, loss: 0.006\n",
      "model: 1, epoch: 762, loss: 0.006\n",
      "model: 1, epoch: 763, loss: 0.006\n",
      "model: 1, epoch: 764, loss: 0.006\n",
      "model: 1, epoch: 765, loss: 0.006\n",
      "model: 1, epoch: 766, loss: 0.006\n",
      "model: 1, epoch: 767, loss: 0.006\n",
      "model: 1, epoch: 768, loss: 0.006\n",
      "model: 1, epoch: 769, loss: 0.006\n",
      "model: 1, epoch: 770, loss: 0.006\n",
      "model: 1, epoch: 771, loss: 0.006\n",
      "model: 1, epoch: 772, loss: 0.006\n",
      "model: 1, epoch: 773, loss: 0.006\n",
      "model: 1, epoch: 774, loss: 0.006\n",
      "model: 1, epoch: 775, loss: 0.006\n",
      "model: 1, epoch: 776, loss: 0.006\n",
      "model: 1, epoch: 777, loss: 0.006\n",
      "model: 1, epoch: 778, loss: 0.006\n",
      "model: 1, epoch: 779, loss: 0.006\n",
      "model: 1, epoch: 780, loss: 0.006\n",
      "model: 1, epoch: 781, loss: 0.006\n",
      "model: 1, epoch: 782, loss: 0.006\n",
      "model: 1, epoch: 783, loss: 0.006\n",
      "model: 1, epoch: 784, loss: 0.006\n",
      "model: 1, epoch: 785, loss: 0.006\n",
      "model: 1, epoch: 786, loss: 0.006\n",
      "model: 1, epoch: 787, loss: 0.006\n",
      "model: 1, epoch: 788, loss: 0.006\n",
      "model: 1, epoch: 789, loss: 0.006\n",
      "model: 1, epoch: 790, loss: 0.006\n",
      "model: 1, epoch: 791, loss: 0.006\n",
      "model: 1, epoch: 792, loss: 0.006\n",
      "model: 1, epoch: 793, loss: 0.006\n",
      "model: 1, epoch: 794, loss: 0.006\n",
      "model: 1, epoch: 795, loss: 0.006\n",
      "model: 1, epoch: 796, loss: 0.006\n",
      "model: 1, epoch: 797, loss: 0.006\n",
      "model: 1, epoch: 798, loss: 0.006\n",
      "model: 1, epoch: 799, loss: 0.006\n",
      "model: 1, epoch: 800, loss: 0.006\n",
      "model: 1, epoch: 801, loss: 0.006\n",
      "model: 1, epoch: 802, loss: 0.006\n",
      "model: 1, epoch: 803, loss: 0.006\n",
      "model: 1, epoch: 804, loss: 0.006\n",
      "model: 1, epoch: 805, loss: 0.006\n",
      "model: 1, epoch: 806, loss: 0.006\n",
      "model: 1, epoch: 807, loss: 0.006\n",
      "model: 1, epoch: 808, loss: 0.006\n",
      "model: 1, epoch: 809, loss: 0.006\n",
      "model: 1, epoch: 810, loss: 0.006\n",
      "model: 1, epoch: 811, loss: 0.006\n",
      "model: 1, epoch: 812, loss: 0.006\n",
      "model: 1, epoch: 813, loss: 0.006\n",
      "model: 1, epoch: 814, loss: 0.006\n",
      "model: 1, epoch: 815, loss: 0.006\n",
      "model: 1, epoch: 816, loss: 0.006\n",
      "model: 1, epoch: 817, loss: 0.006\n",
      "model: 1, epoch: 818, loss: 0.006\n",
      "model: 1, epoch: 819, loss: 0.006\n",
      "model: 1, epoch: 820, loss: 0.006\n",
      "model: 1, epoch: 821, loss: 0.006\n",
      "model: 1, epoch: 822, loss: 0.006\n",
      "model: 1, epoch: 823, loss: 0.006\n",
      "model: 1, epoch: 824, loss: 0.006\n",
      "model: 1, epoch: 825, loss: 0.006\n",
      "model: 1, epoch: 826, loss: 0.006\n",
      "model: 1, epoch: 827, loss: 0.006\n",
      "model: 1, epoch: 828, loss: 0.006\n",
      "model: 1, epoch: 829, loss: 0.006\n",
      "model: 1, epoch: 830, loss: 0.006\n",
      "model: 1, epoch: 831, loss: 0.006\n",
      "model: 1, epoch: 832, loss: 0.006\n",
      "model: 1, epoch: 833, loss: 0.006\n",
      "model: 1, epoch: 834, loss: 0.006\n",
      "model: 1, epoch: 835, loss: 0.006\n",
      "model: 1, epoch: 836, loss: 0.006\n",
      "model: 1, epoch: 837, loss: 0.006\n",
      "model: 1, epoch: 838, loss: 0.006\n",
      "model: 1, epoch: 839, loss: 0.006\n",
      "model: 1, epoch: 840, loss: 0.006\n",
      "model: 1, epoch: 841, loss: 0.006\n",
      "model: 1, epoch: 842, loss: 0.006\n",
      "model: 1, epoch: 843, loss: 0.006\n",
      "model: 1, epoch: 844, loss: 0.006\n",
      "model: 1, epoch: 845, loss: 0.006\n",
      "model: 1, epoch: 846, loss: 0.006\n",
      "model: 1, epoch: 847, loss: 0.006\n",
      "model: 1, epoch: 848, loss: 0.006\n",
      "model: 1, epoch: 849, loss: 0.006\n",
      "model: 1, epoch: 850, loss: 0.006\n",
      "model: 1, epoch: 851, loss: 0.006\n",
      "model: 1, epoch: 852, loss: 0.006\n",
      "model: 1, epoch: 853, loss: 0.006\n",
      "model: 1, epoch: 854, loss: 0.006\n",
      "model: 1, epoch: 855, loss: 0.006\n",
      "model: 1, epoch: 856, loss: 0.006\n",
      "model: 1, epoch: 857, loss: 0.006\n",
      "model: 1, epoch: 858, loss: 0.006\n",
      "model: 1, epoch: 859, loss: 0.006\n",
      "model: 1, epoch: 860, loss: 0.006\n",
      "model: 1, epoch: 861, loss: 0.006\n",
      "model: 1, epoch: 862, loss: 0.006\n",
      "model: 1, epoch: 863, loss: 0.006\n",
      "model: 1, epoch: 864, loss: 0.006\n",
      "model: 1, epoch: 865, loss: 0.006\n",
      "model: 1, epoch: 866, loss: 0.006\n",
      "model: 1, epoch: 867, loss: 0.006\n",
      "model: 1, epoch: 868, loss: 0.006\n",
      "model: 1, epoch: 869, loss: 0.006\n",
      "model: 1, epoch: 870, loss: 0.006\n",
      "model: 1, epoch: 871, loss: 0.006\n",
      "model: 1, epoch: 872, loss: 0.006\n",
      "model: 1, epoch: 873, loss: 0.006\n",
      "model: 1, epoch: 874, loss: 0.006\n",
      "model: 1, epoch: 875, loss: 0.006\n",
      "model: 1, epoch: 876, loss: 0.006\n",
      "model: 1, epoch: 877, loss: 0.006\n",
      "model: 1, epoch: 878, loss: 0.006\n",
      "model: 1, epoch: 879, loss: 0.006\n",
      "model: 1, epoch: 880, loss: 0.006\n",
      "model: 1, epoch: 881, loss: 0.006\n",
      "model: 1, epoch: 882, loss: 0.006\n",
      "model: 1, epoch: 883, loss: 0.006\n",
      "model: 1, epoch: 884, loss: 0.006\n",
      "model: 1, epoch: 885, loss: 0.006\n",
      "model: 1, epoch: 886, loss: 0.006\n",
      "model: 1, epoch: 887, loss: 0.006\n",
      "model: 1, epoch: 888, loss: 0.006\n",
      "model: 1, epoch: 889, loss: 0.006\n",
      "model: 1, epoch: 890, loss: 0.006\n",
      "model: 1, epoch: 891, loss: 0.006\n",
      "model: 1, epoch: 892, loss: 0.006\n",
      "model: 1, epoch: 893, loss: 0.006\n",
      "model: 1, epoch: 894, loss: 0.006\n",
      "model: 1, epoch: 895, loss: 0.006\n",
      "model: 1, epoch: 896, loss: 0.006\n",
      "model: 1, epoch: 897, loss: 0.006\n",
      "model: 1, epoch: 898, loss: 0.006\n",
      "model: 1, epoch: 899, loss: 0.006\n",
      "model: 1, epoch: 900, loss: 0.006\n",
      "model: 1, epoch: 901, loss: 0.006\n",
      "model: 1, epoch: 902, loss: 0.006\n",
      "model: 1, epoch: 903, loss: 0.006\n",
      "model: 1, epoch: 904, loss: 0.006\n",
      "model: 1, epoch: 905, loss: 0.006\n",
      "model: 1, epoch: 906, loss: 0.006\n",
      "model: 1, epoch: 907, loss: 0.006\n",
      "model: 1, epoch: 908, loss: 0.006\n",
      "model: 1, epoch: 909, loss: 0.006\n",
      "model: 1, epoch: 910, loss: 0.006\n",
      "model: 1, epoch: 911, loss: 0.006\n",
      "model: 1, epoch: 912, loss: 0.006\n",
      "model: 1, epoch: 913, loss: 0.006\n",
      "model: 1, epoch: 914, loss: 0.006\n",
      "model: 1, epoch: 915, loss: 0.006\n",
      "model: 1, epoch: 916, loss: 0.006\n",
      "model: 1, epoch: 917, loss: 0.006\n",
      "model: 1, epoch: 918, loss: 0.006\n",
      "model: 1, epoch: 919, loss: 0.006\n",
      "model: 1, epoch: 920, loss: 0.006\n",
      "model: 1, epoch: 921, loss: 0.006\n",
      "model: 1, epoch: 922, loss: 0.006\n",
      "model: 1, epoch: 923, loss: 0.006\n",
      "model: 1, epoch: 924, loss: 0.006\n",
      "model: 1, epoch: 925, loss: 0.006\n",
      "model: 1, epoch: 926, loss: 0.006\n",
      "model: 1, epoch: 927, loss: 0.006\n",
      "model: 1, epoch: 928, loss: 0.006\n",
      "model: 1, epoch: 929, loss: 0.006\n",
      "model: 1, epoch: 930, loss: 0.006\n",
      "model: 1, epoch: 931, loss: 0.006\n",
      "model: 1, epoch: 932, loss: 0.006\n",
      "model: 1, epoch: 933, loss: 0.006\n",
      "model: 1, epoch: 934, loss: 0.006\n",
      "model: 1, epoch: 935, loss: 0.006\n",
      "model: 1, epoch: 936, loss: 0.006\n",
      "model: 1, epoch: 937, loss: 0.006\n",
      "model: 1, epoch: 938, loss: 0.006\n",
      "model: 1, epoch: 939, loss: 0.006\n",
      "model: 1, epoch: 940, loss: 0.006\n",
      "model: 1, epoch: 941, loss: 0.006\n",
      "model: 1, epoch: 942, loss: 0.006\n",
      "model: 1, epoch: 943, loss: 0.006\n",
      "model: 1, epoch: 944, loss: 0.006\n",
      "model: 1, epoch: 945, loss: 0.006\n",
      "model: 1, epoch: 946, loss: 0.006\n",
      "model: 1, epoch: 947, loss: 0.006\n",
      "model: 1, epoch: 948, loss: 0.006\n",
      "model: 1, epoch: 949, loss: 0.006\n",
      "model: 1, epoch: 950, loss: 0.006\n",
      "model: 1, epoch: 951, loss: 0.006\n",
      "model: 1, epoch: 952, loss: 0.006\n",
      "model: 1, epoch: 953, loss: 0.006\n",
      "model: 1, epoch: 954, loss: 0.006\n",
      "model: 1, epoch: 955, loss: 0.006\n",
      "model: 1, epoch: 956, loss: 0.006\n",
      "model: 1, epoch: 957, loss: 0.006\n",
      "model: 1, epoch: 958, loss: 0.006\n",
      "model: 1, epoch: 959, loss: 0.006\n",
      "model: 1, epoch: 960, loss: 0.006\n",
      "model: 1, epoch: 961, loss: 0.006\n",
      "model: 1, epoch: 962, loss: 0.006\n",
      "model: 1, epoch: 963, loss: 0.006\n",
      "model: 1, epoch: 964, loss: 0.006\n",
      "model: 1, epoch: 965, loss: 0.006\n",
      "model: 1, epoch: 966, loss: 0.006\n",
      "model: 1, epoch: 967, loss: 0.006\n",
      "model: 1, epoch: 968, loss: 0.006\n",
      "model: 1, epoch: 969, loss: 0.006\n",
      "model: 1, epoch: 970, loss: 0.006\n",
      "model: 1, epoch: 971, loss: 0.006\n",
      "model: 1, epoch: 972, loss: 0.006\n",
      "model: 1, epoch: 973, loss: 0.006\n",
      "model: 1, epoch: 974, loss: 0.006\n",
      "model: 1, epoch: 975, loss: 0.006\n",
      "model: 1, epoch: 976, loss: 0.006\n",
      "model: 1, epoch: 977, loss: 0.006\n",
      "model: 1, epoch: 978, loss: 0.006\n",
      "model: 1, epoch: 979, loss: 0.006\n",
      "model: 1, epoch: 980, loss: 0.006\n",
      "model: 1, epoch: 981, loss: 0.006\n",
      "model: 1, epoch: 982, loss: 0.006\n",
      "model: 1, epoch: 983, loss: 0.006\n",
      "model: 1, epoch: 984, loss: 0.006\n",
      "model: 1, epoch: 985, loss: 0.006\n",
      "model: 1, epoch: 986, loss: 0.006\n",
      "model: 1, epoch: 987, loss: 0.006\n",
      "model: 1, epoch: 988, loss: 0.006\n",
      "model: 1, epoch: 989, loss: 0.006\n",
      "model: 1, epoch: 990, loss: 0.006\n",
      "model: 1, epoch: 991, loss: 0.006\n",
      "model: 1, epoch: 992, loss: 0.006\n",
      "model: 1, epoch: 993, loss: 0.006\n",
      "model: 1, epoch: 994, loss: 0.006\n",
      "model: 1, epoch: 995, loss: 0.006\n",
      "model: 1, epoch: 996, loss: 0.006\n",
      "model: 1, epoch: 997, loss: 0.006\n",
      "model: 1, epoch: 998, loss: 0.006\n",
      "model: 1, epoch: 999, loss: 0.006\n",
      "model: 2, epoch: 0, loss: 0.214\n",
      "model: 2, epoch: 1, loss: 0.183\n",
      "model: 2, epoch: 2, loss: 0.156\n",
      "model: 2, epoch: 3, loss: 0.133\n",
      "model: 2, epoch: 4, loss: 0.114\n",
      "model: 2, epoch: 5, loss: 0.098\n",
      "model: 2, epoch: 6, loss: 0.086\n",
      "model: 2, epoch: 7, loss: 0.077\n",
      "model: 2, epoch: 8, loss: 0.070\n",
      "model: 2, epoch: 9, loss: 0.066\n",
      "model: 2, epoch: 10, loss: 0.062\n",
      "model: 2, epoch: 11, loss: 0.059\n",
      "model: 2, epoch: 12, loss: 0.056\n",
      "model: 2, epoch: 13, loss: 0.054\n",
      "model: 2, epoch: 14, loss: 0.051\n",
      "model: 2, epoch: 15, loss: 0.048\n",
      "model: 2, epoch: 16, loss: 0.045\n",
      "model: 2, epoch: 17, loss: 0.042\n",
      "model: 2, epoch: 18, loss: 0.040\n",
      "model: 2, epoch: 19, loss: 0.037\n",
      "model: 2, epoch: 20, loss: 0.035\n",
      "model: 2, epoch: 21, loss: 0.033\n",
      "model: 2, epoch: 22, loss: 0.032\n",
      "model: 2, epoch: 23, loss: 0.031\n",
      "model: 2, epoch: 24, loss: 0.030\n",
      "model: 2, epoch: 25, loss: 0.030\n",
      "model: 2, epoch: 26, loss: 0.031\n",
      "model: 2, epoch: 27, loss: 0.031\n",
      "model: 2, epoch: 28, loss: 0.032\n",
      "model: 2, epoch: 29, loss: 0.032\n",
      "model: 2, epoch: 30, loss: 0.033\n",
      "model: 2, epoch: 31, loss: 0.033\n",
      "model: 2, epoch: 32, loss: 0.033\n",
      "model: 2, epoch: 33, loss: 0.033\n",
      "model: 2, epoch: 34, loss: 0.033\n",
      "model: 2, epoch: 35, loss: 0.033\n",
      "model: 2, epoch: 36, loss: 0.032\n",
      "model: 2, epoch: 37, loss: 0.032\n",
      "model: 2, epoch: 38, loss: 0.032\n",
      "model: 2, epoch: 39, loss: 0.032\n",
      "model: 2, epoch: 40, loss: 0.032\n",
      "model: 2, epoch: 41, loss: 0.032\n",
      "model: 2, epoch: 42, loss: 0.032\n",
      "model: 2, epoch: 43, loss: 0.031\n",
      "model: 2, epoch: 44, loss: 0.031\n",
      "model: 2, epoch: 45, loss: 0.031\n",
      "model: 2, epoch: 46, loss: 0.031\n",
      "model: 2, epoch: 47, loss: 0.031\n",
      "model: 2, epoch: 48, loss: 0.031\n",
      "model: 2, epoch: 49, loss: 0.031\n",
      "model: 2, epoch: 50, loss: 0.031\n",
      "model: 2, epoch: 51, loss: 0.030\n",
      "model: 2, epoch: 52, loss: 0.030\n",
      "model: 2, epoch: 53, loss: 0.030\n",
      "model: 2, epoch: 54, loss: 0.030\n",
      "model: 2, epoch: 55, loss: 0.030\n",
      "model: 2, epoch: 56, loss: 0.030\n",
      "model: 2, epoch: 57, loss: 0.030\n",
      "model: 2, epoch: 58, loss: 0.030\n",
      "model: 2, epoch: 59, loss: 0.030\n",
      "model: 2, epoch: 60, loss: 0.030\n",
      "model: 2, epoch: 61, loss: 0.030\n",
      "model: 2, epoch: 62, loss: 0.030\n",
      "model: 2, epoch: 63, loss: 0.030\n",
      "model: 2, epoch: 64, loss: 0.030\n",
      "model: 2, epoch: 65, loss: 0.030\n",
      "model: 2, epoch: 66, loss: 0.030\n",
      "model: 2, epoch: 67, loss: 0.030\n",
      "model: 2, epoch: 68, loss: 0.030\n",
      "model: 2, epoch: 69, loss: 0.030\n",
      "model: 2, epoch: 70, loss: 0.030\n",
      "model: 2, epoch: 71, loss: 0.030\n",
      "model: 2, epoch: 72, loss: 0.030\n",
      "model: 2, epoch: 73, loss: 0.030\n",
      "model: 2, epoch: 74, loss: 0.030\n",
      "model: 2, epoch: 75, loss: 0.030\n",
      "model: 2, epoch: 76, loss: 0.030\n",
      "model: 2, epoch: 77, loss: 0.030\n",
      "model: 2, epoch: 78, loss: 0.030\n",
      "model: 2, epoch: 79, loss: 0.030\n",
      "model: 2, epoch: 80, loss: 0.030\n",
      "model: 2, epoch: 81, loss: 0.030\n",
      "model: 2, epoch: 82, loss: 0.030\n",
      "model: 2, epoch: 83, loss: 0.030\n",
      "model: 2, epoch: 84, loss: 0.030\n",
      "model: 2, epoch: 85, loss: 0.030\n",
      "model: 2, epoch: 86, loss: 0.030\n",
      "model: 2, epoch: 87, loss: 0.030\n",
      "model: 2, epoch: 88, loss: 0.030\n",
      "model: 2, epoch: 89, loss: 0.030\n",
      "model: 2, epoch: 90, loss: 0.030\n",
      "model: 2, epoch: 91, loss: 0.030\n",
      "model: 2, epoch: 92, loss: 0.030\n",
      "model: 2, epoch: 93, loss: 0.030\n",
      "model: 2, epoch: 94, loss: 0.030\n",
      "model: 2, epoch: 95, loss: 0.030\n",
      "model: 2, epoch: 96, loss: 0.030\n",
      "model: 2, epoch: 97, loss: 0.030\n",
      "model: 2, epoch: 98, loss: 0.030\n",
      "model: 2, epoch: 99, loss: 0.030\n",
      "model: 2, epoch: 100, loss: 0.030\n",
      "model: 2, epoch: 101, loss: 0.030\n",
      "model: 2, epoch: 102, loss: 0.030\n",
      "model: 2, epoch: 103, loss: 0.030\n",
      "model: 2, epoch: 104, loss: 0.030\n",
      "model: 2, epoch: 105, loss: 0.030\n",
      "model: 2, epoch: 106, loss: 0.030\n",
      "model: 2, epoch: 107, loss: 0.030\n",
      "model: 2, epoch: 108, loss: 0.030\n",
      "model: 2, epoch: 109, loss: 0.030\n",
      "model: 2, epoch: 110, loss: 0.030\n",
      "model: 2, epoch: 111, loss: 0.030\n",
      "model: 2, epoch: 112, loss: 0.030\n",
      "model: 2, epoch: 113, loss: 0.030\n",
      "model: 2, epoch: 114, loss: 0.030\n",
      "model: 2, epoch: 115, loss: 0.030\n",
      "model: 2, epoch: 116, loss: 0.030\n",
      "model: 2, epoch: 117, loss: 0.030\n",
      "model: 2, epoch: 118, loss: 0.030\n",
      "model: 2, epoch: 119, loss: 0.030\n",
      "model: 2, epoch: 120, loss: 0.030\n",
      "model: 2, epoch: 121, loss: 0.030\n",
      "model: 2, epoch: 122, loss: 0.030\n",
      "model: 2, epoch: 123, loss: 0.030\n",
      "model: 2, epoch: 124, loss: 0.030\n",
      "model: 2, epoch: 125, loss: 0.030\n",
      "model: 2, epoch: 126, loss: 0.030\n",
      "model: 2, epoch: 127, loss: 0.030\n",
      "model: 2, epoch: 128, loss: 0.030\n",
      "model: 2, epoch: 129, loss: 0.030\n",
      "model: 2, epoch: 130, loss: 0.030\n",
      "model: 2, epoch: 131, loss: 0.030\n",
      "model: 2, epoch: 132, loss: 0.030\n",
      "model: 2, epoch: 133, loss: 0.030\n",
      "model: 2, epoch: 134, loss: 0.030\n",
      "model: 2, epoch: 135, loss: 0.030\n",
      "model: 2, epoch: 136, loss: 0.030\n",
      "model: 2, epoch: 137, loss: 0.030\n",
      "model: 2, epoch: 138, loss: 0.030\n",
      "model: 2, epoch: 139, loss: 0.030\n",
      "model: 2, epoch: 140, loss: 0.030\n",
      "model: 2, epoch: 141, loss: 0.030\n",
      "model: 2, epoch: 142, loss: 0.030\n",
      "model: 2, epoch: 143, loss: 0.030\n",
      "model: 2, epoch: 144, loss: 0.030\n",
      "model: 2, epoch: 145, loss: 0.029\n",
      "model: 2, epoch: 146, loss: 0.029\n",
      "model: 2, epoch: 147, loss: 0.029\n",
      "model: 2, epoch: 148, loss: 0.029\n",
      "model: 2, epoch: 149, loss: 0.029\n",
      "model: 2, epoch: 150, loss: 0.029\n",
      "model: 2, epoch: 151, loss: 0.029\n",
      "model: 2, epoch: 152, loss: 0.029\n",
      "model: 2, epoch: 153, loss: 0.029\n",
      "model: 2, epoch: 154, loss: 0.029\n",
      "model: 2, epoch: 155, loss: 0.029\n",
      "model: 2, epoch: 156, loss: 0.029\n",
      "model: 2, epoch: 157, loss: 0.029\n",
      "model: 2, epoch: 158, loss: 0.029\n",
      "model: 2, epoch: 159, loss: 0.029\n",
      "model: 2, epoch: 160, loss: 0.029\n",
      "model: 2, epoch: 161, loss: 0.029\n",
      "model: 2, epoch: 162, loss: 0.029\n",
      "model: 2, epoch: 163, loss: 0.029\n",
      "model: 2, epoch: 164, loss: 0.029\n",
      "model: 2, epoch: 165, loss: 0.029\n",
      "model: 2, epoch: 166, loss: 0.029\n",
      "model: 2, epoch: 167, loss: 0.029\n",
      "model: 2, epoch: 168, loss: 0.029\n",
      "model: 2, epoch: 169, loss: 0.029\n",
      "model: 2, epoch: 170, loss: 0.029\n",
      "model: 2, epoch: 171, loss: 0.029\n",
      "model: 2, epoch: 172, loss: 0.029\n",
      "model: 2, epoch: 173, loss: 0.029\n",
      "model: 2, epoch: 174, loss: 0.029\n",
      "model: 2, epoch: 175, loss: 0.029\n",
      "model: 2, epoch: 176, loss: 0.029\n",
      "model: 2, epoch: 177, loss: 0.029\n",
      "model: 2, epoch: 178, loss: 0.029\n",
      "model: 2, epoch: 179, loss: 0.029\n",
      "model: 2, epoch: 180, loss: 0.029\n",
      "model: 2, epoch: 181, loss: 0.029\n",
      "model: 2, epoch: 182, loss: 0.029\n",
      "model: 2, epoch: 183, loss: 0.029\n",
      "model: 2, epoch: 184, loss: 0.029\n",
      "model: 2, epoch: 185, loss: 0.029\n",
      "model: 2, epoch: 186, loss: 0.029\n",
      "model: 2, epoch: 187, loss: 0.029\n",
      "model: 2, epoch: 188, loss: 0.029\n",
      "model: 2, epoch: 189, loss: 0.029\n",
      "model: 2, epoch: 190, loss: 0.029\n",
      "model: 2, epoch: 191, loss: 0.029\n",
      "model: 2, epoch: 192, loss: 0.029\n",
      "model: 2, epoch: 193, loss: 0.029\n",
      "model: 2, epoch: 194, loss: 0.029\n",
      "model: 2, epoch: 195, loss: 0.029\n",
      "model: 2, epoch: 196, loss: 0.029\n",
      "model: 2, epoch: 197, loss: 0.029\n",
      "model: 2, epoch: 198, loss: 0.029\n",
      "model: 2, epoch: 199, loss: 0.029\n",
      "model: 2, epoch: 200, loss: 0.029\n",
      "model: 2, epoch: 201, loss: 0.029\n",
      "model: 2, epoch: 202, loss: 0.029\n",
      "model: 2, epoch: 203, loss: 0.029\n",
      "model: 2, epoch: 204, loss: 0.029\n",
      "model: 2, epoch: 205, loss: 0.029\n",
      "model: 2, epoch: 206, loss: 0.029\n",
      "model: 2, epoch: 207, loss: 0.029\n",
      "model: 2, epoch: 208, loss: 0.029\n",
      "model: 2, epoch: 209, loss: 0.029\n",
      "model: 2, epoch: 210, loss: 0.029\n",
      "model: 2, epoch: 211, loss: 0.029\n",
      "model: 2, epoch: 212, loss: 0.029\n",
      "model: 2, epoch: 213, loss: 0.029\n",
      "model: 2, epoch: 214, loss: 0.029\n",
      "model: 2, epoch: 215, loss: 0.029\n",
      "model: 2, epoch: 216, loss: 0.029\n",
      "model: 2, epoch: 217, loss: 0.029\n",
      "model: 2, epoch: 218, loss: 0.029\n",
      "model: 2, epoch: 219, loss: 0.029\n",
      "model: 2, epoch: 220, loss: 0.029\n",
      "model: 2, epoch: 221, loss: 0.029\n",
      "model: 2, epoch: 222, loss: 0.029\n",
      "model: 2, epoch: 223, loss: 0.029\n",
      "model: 2, epoch: 224, loss: 0.029\n",
      "model: 2, epoch: 225, loss: 0.029\n",
      "model: 2, epoch: 226, loss: 0.029\n",
      "model: 2, epoch: 227, loss: 0.029\n",
      "model: 2, epoch: 228, loss: 0.029\n",
      "model: 2, epoch: 229, loss: 0.029\n",
      "model: 2, epoch: 230, loss: 0.029\n",
      "model: 2, epoch: 231, loss: 0.029\n",
      "model: 2, epoch: 232, loss: 0.029\n",
      "model: 2, epoch: 233, loss: 0.029\n",
      "model: 2, epoch: 234, loss: 0.029\n",
      "model: 2, epoch: 235, loss: 0.029\n",
      "model: 2, epoch: 236, loss: 0.029\n",
      "model: 2, epoch: 237, loss: 0.029\n",
      "model: 2, epoch: 238, loss: 0.029\n",
      "model: 2, epoch: 239, loss: 0.029\n",
      "model: 2, epoch: 240, loss: 0.029\n",
      "model: 2, epoch: 241, loss: 0.029\n",
      "model: 2, epoch: 242, loss: 0.029\n",
      "model: 2, epoch: 243, loss: 0.029\n",
      "model: 2, epoch: 244, loss: 0.029\n",
      "model: 2, epoch: 245, loss: 0.029\n",
      "model: 2, epoch: 246, loss: 0.029\n",
      "model: 2, epoch: 247, loss: 0.029\n",
      "model: 2, epoch: 248, loss: 0.029\n",
      "model: 2, epoch: 249, loss: 0.029\n",
      "model: 2, epoch: 250, loss: 0.029\n",
      "model: 2, epoch: 251, loss: 0.029\n",
      "model: 2, epoch: 252, loss: 0.029\n",
      "model: 2, epoch: 253, loss: 0.028\n",
      "model: 2, epoch: 254, loss: 0.028\n",
      "model: 2, epoch: 255, loss: 0.028\n",
      "model: 2, epoch: 256, loss: 0.028\n",
      "model: 2, epoch: 257, loss: 0.028\n",
      "model: 2, epoch: 258, loss: 0.028\n",
      "model: 2, epoch: 259, loss: 0.028\n",
      "model: 2, epoch: 260, loss: 0.028\n",
      "model: 2, epoch: 261, loss: 0.028\n",
      "model: 2, epoch: 262, loss: 0.028\n",
      "model: 2, epoch: 263, loss: 0.028\n",
      "model: 2, epoch: 264, loss: 0.028\n",
      "model: 2, epoch: 265, loss: 0.028\n",
      "model: 2, epoch: 266, loss: 0.028\n",
      "model: 2, epoch: 267, loss: 0.028\n",
      "model: 2, epoch: 268, loss: 0.028\n",
      "model: 2, epoch: 269, loss: 0.028\n",
      "model: 2, epoch: 270, loss: 0.028\n",
      "model: 2, epoch: 271, loss: 0.028\n",
      "model: 2, epoch: 272, loss: 0.028\n",
      "model: 2, epoch: 273, loss: 0.028\n",
      "model: 2, epoch: 274, loss: 0.028\n",
      "model: 2, epoch: 275, loss: 0.028\n",
      "model: 2, epoch: 276, loss: 0.028\n",
      "model: 2, epoch: 277, loss: 0.028\n",
      "model: 2, epoch: 278, loss: 0.028\n",
      "model: 2, epoch: 279, loss: 0.028\n",
      "model: 2, epoch: 280, loss: 0.028\n",
      "model: 2, epoch: 281, loss: 0.028\n",
      "model: 2, epoch: 282, loss: 0.028\n",
      "model: 2, epoch: 283, loss: 0.028\n",
      "model: 2, epoch: 284, loss: 0.028\n",
      "model: 2, epoch: 285, loss: 0.028\n",
      "model: 2, epoch: 286, loss: 0.028\n",
      "model: 2, epoch: 287, loss: 0.028\n",
      "model: 2, epoch: 288, loss: 0.028\n",
      "model: 2, epoch: 289, loss: 0.028\n",
      "model: 2, epoch: 290, loss: 0.028\n",
      "model: 2, epoch: 291, loss: 0.028\n",
      "model: 2, epoch: 292, loss: 0.028\n",
      "model: 2, epoch: 293, loss: 0.028\n",
      "model: 2, epoch: 294, loss: 0.028\n",
      "model: 2, epoch: 295, loss: 0.028\n",
      "model: 2, epoch: 296, loss: 0.028\n",
      "model: 2, epoch: 297, loss: 0.028\n",
      "model: 2, epoch: 298, loss: 0.028\n",
      "model: 2, epoch: 299, loss: 0.028\n",
      "model: 2, epoch: 300, loss: 0.028\n",
      "model: 2, epoch: 301, loss: 0.028\n",
      "model: 2, epoch: 302, loss: 0.028\n",
      "model: 2, epoch: 303, loss: 0.028\n",
      "model: 2, epoch: 304, loss: 0.028\n",
      "model: 2, epoch: 305, loss: 0.028\n",
      "model: 2, epoch: 306, loss: 0.028\n",
      "model: 2, epoch: 307, loss: 0.028\n",
      "model: 2, epoch: 308, loss: 0.028\n",
      "model: 2, epoch: 309, loss: 0.028\n",
      "model: 2, epoch: 310, loss: 0.028\n",
      "model: 2, epoch: 311, loss: 0.028\n",
      "model: 2, epoch: 312, loss: 0.028\n",
      "model: 2, epoch: 313, loss: 0.028\n",
      "model: 2, epoch: 314, loss: 0.028\n",
      "model: 2, epoch: 315, loss: 0.028\n",
      "model: 2, epoch: 316, loss: 0.028\n",
      "model: 2, epoch: 317, loss: 0.028\n",
      "model: 2, epoch: 318, loss: 0.028\n",
      "model: 2, epoch: 319, loss: 0.028\n",
      "model: 2, epoch: 320, loss: 0.028\n",
      "model: 2, epoch: 321, loss: 0.028\n",
      "model: 2, epoch: 322, loss: 0.028\n",
      "model: 2, epoch: 323, loss: 0.028\n",
      "model: 2, epoch: 324, loss: 0.028\n",
      "model: 2, epoch: 325, loss: 0.028\n",
      "model: 2, epoch: 326, loss: 0.028\n",
      "model: 2, epoch: 327, loss: 0.028\n",
      "model: 2, epoch: 328, loss: 0.028\n",
      "model: 2, epoch: 329, loss: 0.028\n",
      "model: 2, epoch: 330, loss: 0.028\n",
      "model: 2, epoch: 331, loss: 0.028\n",
      "model: 2, epoch: 332, loss: 0.028\n",
      "model: 2, epoch: 333, loss: 0.028\n",
      "model: 2, epoch: 334, loss: 0.028\n",
      "model: 2, epoch: 335, loss: 0.028\n",
      "model: 2, epoch: 336, loss: 0.028\n",
      "model: 2, epoch: 337, loss: 0.028\n",
      "model: 2, epoch: 338, loss: 0.028\n",
      "model: 2, epoch: 339, loss: 0.028\n",
      "model: 2, epoch: 340, loss: 0.028\n",
      "model: 2, epoch: 341, loss: 0.028\n",
      "model: 2, epoch: 342, loss: 0.028\n",
      "model: 2, epoch: 343, loss: 0.028\n",
      "model: 2, epoch: 344, loss: 0.028\n",
      "model: 2, epoch: 345, loss: 0.028\n",
      "model: 2, epoch: 346, loss: 0.028\n",
      "model: 2, epoch: 347, loss: 0.028\n",
      "model: 2, epoch: 348, loss: 0.027\n",
      "model: 2, epoch: 349, loss: 0.027\n",
      "model: 2, epoch: 350, loss: 0.027\n",
      "model: 2, epoch: 351, loss: 0.027\n",
      "model: 2, epoch: 352, loss: 0.027\n",
      "model: 2, epoch: 353, loss: 0.027\n",
      "model: 2, epoch: 354, loss: 0.027\n",
      "model: 2, epoch: 355, loss: 0.027\n",
      "model: 2, epoch: 356, loss: 0.027\n",
      "model: 2, epoch: 357, loss: 0.027\n",
      "model: 2, epoch: 358, loss: 0.027\n",
      "model: 2, epoch: 359, loss: 0.027\n",
      "model: 2, epoch: 360, loss: 0.027\n",
      "model: 2, epoch: 361, loss: 0.027\n",
      "model: 2, epoch: 362, loss: 0.027\n",
      "model: 2, epoch: 363, loss: 0.027\n",
      "model: 2, epoch: 364, loss: 0.027\n",
      "model: 2, epoch: 365, loss: 0.027\n",
      "model: 2, epoch: 366, loss: 0.027\n",
      "model: 2, epoch: 367, loss: 0.027\n",
      "model: 2, epoch: 368, loss: 0.027\n",
      "model: 2, epoch: 369, loss: 0.027\n",
      "model: 2, epoch: 370, loss: 0.027\n",
      "model: 2, epoch: 371, loss: 0.027\n",
      "model: 2, epoch: 372, loss: 0.027\n",
      "model: 2, epoch: 373, loss: 0.027\n",
      "model: 2, epoch: 374, loss: 0.027\n",
      "model: 2, epoch: 375, loss: 0.027\n",
      "model: 2, epoch: 376, loss: 0.027\n",
      "model: 2, epoch: 377, loss: 0.027\n",
      "model: 2, epoch: 378, loss: 0.027\n",
      "model: 2, epoch: 379, loss: 0.027\n",
      "model: 2, epoch: 380, loss: 0.027\n",
      "model: 2, epoch: 381, loss: 0.027\n",
      "model: 2, epoch: 382, loss: 0.027\n",
      "model: 2, epoch: 383, loss: 0.027\n",
      "model: 2, epoch: 384, loss: 0.027\n",
      "model: 2, epoch: 385, loss: 0.027\n",
      "model: 2, epoch: 386, loss: 0.027\n",
      "model: 2, epoch: 387, loss: 0.027\n",
      "model: 2, epoch: 388, loss: 0.027\n",
      "model: 2, epoch: 389, loss: 0.027\n",
      "model: 2, epoch: 390, loss: 0.027\n",
      "model: 2, epoch: 391, loss: 0.027\n",
      "model: 2, epoch: 392, loss: 0.027\n",
      "model: 2, epoch: 393, loss: 0.027\n",
      "model: 2, epoch: 394, loss: 0.027\n",
      "model: 2, epoch: 395, loss: 0.027\n",
      "model: 2, epoch: 396, loss: 0.027\n",
      "model: 2, epoch: 397, loss: 0.027\n",
      "model: 2, epoch: 398, loss: 0.027\n",
      "model: 2, epoch: 399, loss: 0.027\n",
      "model: 2, epoch: 400, loss: 0.027\n",
      "model: 2, epoch: 401, loss: 0.027\n",
      "model: 2, epoch: 402, loss: 0.027\n",
      "model: 2, epoch: 403, loss: 0.027\n",
      "model: 2, epoch: 404, loss: 0.027\n",
      "model: 2, epoch: 405, loss: 0.027\n",
      "model: 2, epoch: 406, loss: 0.027\n",
      "model: 2, epoch: 407, loss: 0.027\n",
      "model: 2, epoch: 408, loss: 0.027\n",
      "model: 2, epoch: 409, loss: 0.027\n",
      "model: 2, epoch: 410, loss: 0.027\n",
      "model: 2, epoch: 411, loss: 0.027\n",
      "model: 2, epoch: 412, loss: 0.027\n",
      "model: 2, epoch: 413, loss: 0.027\n",
      "model: 2, epoch: 414, loss: 0.027\n",
      "model: 2, epoch: 415, loss: 0.027\n",
      "model: 2, epoch: 416, loss: 0.027\n",
      "model: 2, epoch: 417, loss: 0.027\n",
      "model: 2, epoch: 418, loss: 0.027\n",
      "model: 2, epoch: 419, loss: 0.027\n",
      "model: 2, epoch: 420, loss: 0.027\n",
      "model: 2, epoch: 421, loss: 0.027\n",
      "model: 2, epoch: 422, loss: 0.027\n",
      "model: 2, epoch: 423, loss: 0.027\n",
      "model: 2, epoch: 424, loss: 0.027\n",
      "model: 2, epoch: 425, loss: 0.027\n",
      "model: 2, epoch: 426, loss: 0.027\n",
      "model: 2, epoch: 427, loss: 0.027\n",
      "model: 2, epoch: 428, loss: 0.027\n",
      "model: 2, epoch: 429, loss: 0.027\n",
      "model: 2, epoch: 430, loss: 0.026\n",
      "model: 2, epoch: 431, loss: 0.026\n",
      "model: 2, epoch: 432, loss: 0.026\n",
      "model: 2, epoch: 433, loss: 0.026\n",
      "model: 2, epoch: 434, loss: 0.026\n",
      "model: 2, epoch: 435, loss: 0.026\n",
      "model: 2, epoch: 436, loss: 0.026\n",
      "model: 2, epoch: 437, loss: 0.026\n",
      "model: 2, epoch: 438, loss: 0.026\n",
      "model: 2, epoch: 439, loss: 0.026\n",
      "model: 2, epoch: 440, loss: 0.026\n",
      "model: 2, epoch: 441, loss: 0.026\n",
      "model: 2, epoch: 442, loss: 0.026\n",
      "model: 2, epoch: 443, loss: 0.026\n",
      "model: 2, epoch: 444, loss: 0.026\n",
      "model: 2, epoch: 445, loss: 0.026\n",
      "model: 2, epoch: 446, loss: 0.026\n",
      "model: 2, epoch: 447, loss: 0.026\n",
      "model: 2, epoch: 448, loss: 0.026\n",
      "model: 2, epoch: 449, loss: 0.026\n",
      "model: 2, epoch: 450, loss: 0.026\n",
      "model: 2, epoch: 451, loss: 0.026\n",
      "model: 2, epoch: 452, loss: 0.026\n",
      "model: 2, epoch: 453, loss: 0.026\n",
      "model: 2, epoch: 454, loss: 0.026\n",
      "model: 2, epoch: 455, loss: 0.026\n",
      "model: 2, epoch: 456, loss: 0.026\n",
      "model: 2, epoch: 457, loss: 0.026\n",
      "model: 2, epoch: 458, loss: 0.026\n",
      "model: 2, epoch: 459, loss: 0.026\n",
      "model: 2, epoch: 460, loss: 0.026\n",
      "model: 2, epoch: 461, loss: 0.026\n",
      "model: 2, epoch: 462, loss: 0.026\n",
      "model: 2, epoch: 463, loss: 0.026\n",
      "model: 2, epoch: 464, loss: 0.026\n",
      "model: 2, epoch: 465, loss: 0.026\n",
      "model: 2, epoch: 466, loss: 0.026\n",
      "model: 2, epoch: 467, loss: 0.026\n",
      "model: 2, epoch: 468, loss: 0.026\n",
      "model: 2, epoch: 469, loss: 0.026\n",
      "model: 2, epoch: 470, loss: 0.026\n",
      "model: 2, epoch: 471, loss: 0.026\n",
      "model: 2, epoch: 472, loss: 0.026\n",
      "model: 2, epoch: 473, loss: 0.026\n",
      "model: 2, epoch: 474, loss: 0.026\n",
      "model: 2, epoch: 475, loss: 0.026\n",
      "model: 2, epoch: 476, loss: 0.026\n",
      "model: 2, epoch: 477, loss: 0.026\n",
      "model: 2, epoch: 478, loss: 0.026\n",
      "model: 2, epoch: 479, loss: 0.026\n",
      "model: 2, epoch: 480, loss: 0.026\n",
      "model: 2, epoch: 481, loss: 0.026\n",
      "model: 2, epoch: 482, loss: 0.026\n",
      "model: 2, epoch: 483, loss: 0.026\n",
      "model: 2, epoch: 484, loss: 0.026\n",
      "model: 2, epoch: 485, loss: 0.026\n",
      "model: 2, epoch: 486, loss: 0.026\n",
      "model: 2, epoch: 487, loss: 0.026\n",
      "model: 2, epoch: 488, loss: 0.026\n",
      "model: 2, epoch: 489, loss: 0.026\n",
      "model: 2, epoch: 490, loss: 0.026\n",
      "model: 2, epoch: 491, loss: 0.026\n",
      "model: 2, epoch: 492, loss: 0.026\n",
      "model: 2, epoch: 493, loss: 0.026\n",
      "model: 2, epoch: 494, loss: 0.026\n",
      "model: 2, epoch: 495, loss: 0.026\n",
      "model: 2, epoch: 496, loss: 0.026\n",
      "model: 2, epoch: 497, loss: 0.025\n",
      "model: 2, epoch: 498, loss: 0.025\n",
      "model: 2, epoch: 499, loss: 0.025\n",
      "model: 2, epoch: 500, loss: 0.025\n",
      "model: 2, epoch: 501, loss: 0.025\n",
      "model: 2, epoch: 502, loss: 0.025\n",
      "model: 2, epoch: 503, loss: 0.025\n",
      "model: 2, epoch: 504, loss: 0.025\n",
      "model: 2, epoch: 505, loss: 0.025\n",
      "model: 2, epoch: 506, loss: 0.025\n",
      "model: 2, epoch: 507, loss: 0.025\n",
      "model: 2, epoch: 508, loss: 0.025\n",
      "model: 2, epoch: 509, loss: 0.025\n",
      "model: 2, epoch: 510, loss: 0.025\n",
      "model: 2, epoch: 511, loss: 0.025\n",
      "model: 2, epoch: 512, loss: 0.025\n",
      "model: 2, epoch: 513, loss: 0.025\n",
      "model: 2, epoch: 514, loss: 0.025\n",
      "model: 2, epoch: 515, loss: 0.025\n",
      "model: 2, epoch: 516, loss: 0.025\n",
      "model: 2, epoch: 517, loss: 0.025\n",
      "model: 2, epoch: 518, loss: 0.025\n",
      "model: 2, epoch: 519, loss: 0.025\n",
      "model: 2, epoch: 520, loss: 0.025\n",
      "model: 2, epoch: 521, loss: 0.025\n",
      "model: 2, epoch: 522, loss: 0.025\n",
      "model: 2, epoch: 523, loss: 0.025\n",
      "model: 2, epoch: 524, loss: 0.025\n",
      "model: 2, epoch: 525, loss: 0.025\n",
      "model: 2, epoch: 526, loss: 0.025\n",
      "model: 2, epoch: 527, loss: 0.025\n",
      "model: 2, epoch: 528, loss: 0.025\n",
      "model: 2, epoch: 529, loss: 0.025\n",
      "model: 2, epoch: 530, loss: 0.025\n",
      "model: 2, epoch: 531, loss: 0.025\n",
      "model: 2, epoch: 532, loss: 0.025\n",
      "model: 2, epoch: 533, loss: 0.025\n",
      "model: 2, epoch: 534, loss: 0.025\n",
      "model: 2, epoch: 535, loss: 0.025\n",
      "model: 2, epoch: 536, loss: 0.025\n",
      "model: 2, epoch: 537, loss: 0.025\n",
      "model: 2, epoch: 538, loss: 0.025\n",
      "model: 2, epoch: 539, loss: 0.025\n",
      "model: 2, epoch: 540, loss: 0.025\n",
      "model: 2, epoch: 541, loss: 0.025\n",
      "model: 2, epoch: 542, loss: 0.025\n",
      "model: 2, epoch: 543, loss: 0.025\n",
      "model: 2, epoch: 544, loss: 0.025\n",
      "model: 2, epoch: 545, loss: 0.025\n",
      "model: 2, epoch: 546, loss: 0.025\n",
      "model: 2, epoch: 547, loss: 0.025\n",
      "model: 2, epoch: 548, loss: 0.025\n",
      "model: 2, epoch: 549, loss: 0.025\n",
      "model: 2, epoch: 550, loss: 0.025\n",
      "model: 2, epoch: 551, loss: 0.024\n",
      "model: 2, epoch: 552, loss: 0.024\n",
      "model: 2, epoch: 553, loss: 0.024\n",
      "model: 2, epoch: 554, loss: 0.024\n",
      "model: 2, epoch: 555, loss: 0.024\n",
      "model: 2, epoch: 556, loss: 0.024\n",
      "model: 2, epoch: 557, loss: 0.024\n",
      "model: 2, epoch: 558, loss: 0.024\n",
      "model: 2, epoch: 559, loss: 0.024\n",
      "model: 2, epoch: 560, loss: 0.024\n",
      "model: 2, epoch: 561, loss: 0.024\n",
      "model: 2, epoch: 562, loss: 0.024\n",
      "model: 2, epoch: 563, loss: 0.024\n",
      "model: 2, epoch: 564, loss: 0.024\n",
      "model: 2, epoch: 565, loss: 0.024\n",
      "model: 2, epoch: 566, loss: 0.024\n",
      "model: 2, epoch: 567, loss: 0.024\n",
      "model: 2, epoch: 568, loss: 0.024\n",
      "model: 2, epoch: 569, loss: 0.024\n",
      "model: 2, epoch: 570, loss: 0.024\n",
      "model: 2, epoch: 571, loss: 0.024\n",
      "model: 2, epoch: 572, loss: 0.024\n",
      "model: 2, epoch: 573, loss: 0.024\n",
      "model: 2, epoch: 574, loss: 0.024\n",
      "model: 2, epoch: 575, loss: 0.024\n",
      "model: 2, epoch: 576, loss: 0.024\n",
      "model: 2, epoch: 577, loss: 0.024\n",
      "model: 2, epoch: 578, loss: 0.024\n",
      "model: 2, epoch: 579, loss: 0.024\n",
      "model: 2, epoch: 580, loss: 0.024\n",
      "model: 2, epoch: 581, loss: 0.024\n",
      "model: 2, epoch: 582, loss: 0.024\n",
      "model: 2, epoch: 583, loss: 0.024\n",
      "model: 2, epoch: 584, loss: 0.024\n",
      "model: 2, epoch: 585, loss: 0.024\n",
      "model: 2, epoch: 586, loss: 0.024\n",
      "model: 2, epoch: 587, loss: 0.024\n",
      "model: 2, epoch: 588, loss: 0.024\n",
      "model: 2, epoch: 589, loss: 0.024\n",
      "model: 2, epoch: 590, loss: 0.024\n",
      "model: 2, epoch: 591, loss: 0.024\n",
      "model: 2, epoch: 592, loss: 0.024\n",
      "model: 2, epoch: 593, loss: 0.024\n",
      "model: 2, epoch: 594, loss: 0.024\n",
      "model: 2, epoch: 595, loss: 0.024\n",
      "model: 2, epoch: 596, loss: 0.024\n",
      "model: 2, epoch: 597, loss: 0.023\n",
      "model: 2, epoch: 598, loss: 0.023\n",
      "model: 2, epoch: 599, loss: 0.023\n",
      "model: 2, epoch: 600, loss: 0.023\n",
      "model: 2, epoch: 601, loss: 0.023\n",
      "model: 2, epoch: 602, loss: 0.023\n",
      "model: 2, epoch: 603, loss: 0.023\n",
      "model: 2, epoch: 604, loss: 0.023\n",
      "model: 2, epoch: 605, loss: 0.023\n",
      "model: 2, epoch: 606, loss: 0.023\n",
      "model: 2, epoch: 607, loss: 0.023\n",
      "model: 2, epoch: 608, loss: 0.023\n",
      "model: 2, epoch: 609, loss: 0.023\n",
      "model: 2, epoch: 610, loss: 0.023\n",
      "model: 2, epoch: 611, loss: 0.023\n",
      "model: 2, epoch: 612, loss: 0.023\n",
      "model: 2, epoch: 613, loss: 0.023\n",
      "model: 2, epoch: 614, loss: 0.023\n",
      "model: 2, epoch: 615, loss: 0.023\n",
      "model: 2, epoch: 616, loss: 0.023\n",
      "model: 2, epoch: 617, loss: 0.023\n",
      "model: 2, epoch: 618, loss: 0.023\n",
      "model: 2, epoch: 619, loss: 0.023\n",
      "model: 2, epoch: 620, loss: 0.023\n",
      "model: 2, epoch: 621, loss: 0.023\n",
      "model: 2, epoch: 622, loss: 0.023\n",
      "model: 2, epoch: 623, loss: 0.023\n",
      "model: 2, epoch: 624, loss: 0.023\n",
      "model: 2, epoch: 625, loss: 0.023\n",
      "model: 2, epoch: 626, loss: 0.023\n",
      "model: 2, epoch: 627, loss: 0.023\n",
      "model: 2, epoch: 628, loss: 0.023\n",
      "model: 2, epoch: 629, loss: 0.023\n",
      "model: 2, epoch: 630, loss: 0.023\n",
      "model: 2, epoch: 631, loss: 0.023\n",
      "model: 2, epoch: 632, loss: 0.023\n",
      "model: 2, epoch: 633, loss: 0.023\n",
      "model: 2, epoch: 634, loss: 0.023\n",
      "model: 2, epoch: 635, loss: 0.023\n",
      "model: 2, epoch: 636, loss: 0.022\n",
      "model: 2, epoch: 637, loss: 0.022\n",
      "model: 2, epoch: 638, loss: 0.022\n",
      "model: 2, epoch: 639, loss: 0.022\n",
      "model: 2, epoch: 640, loss: 0.022\n",
      "model: 2, epoch: 641, loss: 0.022\n",
      "model: 2, epoch: 642, loss: 0.022\n",
      "model: 2, epoch: 643, loss: 0.022\n",
      "model: 2, epoch: 644, loss: 0.022\n",
      "model: 2, epoch: 645, loss: 0.022\n",
      "model: 2, epoch: 646, loss: 0.022\n",
      "model: 2, epoch: 647, loss: 0.022\n",
      "model: 2, epoch: 648, loss: 0.022\n",
      "model: 2, epoch: 649, loss: 0.022\n",
      "model: 2, epoch: 650, loss: 0.022\n",
      "model: 2, epoch: 651, loss: 0.022\n",
      "model: 2, epoch: 652, loss: 0.022\n",
      "model: 2, epoch: 653, loss: 0.022\n",
      "model: 2, epoch: 654, loss: 0.022\n",
      "model: 2, epoch: 655, loss: 0.022\n",
      "model: 2, epoch: 656, loss: 0.022\n",
      "model: 2, epoch: 657, loss: 0.022\n",
      "model: 2, epoch: 658, loss: 0.022\n",
      "model: 2, epoch: 659, loss: 0.022\n",
      "model: 2, epoch: 660, loss: 0.022\n",
      "model: 2, epoch: 661, loss: 0.022\n",
      "model: 2, epoch: 662, loss: 0.022\n",
      "model: 2, epoch: 663, loss: 0.022\n",
      "model: 2, epoch: 664, loss: 0.022\n",
      "model: 2, epoch: 665, loss: 0.022\n",
      "model: 2, epoch: 666, loss: 0.022\n",
      "model: 2, epoch: 667, loss: 0.022\n",
      "model: 2, epoch: 668, loss: 0.022\n",
      "model: 2, epoch: 669, loss: 0.022\n",
      "model: 2, epoch: 670, loss: 0.021\n",
      "model: 2, epoch: 671, loss: 0.021\n",
      "model: 2, epoch: 672, loss: 0.021\n",
      "model: 2, epoch: 673, loss: 0.021\n",
      "model: 2, epoch: 674, loss: 0.021\n",
      "model: 2, epoch: 675, loss: 0.021\n",
      "model: 2, epoch: 676, loss: 0.021\n",
      "model: 2, epoch: 677, loss: 0.021\n",
      "model: 2, epoch: 678, loss: 0.021\n",
      "model: 2, epoch: 679, loss: 0.021\n",
      "model: 2, epoch: 680, loss: 0.021\n",
      "model: 2, epoch: 681, loss: 0.021\n",
      "model: 2, epoch: 682, loss: 0.021\n",
      "model: 2, epoch: 683, loss: 0.021\n",
      "model: 2, epoch: 684, loss: 0.021\n",
      "model: 2, epoch: 685, loss: 0.021\n",
      "model: 2, epoch: 686, loss: 0.021\n",
      "model: 2, epoch: 687, loss: 0.021\n",
      "model: 2, epoch: 688, loss: 0.021\n",
      "model: 2, epoch: 689, loss: 0.021\n",
      "model: 2, epoch: 690, loss: 0.021\n",
      "model: 2, epoch: 691, loss: 0.021\n",
      "model: 2, epoch: 692, loss: 0.021\n",
      "model: 2, epoch: 693, loss: 0.021\n",
      "model: 2, epoch: 694, loss: 0.021\n",
      "model: 2, epoch: 695, loss: 0.021\n",
      "model: 2, epoch: 696, loss: 0.021\n",
      "model: 2, epoch: 697, loss: 0.021\n",
      "model: 2, epoch: 698, loss: 0.021\n",
      "model: 2, epoch: 699, loss: 0.021\n",
      "model: 2, epoch: 700, loss: 0.021\n",
      "model: 2, epoch: 701, loss: 0.020\n",
      "model: 2, epoch: 702, loss: 0.020\n",
      "model: 2, epoch: 703, loss: 0.020\n",
      "model: 2, epoch: 704, loss: 0.020\n",
      "model: 2, epoch: 705, loss: 0.020\n",
      "model: 2, epoch: 706, loss: 0.020\n",
      "model: 2, epoch: 707, loss: 0.020\n",
      "model: 2, epoch: 708, loss: 0.020\n",
      "model: 2, epoch: 709, loss: 0.020\n",
      "model: 2, epoch: 710, loss: 0.020\n",
      "model: 2, epoch: 711, loss: 0.020\n",
      "model: 2, epoch: 712, loss: 0.020\n",
      "model: 2, epoch: 713, loss: 0.020\n",
      "model: 2, epoch: 714, loss: 0.020\n",
      "model: 2, epoch: 715, loss: 0.020\n",
      "model: 2, epoch: 716, loss: 0.020\n",
      "model: 2, epoch: 717, loss: 0.020\n",
      "model: 2, epoch: 718, loss: 0.020\n",
      "model: 2, epoch: 719, loss: 0.020\n",
      "model: 2, epoch: 720, loss: 0.020\n",
      "model: 2, epoch: 721, loss: 0.020\n",
      "model: 2, epoch: 722, loss: 0.020\n",
      "model: 2, epoch: 723, loss: 0.020\n",
      "model: 2, epoch: 724, loss: 0.020\n",
      "model: 2, epoch: 725, loss: 0.020\n",
      "model: 2, epoch: 726, loss: 0.020\n",
      "model: 2, epoch: 727, loss: 0.020\n",
      "model: 2, epoch: 728, loss: 0.020\n",
      "model: 2, epoch: 729, loss: 0.019\n",
      "model: 2, epoch: 730, loss: 0.019\n",
      "model: 2, epoch: 731, loss: 0.019\n",
      "model: 2, epoch: 732, loss: 0.019\n",
      "model: 2, epoch: 733, loss: 0.019\n",
      "model: 2, epoch: 734, loss: 0.019\n",
      "model: 2, epoch: 735, loss: 0.019\n",
      "model: 2, epoch: 736, loss: 0.019\n",
      "model: 2, epoch: 737, loss: 0.019\n",
      "model: 2, epoch: 738, loss: 0.019\n",
      "model: 2, epoch: 739, loss: 0.019\n",
      "model: 2, epoch: 740, loss: 0.019\n",
      "model: 2, epoch: 741, loss: 0.019\n",
      "model: 2, epoch: 742, loss: 0.019\n",
      "model: 2, epoch: 743, loss: 0.019\n",
      "model: 2, epoch: 744, loss: 0.019\n",
      "model: 2, epoch: 745, loss: 0.019\n",
      "model: 2, epoch: 746, loss: 0.019\n",
      "model: 2, epoch: 747, loss: 0.019\n",
      "model: 2, epoch: 748, loss: 0.019\n",
      "model: 2, epoch: 749, loss: 0.019\n",
      "model: 2, epoch: 750, loss: 0.019\n",
      "model: 2, epoch: 751, loss: 0.019\n",
      "model: 2, epoch: 752, loss: 0.019\n",
      "model: 2, epoch: 753, loss: 0.019\n",
      "model: 2, epoch: 754, loss: 0.019\n",
      "model: 2, epoch: 755, loss: 0.019\n",
      "model: 2, epoch: 756, loss: 0.018\n",
      "model: 2, epoch: 757, loss: 0.018\n",
      "model: 2, epoch: 758, loss: 0.018\n",
      "model: 2, epoch: 759, loss: 0.018\n",
      "model: 2, epoch: 760, loss: 0.018\n",
      "model: 2, epoch: 761, loss: 0.018\n",
      "model: 2, epoch: 762, loss: 0.018\n",
      "model: 2, epoch: 763, loss: 0.018\n",
      "model: 2, epoch: 764, loss: 0.018\n",
      "model: 2, epoch: 765, loss: 0.018\n",
      "model: 2, epoch: 766, loss: 0.018\n",
      "model: 2, epoch: 767, loss: 0.018\n",
      "model: 2, epoch: 768, loss: 0.018\n",
      "model: 2, epoch: 769, loss: 0.018\n",
      "model: 2, epoch: 770, loss: 0.018\n",
      "model: 2, epoch: 771, loss: 0.018\n",
      "model: 2, epoch: 772, loss: 0.018\n",
      "model: 2, epoch: 773, loss: 0.018\n",
      "model: 2, epoch: 774, loss: 0.018\n",
      "model: 2, epoch: 775, loss: 0.018\n",
      "model: 2, epoch: 776, loss: 0.018\n",
      "model: 2, epoch: 777, loss: 0.018\n",
      "model: 2, epoch: 778, loss: 0.018\n",
      "model: 2, epoch: 779, loss: 0.018\n",
      "model: 2, epoch: 780, loss: 0.018\n",
      "model: 2, epoch: 781, loss: 0.017\n",
      "model: 2, epoch: 782, loss: 0.017\n",
      "model: 2, epoch: 783, loss: 0.017\n",
      "model: 2, epoch: 784, loss: 0.017\n",
      "model: 2, epoch: 785, loss: 0.017\n",
      "model: 2, epoch: 786, loss: 0.017\n",
      "model: 2, epoch: 787, loss: 0.017\n",
      "model: 2, epoch: 788, loss: 0.017\n",
      "model: 2, epoch: 789, loss: 0.017\n",
      "model: 2, epoch: 790, loss: 0.017\n",
      "model: 2, epoch: 791, loss: 0.017\n",
      "model: 2, epoch: 792, loss: 0.017\n",
      "model: 2, epoch: 793, loss: 0.017\n",
      "model: 2, epoch: 794, loss: 0.017\n",
      "model: 2, epoch: 795, loss: 0.017\n",
      "model: 2, epoch: 796, loss: 0.017\n",
      "model: 2, epoch: 797, loss: 0.017\n",
      "model: 2, epoch: 798, loss: 0.017\n",
      "model: 2, epoch: 799, loss: 0.017\n",
      "model: 2, epoch: 800, loss: 0.017\n",
      "model: 2, epoch: 801, loss: 0.017\n",
      "model: 2, epoch: 802, loss: 0.017\n",
      "model: 2, epoch: 803, loss: 0.017\n",
      "model: 2, epoch: 804, loss: 0.017\n",
      "model: 2, epoch: 805, loss: 0.017\n",
      "model: 2, epoch: 806, loss: 0.016\n",
      "model: 2, epoch: 807, loss: 0.016\n",
      "model: 2, epoch: 808, loss: 0.016\n",
      "model: 2, epoch: 809, loss: 0.016\n",
      "model: 2, epoch: 810, loss: 0.016\n",
      "model: 2, epoch: 811, loss: 0.016\n",
      "model: 2, epoch: 812, loss: 0.016\n",
      "model: 2, epoch: 813, loss: 0.016\n",
      "model: 2, epoch: 814, loss: 0.016\n",
      "model: 2, epoch: 815, loss: 0.016\n",
      "model: 2, epoch: 816, loss: 0.016\n",
      "model: 2, epoch: 817, loss: 0.016\n",
      "model: 2, epoch: 818, loss: 0.016\n",
      "model: 2, epoch: 819, loss: 0.016\n",
      "model: 2, epoch: 820, loss: 0.016\n",
      "model: 2, epoch: 821, loss: 0.016\n",
      "model: 2, epoch: 822, loss: 0.016\n",
      "model: 2, epoch: 823, loss: 0.016\n",
      "model: 2, epoch: 824, loss: 0.016\n",
      "model: 2, epoch: 825, loss: 0.016\n",
      "model: 2, epoch: 826, loss: 0.016\n",
      "model: 2, epoch: 827, loss: 0.016\n",
      "model: 2, epoch: 828, loss: 0.016\n",
      "model: 2, epoch: 829, loss: 0.016\n",
      "model: 2, epoch: 830, loss: 0.016\n",
      "model: 2, epoch: 831, loss: 0.015\n",
      "model: 2, epoch: 832, loss: 0.015\n",
      "model: 2, epoch: 833, loss: 0.015\n",
      "model: 2, epoch: 834, loss: 0.015\n",
      "model: 2, epoch: 835, loss: 0.015\n",
      "model: 2, epoch: 836, loss: 0.015\n",
      "model: 2, epoch: 837, loss: 0.015\n",
      "model: 2, epoch: 838, loss: 0.015\n",
      "model: 2, epoch: 839, loss: 0.015\n",
      "model: 2, epoch: 840, loss: 0.015\n",
      "model: 2, epoch: 841, loss: 0.015\n",
      "model: 2, epoch: 842, loss: 0.015\n",
      "model: 2, epoch: 843, loss: 0.015\n",
      "model: 2, epoch: 844, loss: 0.015\n",
      "model: 2, epoch: 845, loss: 0.015\n",
      "model: 2, epoch: 846, loss: 0.015\n",
      "model: 2, epoch: 847, loss: 0.015\n",
      "model: 2, epoch: 848, loss: 0.015\n",
      "model: 2, epoch: 849, loss: 0.015\n",
      "model: 2, epoch: 850, loss: 0.015\n",
      "model: 2, epoch: 851, loss: 0.015\n",
      "model: 2, epoch: 852, loss: 0.015\n",
      "model: 2, epoch: 853, loss: 0.015\n",
      "model: 2, epoch: 854, loss: 0.015\n",
      "model: 2, epoch: 855, loss: 0.014\n",
      "model: 2, epoch: 856, loss: 0.014\n",
      "model: 2, epoch: 857, loss: 0.014\n",
      "model: 2, epoch: 858, loss: 0.014\n",
      "model: 2, epoch: 859, loss: 0.014\n",
      "model: 2, epoch: 860, loss: 0.014\n",
      "model: 2, epoch: 861, loss: 0.014\n",
      "model: 2, epoch: 862, loss: 0.014\n",
      "model: 2, epoch: 863, loss: 0.014\n",
      "model: 2, epoch: 864, loss: 0.014\n",
      "model: 2, epoch: 865, loss: 0.014\n",
      "model: 2, epoch: 866, loss: 0.014\n",
      "model: 2, epoch: 867, loss: 0.014\n",
      "model: 2, epoch: 868, loss: 0.014\n",
      "model: 2, epoch: 869, loss: 0.014\n",
      "model: 2, epoch: 870, loss: 0.014\n",
      "model: 2, epoch: 871, loss: 0.014\n",
      "model: 2, epoch: 872, loss: 0.014\n",
      "model: 2, epoch: 873, loss: 0.014\n",
      "model: 2, epoch: 874, loss: 0.014\n",
      "model: 2, epoch: 875, loss: 0.014\n",
      "model: 2, epoch: 876, loss: 0.014\n",
      "model: 2, epoch: 877, loss: 0.014\n",
      "model: 2, epoch: 878, loss: 0.014\n",
      "model: 2, epoch: 879, loss: 0.014\n",
      "model: 2, epoch: 880, loss: 0.014\n",
      "model: 2, epoch: 881, loss: 0.013\n",
      "model: 2, epoch: 882, loss: 0.013\n",
      "model: 2, epoch: 883, loss: 0.013\n",
      "model: 2, epoch: 884, loss: 0.013\n",
      "model: 2, epoch: 885, loss: 0.013\n",
      "model: 2, epoch: 886, loss: 0.013\n",
      "model: 2, epoch: 887, loss: 0.013\n",
      "model: 2, epoch: 888, loss: 0.013\n",
      "model: 2, epoch: 889, loss: 0.013\n",
      "model: 2, epoch: 890, loss: 0.013\n",
      "model: 2, epoch: 891, loss: 0.013\n",
      "model: 2, epoch: 892, loss: 0.013\n",
      "model: 2, epoch: 893, loss: 0.013\n",
      "model: 2, epoch: 894, loss: 0.013\n",
      "model: 2, epoch: 895, loss: 0.013\n",
      "model: 2, epoch: 896, loss: 0.013\n",
      "model: 2, epoch: 897, loss: 0.013\n",
      "model: 2, epoch: 898, loss: 0.013\n",
      "model: 2, epoch: 899, loss: 0.013\n",
      "model: 2, epoch: 900, loss: 0.013\n",
      "model: 2, epoch: 901, loss: 0.013\n",
      "model: 2, epoch: 902, loss: 0.013\n",
      "model: 2, epoch: 903, loss: 0.013\n",
      "model: 2, epoch: 904, loss: 0.013\n",
      "model: 2, epoch: 905, loss: 0.013\n",
      "model: 2, epoch: 906, loss: 0.013\n",
      "model: 2, epoch: 907, loss: 0.012\n",
      "model: 2, epoch: 908, loss: 0.012\n",
      "model: 2, epoch: 909, loss: 0.012\n",
      "model: 2, epoch: 910, loss: 0.012\n",
      "model: 2, epoch: 911, loss: 0.012\n",
      "model: 2, epoch: 912, loss: 0.012\n",
      "model: 2, epoch: 913, loss: 0.012\n",
      "model: 2, epoch: 914, loss: 0.012\n",
      "model: 2, epoch: 915, loss: 0.012\n",
      "model: 2, epoch: 916, loss: 0.012\n",
      "model: 2, epoch: 917, loss: 0.012\n",
      "model: 2, epoch: 918, loss: 0.012\n",
      "model: 2, epoch: 919, loss: 0.012\n",
      "model: 2, epoch: 920, loss: 0.012\n",
      "model: 2, epoch: 921, loss: 0.012\n",
      "model: 2, epoch: 922, loss: 0.012\n",
      "model: 2, epoch: 923, loss: 0.012\n",
      "model: 2, epoch: 924, loss: 0.012\n",
      "model: 2, epoch: 925, loss: 0.012\n",
      "model: 2, epoch: 926, loss: 0.012\n",
      "model: 2, epoch: 927, loss: 0.012\n",
      "model: 2, epoch: 928, loss: 0.012\n",
      "model: 2, epoch: 929, loss: 0.012\n",
      "model: 2, epoch: 930, loss: 0.012\n",
      "model: 2, epoch: 931, loss: 0.012\n",
      "model: 2, epoch: 932, loss: 0.012\n",
      "model: 2, epoch: 933, loss: 0.012\n",
      "model: 2, epoch: 934, loss: 0.012\n",
      "model: 2, epoch: 935, loss: 0.011\n",
      "model: 2, epoch: 936, loss: 0.011\n",
      "model: 2, epoch: 937, loss: 0.011\n",
      "model: 2, epoch: 938, loss: 0.011\n",
      "model: 2, epoch: 939, loss: 0.011\n",
      "model: 2, epoch: 940, loss: 0.011\n",
      "model: 2, epoch: 941, loss: 0.011\n",
      "model: 2, epoch: 942, loss: 0.011\n",
      "model: 2, epoch: 943, loss: 0.011\n",
      "model: 2, epoch: 944, loss: 0.011\n",
      "model: 2, epoch: 945, loss: 0.011\n",
      "model: 2, epoch: 946, loss: 0.011\n",
      "model: 2, epoch: 947, loss: 0.011\n",
      "model: 2, epoch: 948, loss: 0.011\n",
      "model: 2, epoch: 949, loss: 0.011\n",
      "model: 2, epoch: 950, loss: 0.011\n",
      "model: 2, epoch: 951, loss: 0.011\n",
      "model: 2, epoch: 952, loss: 0.011\n",
      "model: 2, epoch: 953, loss: 0.011\n",
      "model: 2, epoch: 954, loss: 0.011\n",
      "model: 2, epoch: 955, loss: 0.011\n",
      "model: 2, epoch: 956, loss: 0.011\n",
      "model: 2, epoch: 957, loss: 0.011\n",
      "model: 2, epoch: 958, loss: 0.011\n",
      "model: 2, epoch: 959, loss: 0.011\n",
      "model: 2, epoch: 960, loss: 0.011\n",
      "model: 2, epoch: 961, loss: 0.011\n",
      "model: 2, epoch: 962, loss: 0.011\n",
      "model: 2, epoch: 963, loss: 0.011\n",
      "model: 2, epoch: 964, loss: 0.011\n",
      "model: 2, epoch: 965, loss: 0.011\n",
      "model: 2, epoch: 966, loss: 0.010\n",
      "model: 2, epoch: 967, loss: 0.010\n",
      "model: 2, epoch: 968, loss: 0.010\n",
      "model: 2, epoch: 969, loss: 0.010\n",
      "model: 2, epoch: 970, loss: 0.010\n",
      "model: 2, epoch: 971, loss: 0.010\n",
      "model: 2, epoch: 972, loss: 0.010\n",
      "model: 2, epoch: 973, loss: 0.010\n",
      "model: 2, epoch: 974, loss: 0.010\n",
      "model: 2, epoch: 975, loss: 0.010\n",
      "model: 2, epoch: 976, loss: 0.010\n",
      "model: 2, epoch: 977, loss: 0.010\n",
      "model: 2, epoch: 978, loss: 0.010\n",
      "model: 2, epoch: 979, loss: 0.010\n",
      "model: 2, epoch: 980, loss: 0.010\n",
      "model: 2, epoch: 981, loss: 0.010\n",
      "model: 2, epoch: 982, loss: 0.010\n",
      "model: 2, epoch: 983, loss: 0.010\n",
      "model: 2, epoch: 984, loss: 0.010\n",
      "model: 2, epoch: 985, loss: 0.010\n",
      "model: 2, epoch: 986, loss: 0.010\n",
      "model: 2, epoch: 987, loss: 0.010\n",
      "model: 2, epoch: 988, loss: 0.010\n",
      "model: 2, epoch: 989, loss: 0.010\n",
      "model: 2, epoch: 990, loss: 0.010\n",
      "model: 2, epoch: 991, loss: 0.010\n",
      "model: 2, epoch: 992, loss: 0.010\n",
      "model: 2, epoch: 993, loss: 0.010\n",
      "model: 2, epoch: 994, loss: 0.010\n",
      "model: 2, epoch: 995, loss: 0.010\n",
      "model: 2, epoch: 996, loss: 0.010\n",
      "model: 2, epoch: 997, loss: 0.010\n",
      "model: 2, epoch: 998, loss: 0.010\n",
      "model: 2, epoch: 999, loss: 0.010\n",
      "model: 3, epoch: 0, loss: 0.169\n",
      "model: 3, epoch: 1, loss: 0.160\n",
      "model: 3, epoch: 2, loss: 0.156\n",
      "model: 3, epoch: 3, loss: 0.153\n",
      "model: 3, epoch: 4, loss: 0.150\n",
      "model: 3, epoch: 5, loss: 0.147\n",
      "model: 3, epoch: 6, loss: 0.143\n",
      "model: 3, epoch: 7, loss: 0.139\n",
      "model: 3, epoch: 8, loss: 0.135\n",
      "model: 3, epoch: 9, loss: 0.131\n",
      "model: 3, epoch: 10, loss: 0.128\n",
      "model: 3, epoch: 11, loss: 0.124\n",
      "model: 3, epoch: 12, loss: 0.121\n",
      "model: 3, epoch: 13, loss: 0.118\n",
      "model: 3, epoch: 14, loss: 0.115\n",
      "model: 3, epoch: 15, loss: 0.112\n",
      "model: 3, epoch: 16, loss: 0.109\n",
      "model: 3, epoch: 17, loss: 0.105\n",
      "model: 3, epoch: 18, loss: 0.102\n",
      "model: 3, epoch: 19, loss: 0.099\n",
      "model: 3, epoch: 20, loss: 0.096\n",
      "model: 3, epoch: 21, loss: 0.094\n",
      "model: 3, epoch: 22, loss: 0.091\n",
      "model: 3, epoch: 23, loss: 0.088\n",
      "model: 3, epoch: 24, loss: 0.086\n",
      "model: 3, epoch: 25, loss: 0.083\n",
      "model: 3, epoch: 26, loss: 0.081\n",
      "model: 3, epoch: 27, loss: 0.078\n",
      "model: 3, epoch: 28, loss: 0.076\n",
      "model: 3, epoch: 29, loss: 0.074\n",
      "model: 3, epoch: 30, loss: 0.072\n",
      "model: 3, epoch: 31, loss: 0.070\n",
      "model: 3, epoch: 32, loss: 0.068\n",
      "model: 3, epoch: 33, loss: 0.066\n",
      "model: 3, epoch: 34, loss: 0.064\n",
      "model: 3, epoch: 35, loss: 0.062\n",
      "model: 3, epoch: 36, loss: 0.060\n",
      "model: 3, epoch: 37, loss: 0.058\n",
      "model: 3, epoch: 38, loss: 0.057\n",
      "model: 3, epoch: 39, loss: 0.055\n",
      "model: 3, epoch: 40, loss: 0.054\n",
      "model: 3, epoch: 41, loss: 0.052\n",
      "model: 3, epoch: 42, loss: 0.051\n",
      "model: 3, epoch: 43, loss: 0.050\n",
      "model: 3, epoch: 44, loss: 0.048\n",
      "model: 3, epoch: 45, loss: 0.047\n",
      "model: 3, epoch: 46, loss: 0.046\n",
      "model: 3, epoch: 47, loss: 0.045\n",
      "model: 3, epoch: 48, loss: 0.044\n",
      "model: 3, epoch: 49, loss: 0.043\n",
      "model: 3, epoch: 50, loss: 0.042\n",
      "model: 3, epoch: 51, loss: 0.041\n",
      "model: 3, epoch: 52, loss: 0.040\n",
      "model: 3, epoch: 53, loss: 0.039\n",
      "model: 3, epoch: 54, loss: 0.039\n",
      "model: 3, epoch: 55, loss: 0.038\n",
      "model: 3, epoch: 56, loss: 0.037\n",
      "model: 3, epoch: 57, loss: 0.037\n",
      "model: 3, epoch: 58, loss: 0.036\n",
      "model: 3, epoch: 59, loss: 0.035\n",
      "model: 3, epoch: 60, loss: 0.035\n",
      "model: 3, epoch: 61, loss: 0.034\n",
      "model: 3, epoch: 62, loss: 0.034\n",
      "model: 3, epoch: 63, loss: 0.033\n",
      "model: 3, epoch: 64, loss: 0.033\n",
      "model: 3, epoch: 65, loss: 0.033\n",
      "model: 3, epoch: 66, loss: 0.032\n",
      "model: 3, epoch: 67, loss: 0.032\n",
      "model: 3, epoch: 68, loss: 0.032\n",
      "model: 3, epoch: 69, loss: 0.031\n",
      "model: 3, epoch: 70, loss: 0.031\n",
      "model: 3, epoch: 71, loss: 0.031\n",
      "model: 3, epoch: 72, loss: 0.031\n",
      "model: 3, epoch: 73, loss: 0.030\n",
      "model: 3, epoch: 74, loss: 0.030\n",
      "model: 3, epoch: 75, loss: 0.030\n",
      "model: 3, epoch: 76, loss: 0.030\n",
      "model: 3, epoch: 77, loss: 0.030\n",
      "model: 3, epoch: 78, loss: 0.030\n",
      "model: 3, epoch: 79, loss: 0.029\n",
      "model: 3, epoch: 80, loss: 0.029\n",
      "model: 3, epoch: 81, loss: 0.029\n",
      "model: 3, epoch: 82, loss: 0.029\n",
      "model: 3, epoch: 83, loss: 0.029\n",
      "model: 3, epoch: 84, loss: 0.029\n",
      "model: 3, epoch: 85, loss: 0.029\n",
      "model: 3, epoch: 86, loss: 0.029\n",
      "model: 3, epoch: 87, loss: 0.029\n",
      "model: 3, epoch: 88, loss: 0.029\n",
      "model: 3, epoch: 89, loss: 0.029\n",
      "model: 3, epoch: 90, loss: 0.029\n",
      "model: 3, epoch: 91, loss: 0.029\n",
      "model: 3, epoch: 92, loss: 0.029\n",
      "model: 3, epoch: 93, loss: 0.029\n",
      "model: 3, epoch: 94, loss: 0.029\n",
      "model: 3, epoch: 95, loss: 0.028\n",
      "model: 3, epoch: 96, loss: 0.028\n",
      "model: 3, epoch: 97, loss: 0.028\n",
      "model: 3, epoch: 98, loss: 0.028\n",
      "model: 3, epoch: 99, loss: 0.028\n",
      "model: 3, epoch: 100, loss: 0.028\n",
      "model: 3, epoch: 101, loss: 0.028\n",
      "model: 3, epoch: 102, loss: 0.028\n",
      "model: 3, epoch: 103, loss: 0.028\n",
      "model: 3, epoch: 104, loss: 0.028\n",
      "model: 3, epoch: 105, loss: 0.028\n",
      "model: 3, epoch: 106, loss: 0.028\n",
      "model: 3, epoch: 107, loss: 0.028\n",
      "model: 3, epoch: 108, loss: 0.028\n",
      "model: 3, epoch: 109, loss: 0.028\n",
      "model: 3, epoch: 110, loss: 0.028\n",
      "model: 3, epoch: 111, loss: 0.028\n",
      "model: 3, epoch: 112, loss: 0.028\n",
      "model: 3, epoch: 113, loss: 0.028\n",
      "model: 3, epoch: 114, loss: 0.028\n",
      "model: 3, epoch: 115, loss: 0.028\n",
      "model: 3, epoch: 116, loss: 0.028\n",
      "model: 3, epoch: 117, loss: 0.028\n",
      "model: 3, epoch: 118, loss: 0.028\n",
      "model: 3, epoch: 119, loss: 0.028\n",
      "model: 3, epoch: 120, loss: 0.028\n",
      "model: 3, epoch: 121, loss: 0.028\n",
      "model: 3, epoch: 122, loss: 0.028\n",
      "model: 3, epoch: 123, loss: 0.028\n",
      "model: 3, epoch: 124, loss: 0.028\n",
      "model: 3, epoch: 125, loss: 0.028\n",
      "model: 3, epoch: 126, loss: 0.028\n",
      "model: 3, epoch: 127, loss: 0.028\n",
      "model: 3, epoch: 128, loss: 0.028\n",
      "model: 3, epoch: 129, loss: 0.028\n",
      "model: 3, epoch: 130, loss: 0.028\n",
      "model: 3, epoch: 131, loss: 0.028\n",
      "model: 3, epoch: 132, loss: 0.028\n",
      "model: 3, epoch: 133, loss: 0.028\n",
      "model: 3, epoch: 134, loss: 0.028\n",
      "model: 3, epoch: 135, loss: 0.028\n",
      "model: 3, epoch: 136, loss: 0.028\n",
      "model: 3, epoch: 137, loss: 0.028\n",
      "model: 3, epoch: 138, loss: 0.028\n",
      "model: 3, epoch: 139, loss: 0.028\n",
      "model: 3, epoch: 140, loss: 0.028\n",
      "model: 3, epoch: 141, loss: 0.028\n",
      "model: 3, epoch: 142, loss: 0.028\n",
      "model: 3, epoch: 143, loss: 0.028\n",
      "model: 3, epoch: 144, loss: 0.028\n",
      "model: 3, epoch: 145, loss: 0.028\n",
      "model: 3, epoch: 146, loss: 0.028\n",
      "model: 3, epoch: 147, loss: 0.028\n",
      "model: 3, epoch: 148, loss: 0.028\n",
      "model: 3, epoch: 149, loss: 0.028\n",
      "model: 3, epoch: 150, loss: 0.028\n",
      "model: 3, epoch: 151, loss: 0.028\n",
      "model: 3, epoch: 152, loss: 0.028\n",
      "model: 3, epoch: 153, loss: 0.028\n",
      "model: 3, epoch: 154, loss: 0.028\n",
      "model: 3, epoch: 155, loss: 0.028\n",
      "model: 3, epoch: 156, loss: 0.028\n",
      "model: 3, epoch: 157, loss: 0.028\n",
      "model: 3, epoch: 158, loss: 0.028\n",
      "model: 3, epoch: 159, loss: 0.028\n",
      "model: 3, epoch: 160, loss: 0.028\n",
      "model: 3, epoch: 161, loss: 0.028\n",
      "model: 3, epoch: 162, loss: 0.028\n",
      "model: 3, epoch: 163, loss: 0.028\n",
      "model: 3, epoch: 164, loss: 0.028\n",
      "model: 3, epoch: 165, loss: 0.028\n",
      "model: 3, epoch: 166, loss: 0.028\n",
      "model: 3, epoch: 167, loss: 0.028\n",
      "model: 3, epoch: 168, loss: 0.028\n",
      "model: 3, epoch: 169, loss: 0.028\n",
      "model: 3, epoch: 170, loss: 0.028\n",
      "model: 3, epoch: 171, loss: 0.028\n",
      "model: 3, epoch: 172, loss: 0.028\n",
      "model: 3, epoch: 173, loss: 0.028\n",
      "model: 3, epoch: 174, loss: 0.028\n",
      "model: 3, epoch: 175, loss: 0.028\n",
      "model: 3, epoch: 176, loss: 0.028\n",
      "model: 3, epoch: 177, loss: 0.028\n",
      "model: 3, epoch: 178, loss: 0.028\n",
      "model: 3, epoch: 179, loss: 0.028\n",
      "model: 3, epoch: 180, loss: 0.028\n",
      "model: 3, epoch: 181, loss: 0.028\n",
      "model: 3, epoch: 182, loss: 0.028\n",
      "model: 3, epoch: 183, loss: 0.028\n",
      "model: 3, epoch: 184, loss: 0.028\n",
      "model: 3, epoch: 185, loss: 0.028\n",
      "model: 3, epoch: 186, loss: 0.028\n",
      "model: 3, epoch: 187, loss: 0.028\n",
      "model: 3, epoch: 188, loss: 0.028\n",
      "model: 3, epoch: 189, loss: 0.028\n",
      "model: 3, epoch: 190, loss: 0.028\n",
      "model: 3, epoch: 191, loss: 0.028\n",
      "model: 3, epoch: 192, loss: 0.028\n",
      "model: 3, epoch: 193, loss: 0.028\n",
      "model: 3, epoch: 194, loss: 0.028\n",
      "model: 3, epoch: 195, loss: 0.028\n",
      "model: 3, epoch: 196, loss: 0.028\n",
      "model: 3, epoch: 197, loss: 0.028\n",
      "model: 3, epoch: 198, loss: 0.028\n",
      "model: 3, epoch: 199, loss: 0.028\n",
      "model: 3, epoch: 200, loss: 0.028\n",
      "model: 3, epoch: 201, loss: 0.028\n",
      "model: 3, epoch: 202, loss: 0.028\n",
      "model: 3, epoch: 203, loss: 0.028\n",
      "model: 3, epoch: 204, loss: 0.028\n",
      "model: 3, epoch: 205, loss: 0.028\n",
      "model: 3, epoch: 206, loss: 0.028\n",
      "model: 3, epoch: 207, loss: 0.028\n",
      "model: 3, epoch: 208, loss: 0.028\n",
      "model: 3, epoch: 209, loss: 0.028\n",
      "model: 3, epoch: 210, loss: 0.028\n",
      "model: 3, epoch: 211, loss: 0.028\n",
      "model: 3, epoch: 212, loss: 0.028\n",
      "model: 3, epoch: 213, loss: 0.028\n",
      "model: 3, epoch: 214, loss: 0.028\n",
      "model: 3, epoch: 215, loss: 0.028\n",
      "model: 3, epoch: 216, loss: 0.028\n",
      "model: 3, epoch: 217, loss: 0.028\n",
      "model: 3, epoch: 218, loss: 0.028\n",
      "model: 3, epoch: 219, loss: 0.028\n",
      "model: 3, epoch: 220, loss: 0.028\n",
      "model: 3, epoch: 221, loss: 0.028\n",
      "model: 3, epoch: 222, loss: 0.028\n",
      "model: 3, epoch: 223, loss: 0.028\n",
      "model: 3, epoch: 224, loss: 0.028\n",
      "model: 3, epoch: 225, loss: 0.028\n",
      "model: 3, epoch: 226, loss: 0.028\n",
      "model: 3, epoch: 227, loss: 0.028\n",
      "model: 3, epoch: 228, loss: 0.028\n",
      "model: 3, epoch: 229, loss: 0.028\n",
      "model: 3, epoch: 230, loss: 0.028\n",
      "model: 3, epoch: 231, loss: 0.028\n",
      "model: 3, epoch: 232, loss: 0.028\n",
      "model: 3, epoch: 233, loss: 0.028\n",
      "model: 3, epoch: 234, loss: 0.028\n",
      "model: 3, epoch: 235, loss: 0.028\n",
      "model: 3, epoch: 236, loss: 0.028\n",
      "model: 3, epoch: 237, loss: 0.028\n",
      "model: 3, epoch: 238, loss: 0.028\n",
      "model: 3, epoch: 239, loss: 0.028\n",
      "model: 3, epoch: 240, loss: 0.028\n",
      "model: 3, epoch: 241, loss: 0.028\n",
      "model: 3, epoch: 242, loss: 0.028\n",
      "model: 3, epoch: 243, loss: 0.028\n",
      "model: 3, epoch: 244, loss: 0.028\n",
      "model: 3, epoch: 245, loss: 0.028\n",
      "model: 3, epoch: 246, loss: 0.028\n",
      "model: 3, epoch: 247, loss: 0.028\n",
      "model: 3, epoch: 248, loss: 0.028\n",
      "model: 3, epoch: 249, loss: 0.028\n",
      "model: 3, epoch: 250, loss: 0.028\n",
      "model: 3, epoch: 251, loss: 0.028\n",
      "model: 3, epoch: 252, loss: 0.028\n",
      "model: 3, epoch: 253, loss: 0.028\n",
      "model: 3, epoch: 254, loss: 0.028\n",
      "model: 3, epoch: 255, loss: 0.028\n",
      "model: 3, epoch: 256, loss: 0.028\n",
      "model: 3, epoch: 257, loss: 0.028\n",
      "model: 3, epoch: 258, loss: 0.028\n",
      "model: 3, epoch: 259, loss: 0.028\n",
      "model: 3, epoch: 260, loss: 0.028\n",
      "model: 3, epoch: 261, loss: 0.028\n",
      "model: 3, epoch: 262, loss: 0.028\n",
      "model: 3, epoch: 263, loss: 0.028\n",
      "model: 3, epoch: 264, loss: 0.028\n",
      "model: 3, epoch: 265, loss: 0.028\n",
      "model: 3, epoch: 266, loss: 0.028\n",
      "model: 3, epoch: 267, loss: 0.028\n",
      "model: 3, epoch: 268, loss: 0.028\n",
      "model: 3, epoch: 269, loss: 0.028\n",
      "model: 3, epoch: 270, loss: 0.028\n",
      "model: 3, epoch: 271, loss: 0.028\n",
      "model: 3, epoch: 272, loss: 0.028\n",
      "model: 3, epoch: 273, loss: 0.028\n",
      "model: 3, epoch: 274, loss: 0.028\n",
      "model: 3, epoch: 275, loss: 0.028\n",
      "model: 3, epoch: 276, loss: 0.028\n",
      "model: 3, epoch: 277, loss: 0.028\n",
      "model: 3, epoch: 278, loss: 0.028\n",
      "model: 3, epoch: 279, loss: 0.028\n",
      "model: 3, epoch: 280, loss: 0.028\n",
      "model: 3, epoch: 281, loss: 0.028\n",
      "model: 3, epoch: 282, loss: 0.028\n",
      "model: 3, epoch: 283, loss: 0.028\n",
      "model: 3, epoch: 284, loss: 0.028\n",
      "model: 3, epoch: 285, loss: 0.028\n",
      "model: 3, epoch: 286, loss: 0.028\n",
      "model: 3, epoch: 287, loss: 0.028\n",
      "model: 3, epoch: 288, loss: 0.028\n",
      "model: 3, epoch: 289, loss: 0.028\n",
      "model: 3, epoch: 290, loss: 0.028\n",
      "model: 3, epoch: 291, loss: 0.028\n",
      "model: 3, epoch: 292, loss: 0.028\n",
      "model: 3, epoch: 293, loss: 0.028\n",
      "model: 3, epoch: 294, loss: 0.028\n",
      "model: 3, epoch: 295, loss: 0.028\n",
      "model: 3, epoch: 296, loss: 0.028\n",
      "model: 3, epoch: 297, loss: 0.028\n",
      "model: 3, epoch: 298, loss: 0.028\n",
      "model: 3, epoch: 299, loss: 0.028\n",
      "model: 3, epoch: 300, loss: 0.028\n",
      "model: 3, epoch: 301, loss: 0.028\n",
      "model: 3, epoch: 302, loss: 0.028\n",
      "model: 3, epoch: 303, loss: 0.028\n",
      "model: 3, epoch: 304, loss: 0.028\n",
      "model: 3, epoch: 305, loss: 0.028\n",
      "model: 3, epoch: 306, loss: 0.028\n",
      "model: 3, epoch: 307, loss: 0.028\n",
      "model: 3, epoch: 308, loss: 0.028\n",
      "model: 3, epoch: 309, loss: 0.028\n",
      "model: 3, epoch: 310, loss: 0.028\n",
      "model: 3, epoch: 311, loss: 0.028\n",
      "model: 3, epoch: 312, loss: 0.028\n",
      "model: 3, epoch: 313, loss: 0.028\n",
      "model: 3, epoch: 314, loss: 0.028\n",
      "model: 3, epoch: 315, loss: 0.028\n",
      "model: 3, epoch: 316, loss: 0.028\n",
      "model: 3, epoch: 317, loss: 0.028\n",
      "model: 3, epoch: 318, loss: 0.028\n",
      "model: 3, epoch: 319, loss: 0.028\n",
      "model: 3, epoch: 320, loss: 0.028\n",
      "model: 3, epoch: 321, loss: 0.028\n",
      "model: 3, epoch: 322, loss: 0.028\n",
      "model: 3, epoch: 323, loss: 0.028\n",
      "model: 3, epoch: 324, loss: 0.028\n",
      "model: 3, epoch: 325, loss: 0.028\n",
      "model: 3, epoch: 326, loss: 0.028\n",
      "model: 3, epoch: 327, loss: 0.028\n",
      "model: 3, epoch: 328, loss: 0.028\n",
      "model: 3, epoch: 329, loss: 0.028\n",
      "model: 3, epoch: 330, loss: 0.028\n",
      "model: 3, epoch: 331, loss: 0.028\n",
      "model: 3, epoch: 332, loss: 0.028\n",
      "model: 3, epoch: 333, loss: 0.028\n",
      "model: 3, epoch: 334, loss: 0.028\n",
      "model: 3, epoch: 335, loss: 0.028\n",
      "model: 3, epoch: 336, loss: 0.028\n",
      "model: 3, epoch: 337, loss: 0.028\n",
      "model: 3, epoch: 338, loss: 0.028\n",
      "model: 3, epoch: 339, loss: 0.028\n",
      "model: 3, epoch: 340, loss: 0.028\n",
      "model: 3, epoch: 341, loss: 0.028\n",
      "model: 3, epoch: 342, loss: 0.028\n",
      "model: 3, epoch: 343, loss: 0.028\n",
      "model: 3, epoch: 344, loss: 0.028\n",
      "model: 3, epoch: 345, loss: 0.028\n",
      "model: 3, epoch: 346, loss: 0.028\n",
      "model: 3, epoch: 347, loss: 0.028\n",
      "model: 3, epoch: 348, loss: 0.028\n",
      "model: 3, epoch: 349, loss: 0.028\n",
      "model: 3, epoch: 350, loss: 0.028\n",
      "model: 3, epoch: 351, loss: 0.028\n",
      "model: 3, epoch: 352, loss: 0.028\n",
      "model: 3, epoch: 353, loss: 0.028\n",
      "model: 3, epoch: 354, loss: 0.028\n",
      "model: 3, epoch: 355, loss: 0.028\n",
      "model: 3, epoch: 356, loss: 0.028\n",
      "model: 3, epoch: 357, loss: 0.028\n",
      "model: 3, epoch: 358, loss: 0.028\n",
      "model: 3, epoch: 359, loss: 0.028\n",
      "model: 3, epoch: 360, loss: 0.028\n",
      "model: 3, epoch: 361, loss: 0.028\n",
      "model: 3, epoch: 362, loss: 0.028\n",
      "model: 3, epoch: 363, loss: 0.028\n",
      "model: 3, epoch: 364, loss: 0.028\n",
      "model: 3, epoch: 365, loss: 0.028\n",
      "model: 3, epoch: 366, loss: 0.028\n",
      "model: 3, epoch: 367, loss: 0.028\n",
      "model: 3, epoch: 368, loss: 0.028\n",
      "model: 3, epoch: 369, loss: 0.028\n",
      "model: 3, epoch: 370, loss: 0.028\n",
      "model: 3, epoch: 371, loss: 0.028\n",
      "model: 3, epoch: 372, loss: 0.028\n",
      "model: 3, epoch: 373, loss: 0.028\n",
      "model: 3, epoch: 374, loss: 0.028\n",
      "model: 3, epoch: 375, loss: 0.028\n",
      "model: 3, epoch: 376, loss: 0.028\n",
      "model: 3, epoch: 377, loss: 0.028\n",
      "model: 3, epoch: 378, loss: 0.028\n",
      "model: 3, epoch: 379, loss: 0.028\n",
      "model: 3, epoch: 380, loss: 0.028\n",
      "model: 3, epoch: 381, loss: 0.028\n",
      "model: 3, epoch: 382, loss: 0.028\n",
      "model: 3, epoch: 383, loss: 0.028\n",
      "model: 3, epoch: 384, loss: 0.028\n",
      "model: 3, epoch: 385, loss: 0.028\n",
      "model: 3, epoch: 386, loss: 0.028\n",
      "model: 3, epoch: 387, loss: 0.028\n",
      "model: 3, epoch: 388, loss: 0.028\n",
      "model: 3, epoch: 389, loss: 0.028\n",
      "model: 3, epoch: 390, loss: 0.028\n",
      "model: 3, epoch: 391, loss: 0.028\n",
      "model: 3, epoch: 392, loss: 0.028\n",
      "model: 3, epoch: 393, loss: 0.028\n",
      "model: 3, epoch: 394, loss: 0.028\n",
      "model: 3, epoch: 395, loss: 0.028\n",
      "model: 3, epoch: 396, loss: 0.028\n",
      "model: 3, epoch: 397, loss: 0.028\n",
      "model: 3, epoch: 398, loss: 0.028\n",
      "model: 3, epoch: 399, loss: 0.028\n",
      "model: 3, epoch: 400, loss: 0.028\n",
      "model: 3, epoch: 401, loss: 0.028\n",
      "model: 3, epoch: 402, loss: 0.028\n",
      "model: 3, epoch: 403, loss: 0.028\n",
      "model: 3, epoch: 404, loss: 0.028\n",
      "model: 3, epoch: 405, loss: 0.028\n",
      "model: 3, epoch: 406, loss: 0.028\n",
      "model: 3, epoch: 407, loss: 0.028\n",
      "model: 3, epoch: 408, loss: 0.028\n",
      "model: 3, epoch: 409, loss: 0.028\n",
      "model: 3, epoch: 410, loss: 0.028\n",
      "model: 3, epoch: 411, loss: 0.028\n",
      "model: 3, epoch: 412, loss: 0.028\n",
      "model: 3, epoch: 413, loss: 0.028\n",
      "model: 3, epoch: 414, loss: 0.028\n",
      "model: 3, epoch: 415, loss: 0.028\n",
      "model: 3, epoch: 416, loss: 0.028\n",
      "model: 3, epoch: 417, loss: 0.028\n",
      "model: 3, epoch: 418, loss: 0.028\n",
      "model: 3, epoch: 419, loss: 0.028\n",
      "model: 3, epoch: 420, loss: 0.028\n",
      "model: 3, epoch: 421, loss: 0.028\n",
      "model: 3, epoch: 422, loss: 0.028\n",
      "model: 3, epoch: 423, loss: 0.028\n",
      "model: 3, epoch: 424, loss: 0.028\n",
      "model: 3, epoch: 425, loss: 0.028\n",
      "model: 3, epoch: 426, loss: 0.028\n",
      "model: 3, epoch: 427, loss: 0.028\n",
      "model: 3, epoch: 428, loss: 0.028\n",
      "model: 3, epoch: 429, loss: 0.028\n",
      "model: 3, epoch: 430, loss: 0.028\n",
      "model: 3, epoch: 431, loss: 0.028\n",
      "model: 3, epoch: 432, loss: 0.028\n",
      "model: 3, epoch: 433, loss: 0.028\n",
      "model: 3, epoch: 434, loss: 0.028\n",
      "model: 3, epoch: 435, loss: 0.028\n",
      "model: 3, epoch: 436, loss: 0.028\n",
      "model: 3, epoch: 437, loss: 0.028\n",
      "model: 3, epoch: 438, loss: 0.028\n",
      "model: 3, epoch: 439, loss: 0.028\n",
      "model: 3, epoch: 440, loss: 0.028\n",
      "model: 3, epoch: 441, loss: 0.028\n",
      "model: 3, epoch: 442, loss: 0.028\n",
      "model: 3, epoch: 443, loss: 0.028\n",
      "model: 3, epoch: 444, loss: 0.028\n",
      "model: 3, epoch: 445, loss: 0.028\n",
      "model: 3, epoch: 446, loss: 0.028\n",
      "model: 3, epoch: 447, loss: 0.028\n",
      "model: 3, epoch: 448, loss: 0.028\n",
      "model: 3, epoch: 449, loss: 0.028\n",
      "model: 3, epoch: 450, loss: 0.028\n",
      "model: 3, epoch: 451, loss: 0.028\n",
      "model: 3, epoch: 452, loss: 0.028\n",
      "model: 3, epoch: 453, loss: 0.028\n",
      "model: 3, epoch: 454, loss: 0.028\n",
      "model: 3, epoch: 455, loss: 0.028\n",
      "model: 3, epoch: 456, loss: 0.028\n",
      "model: 3, epoch: 457, loss: 0.028\n",
      "model: 3, epoch: 458, loss: 0.028\n",
      "model: 3, epoch: 459, loss: 0.028\n",
      "model: 3, epoch: 460, loss: 0.028\n",
      "model: 3, epoch: 461, loss: 0.028\n",
      "model: 3, epoch: 462, loss: 0.028\n",
      "model: 3, epoch: 463, loss: 0.028\n",
      "model: 3, epoch: 464, loss: 0.028\n",
      "model: 3, epoch: 465, loss: 0.028\n",
      "model: 3, epoch: 466, loss: 0.028\n",
      "model: 3, epoch: 467, loss: 0.028\n",
      "model: 3, epoch: 468, loss: 0.028\n",
      "model: 3, epoch: 469, loss: 0.028\n",
      "model: 3, epoch: 470, loss: 0.028\n",
      "model: 3, epoch: 471, loss: 0.028\n",
      "model: 3, epoch: 472, loss: 0.028\n",
      "model: 3, epoch: 473, loss: 0.028\n",
      "model: 3, epoch: 474, loss: 0.028\n",
      "model: 3, epoch: 475, loss: 0.028\n",
      "model: 3, epoch: 476, loss: 0.028\n",
      "model: 3, epoch: 477, loss: 0.028\n",
      "model: 3, epoch: 478, loss: 0.028\n",
      "model: 3, epoch: 479, loss: 0.028\n",
      "model: 3, epoch: 480, loss: 0.028\n",
      "model: 3, epoch: 481, loss: 0.028\n",
      "model: 3, epoch: 482, loss: 0.028\n",
      "model: 3, epoch: 483, loss: 0.028\n",
      "model: 3, epoch: 484, loss: 0.028\n",
      "model: 3, epoch: 485, loss: 0.028\n",
      "model: 3, epoch: 486, loss: 0.028\n",
      "model: 3, epoch: 487, loss: 0.028\n",
      "model: 3, epoch: 488, loss: 0.028\n",
      "model: 3, epoch: 489, loss: 0.028\n",
      "model: 3, epoch: 490, loss: 0.028\n",
      "model: 3, epoch: 491, loss: 0.028\n",
      "model: 3, epoch: 492, loss: 0.028\n",
      "model: 3, epoch: 493, loss: 0.028\n",
      "model: 3, epoch: 494, loss: 0.028\n",
      "model: 3, epoch: 495, loss: 0.028\n",
      "model: 3, epoch: 496, loss: 0.028\n",
      "model: 3, epoch: 497, loss: 0.028\n",
      "model: 3, epoch: 498, loss: 0.028\n",
      "model: 3, epoch: 499, loss: 0.028\n",
      "model: 3, epoch: 500, loss: 0.028\n",
      "model: 3, epoch: 501, loss: 0.028\n",
      "model: 3, epoch: 502, loss: 0.028\n",
      "model: 3, epoch: 503, loss: 0.028\n",
      "model: 3, epoch: 504, loss: 0.028\n",
      "model: 3, epoch: 505, loss: 0.028\n",
      "model: 3, epoch: 506, loss: 0.028\n",
      "model: 3, epoch: 507, loss: 0.028\n",
      "model: 3, epoch: 508, loss: 0.028\n",
      "model: 3, epoch: 509, loss: 0.028\n",
      "model: 3, epoch: 510, loss: 0.028\n",
      "model: 3, epoch: 511, loss: 0.028\n",
      "model: 3, epoch: 512, loss: 0.028\n",
      "model: 3, epoch: 513, loss: 0.028\n",
      "model: 3, epoch: 514, loss: 0.028\n",
      "model: 3, epoch: 515, loss: 0.028\n",
      "model: 3, epoch: 516, loss: 0.028\n",
      "model: 3, epoch: 517, loss: 0.028\n",
      "model: 3, epoch: 518, loss: 0.028\n",
      "model: 3, epoch: 519, loss: 0.028\n",
      "model: 3, epoch: 520, loss: 0.028\n",
      "model: 3, epoch: 521, loss: 0.028\n",
      "model: 3, epoch: 522, loss: 0.028\n",
      "model: 3, epoch: 523, loss: 0.028\n",
      "model: 3, epoch: 524, loss: 0.028\n",
      "model: 3, epoch: 525, loss: 0.028\n",
      "model: 3, epoch: 526, loss: 0.028\n",
      "model: 3, epoch: 527, loss: 0.028\n",
      "model: 3, epoch: 528, loss: 0.028\n",
      "model: 3, epoch: 529, loss: 0.028\n",
      "model: 3, epoch: 530, loss: 0.028\n",
      "model: 3, epoch: 531, loss: 0.028\n",
      "model: 3, epoch: 532, loss: 0.028\n",
      "model: 3, epoch: 533, loss: 0.028\n",
      "model: 3, epoch: 534, loss: 0.028\n",
      "model: 3, epoch: 535, loss: 0.028\n",
      "model: 3, epoch: 536, loss: 0.028\n",
      "model: 3, epoch: 537, loss: 0.028\n",
      "model: 3, epoch: 538, loss: 0.028\n",
      "model: 3, epoch: 539, loss: 0.028\n",
      "model: 3, epoch: 540, loss: 0.028\n",
      "model: 3, epoch: 541, loss: 0.028\n",
      "model: 3, epoch: 542, loss: 0.028\n",
      "model: 3, epoch: 543, loss: 0.028\n",
      "model: 3, epoch: 544, loss: 0.028\n",
      "model: 3, epoch: 545, loss: 0.028\n",
      "model: 3, epoch: 546, loss: 0.028\n",
      "model: 3, epoch: 547, loss: 0.028\n",
      "model: 3, epoch: 548, loss: 0.028\n",
      "model: 3, epoch: 549, loss: 0.028\n",
      "model: 3, epoch: 550, loss: 0.028\n",
      "model: 3, epoch: 551, loss: 0.028\n",
      "model: 3, epoch: 552, loss: 0.028\n",
      "model: 3, epoch: 553, loss: 0.028\n",
      "model: 3, epoch: 554, loss: 0.028\n",
      "model: 3, epoch: 555, loss: 0.028\n",
      "model: 3, epoch: 556, loss: 0.028\n",
      "model: 3, epoch: 557, loss: 0.028\n",
      "model: 3, epoch: 558, loss: 0.028\n",
      "model: 3, epoch: 559, loss: 0.028\n",
      "model: 3, epoch: 560, loss: 0.028\n",
      "model: 3, epoch: 561, loss: 0.028\n",
      "model: 3, epoch: 562, loss: 0.028\n",
      "model: 3, epoch: 563, loss: 0.028\n",
      "model: 3, epoch: 564, loss: 0.028\n",
      "model: 3, epoch: 565, loss: 0.028\n",
      "model: 3, epoch: 566, loss: 0.028\n",
      "model: 3, epoch: 567, loss: 0.028\n",
      "model: 3, epoch: 568, loss: 0.028\n",
      "model: 3, epoch: 569, loss: 0.028\n",
      "model: 3, epoch: 570, loss: 0.028\n",
      "model: 3, epoch: 571, loss: 0.028\n",
      "model: 3, epoch: 572, loss: 0.028\n",
      "model: 3, epoch: 573, loss: 0.028\n",
      "model: 3, epoch: 574, loss: 0.028\n",
      "model: 3, epoch: 575, loss: 0.028\n",
      "model: 3, epoch: 576, loss: 0.028\n",
      "model: 3, epoch: 577, loss: 0.028\n",
      "model: 3, epoch: 578, loss: 0.028\n",
      "model: 3, epoch: 579, loss: 0.028\n",
      "model: 3, epoch: 580, loss: 0.028\n",
      "model: 3, epoch: 581, loss: 0.028\n",
      "model: 3, epoch: 582, loss: 0.028\n",
      "model: 3, epoch: 583, loss: 0.028\n",
      "model: 3, epoch: 584, loss: 0.028\n",
      "model: 3, epoch: 585, loss: 0.028\n",
      "model: 3, epoch: 586, loss: 0.028\n",
      "model: 3, epoch: 587, loss: 0.028\n",
      "model: 3, epoch: 588, loss: 0.028\n",
      "model: 3, epoch: 589, loss: 0.028\n",
      "model: 3, epoch: 590, loss: 0.028\n",
      "model: 3, epoch: 591, loss: 0.028\n",
      "model: 3, epoch: 592, loss: 0.028\n",
      "model: 3, epoch: 593, loss: 0.028\n",
      "model: 3, epoch: 594, loss: 0.028\n",
      "model: 3, epoch: 595, loss: 0.028\n",
      "model: 3, epoch: 596, loss: 0.028\n",
      "model: 3, epoch: 597, loss: 0.028\n",
      "model: 3, epoch: 598, loss: 0.028\n",
      "model: 3, epoch: 599, loss: 0.028\n",
      "model: 3, epoch: 600, loss: 0.028\n",
      "model: 3, epoch: 601, loss: 0.028\n",
      "model: 3, epoch: 602, loss: 0.028\n",
      "model: 3, epoch: 603, loss: 0.028\n",
      "model: 3, epoch: 604, loss: 0.028\n",
      "model: 3, epoch: 605, loss: 0.028\n",
      "model: 3, epoch: 606, loss: 0.028\n",
      "model: 3, epoch: 607, loss: 0.028\n",
      "model: 3, epoch: 608, loss: 0.028\n",
      "model: 3, epoch: 609, loss: 0.028\n",
      "model: 3, epoch: 610, loss: 0.028\n",
      "model: 3, epoch: 611, loss: 0.028\n",
      "model: 3, epoch: 612, loss: 0.028\n",
      "model: 3, epoch: 613, loss: 0.028\n",
      "model: 3, epoch: 614, loss: 0.028\n",
      "model: 3, epoch: 615, loss: 0.028\n",
      "model: 3, epoch: 616, loss: 0.028\n",
      "model: 3, epoch: 617, loss: 0.028\n",
      "model: 3, epoch: 618, loss: 0.028\n",
      "model: 3, epoch: 619, loss: 0.028\n",
      "model: 3, epoch: 620, loss: 0.028\n",
      "model: 3, epoch: 621, loss: 0.028\n",
      "model: 3, epoch: 622, loss: 0.028\n",
      "model: 3, epoch: 623, loss: 0.028\n",
      "model: 3, epoch: 624, loss: 0.028\n",
      "model: 3, epoch: 625, loss: 0.028\n",
      "model: 3, epoch: 626, loss: 0.028\n",
      "model: 3, epoch: 627, loss: 0.028\n",
      "model: 3, epoch: 628, loss: 0.028\n",
      "model: 3, epoch: 629, loss: 0.028\n",
      "model: 3, epoch: 630, loss: 0.028\n",
      "model: 3, epoch: 631, loss: 0.028\n",
      "model: 3, epoch: 632, loss: 0.028\n",
      "model: 3, epoch: 633, loss: 0.028\n",
      "model: 3, epoch: 634, loss: 0.028\n",
      "model: 3, epoch: 635, loss: 0.028\n",
      "model: 3, epoch: 636, loss: 0.028\n",
      "model: 3, epoch: 637, loss: 0.028\n",
      "model: 3, epoch: 638, loss: 0.028\n",
      "model: 3, epoch: 639, loss: 0.028\n",
      "model: 3, epoch: 640, loss: 0.028\n",
      "model: 3, epoch: 641, loss: 0.028\n",
      "model: 3, epoch: 642, loss: 0.028\n",
      "model: 3, epoch: 643, loss: 0.028\n",
      "model: 3, epoch: 644, loss: 0.028\n",
      "model: 3, epoch: 645, loss: 0.028\n",
      "model: 3, epoch: 646, loss: 0.028\n",
      "model: 3, epoch: 647, loss: 0.028\n",
      "model: 3, epoch: 648, loss: 0.028\n",
      "model: 3, epoch: 649, loss: 0.028\n",
      "model: 3, epoch: 650, loss: 0.028\n",
      "model: 3, epoch: 651, loss: 0.028\n",
      "model: 3, epoch: 652, loss: 0.028\n",
      "model: 3, epoch: 653, loss: 0.028\n",
      "model: 3, epoch: 654, loss: 0.028\n",
      "model: 3, epoch: 655, loss: 0.028\n",
      "model: 3, epoch: 656, loss: 0.028\n",
      "model: 3, epoch: 657, loss: 0.028\n",
      "model: 3, epoch: 658, loss: 0.028\n",
      "model: 3, epoch: 659, loss: 0.028\n",
      "model: 3, epoch: 660, loss: 0.028\n",
      "model: 3, epoch: 661, loss: 0.028\n",
      "model: 3, epoch: 662, loss: 0.028\n",
      "model: 3, epoch: 663, loss: 0.028\n",
      "model: 3, epoch: 664, loss: 0.028\n",
      "model: 3, epoch: 665, loss: 0.028\n",
      "model: 3, epoch: 666, loss: 0.028\n",
      "model: 3, epoch: 667, loss: 0.028\n",
      "model: 3, epoch: 668, loss: 0.028\n",
      "model: 3, epoch: 669, loss: 0.028\n",
      "model: 3, epoch: 670, loss: 0.028\n",
      "model: 3, epoch: 671, loss: 0.028\n",
      "model: 3, epoch: 672, loss: 0.028\n",
      "model: 3, epoch: 673, loss: 0.028\n",
      "model: 3, epoch: 674, loss: 0.028\n",
      "model: 3, epoch: 675, loss: 0.028\n",
      "model: 3, epoch: 676, loss: 0.028\n",
      "model: 3, epoch: 677, loss: 0.028\n",
      "model: 3, epoch: 678, loss: 0.028\n",
      "model: 3, epoch: 679, loss: 0.028\n",
      "model: 3, epoch: 680, loss: 0.028\n",
      "model: 3, epoch: 681, loss: 0.028\n",
      "model: 3, epoch: 682, loss: 0.028\n",
      "model: 3, epoch: 683, loss: 0.028\n",
      "model: 3, epoch: 684, loss: 0.028\n",
      "model: 3, epoch: 685, loss: 0.028\n",
      "model: 3, epoch: 686, loss: 0.028\n",
      "model: 3, epoch: 687, loss: 0.028\n",
      "model: 3, epoch: 688, loss: 0.028\n",
      "model: 3, epoch: 689, loss: 0.028\n",
      "model: 3, epoch: 690, loss: 0.028\n",
      "model: 3, epoch: 691, loss: 0.028\n",
      "model: 3, epoch: 692, loss: 0.028\n",
      "model: 3, epoch: 693, loss: 0.028\n",
      "model: 3, epoch: 694, loss: 0.028\n",
      "model: 3, epoch: 695, loss: 0.028\n",
      "model: 3, epoch: 696, loss: 0.028\n",
      "model: 3, epoch: 697, loss: 0.028\n",
      "model: 3, epoch: 698, loss: 0.028\n",
      "model: 3, epoch: 699, loss: 0.028\n",
      "model: 3, epoch: 700, loss: 0.028\n",
      "model: 3, epoch: 701, loss: 0.028\n",
      "model: 3, epoch: 702, loss: 0.028\n",
      "model: 3, epoch: 703, loss: 0.028\n",
      "model: 3, epoch: 704, loss: 0.028\n",
      "model: 3, epoch: 705, loss: 0.028\n",
      "model: 3, epoch: 706, loss: 0.028\n",
      "model: 3, epoch: 707, loss: 0.028\n",
      "model: 3, epoch: 708, loss: 0.028\n",
      "model: 3, epoch: 709, loss: 0.028\n",
      "model: 3, epoch: 710, loss: 0.028\n",
      "model: 3, epoch: 711, loss: 0.028\n",
      "model: 3, epoch: 712, loss: 0.028\n",
      "model: 3, epoch: 713, loss: 0.028\n",
      "model: 3, epoch: 714, loss: 0.028\n",
      "model: 3, epoch: 715, loss: 0.028\n",
      "model: 3, epoch: 716, loss: 0.028\n",
      "model: 3, epoch: 717, loss: 0.028\n",
      "model: 3, epoch: 718, loss: 0.028\n",
      "model: 3, epoch: 719, loss: 0.028\n",
      "model: 3, epoch: 720, loss: 0.028\n",
      "model: 3, epoch: 721, loss: 0.028\n",
      "model: 3, epoch: 722, loss: 0.028\n",
      "model: 3, epoch: 723, loss: 0.028\n",
      "model: 3, epoch: 724, loss: 0.028\n",
      "model: 3, epoch: 725, loss: 0.028\n",
      "model: 3, epoch: 726, loss: 0.028\n",
      "model: 3, epoch: 727, loss: 0.028\n",
      "model: 3, epoch: 728, loss: 0.028\n",
      "model: 3, epoch: 729, loss: 0.028\n",
      "model: 3, epoch: 730, loss: 0.028\n",
      "model: 3, epoch: 731, loss: 0.028\n",
      "model: 3, epoch: 732, loss: 0.028\n",
      "model: 3, epoch: 733, loss: 0.028\n",
      "model: 3, epoch: 734, loss: 0.028\n",
      "model: 3, epoch: 735, loss: 0.028\n",
      "model: 3, epoch: 736, loss: 0.028\n",
      "model: 3, epoch: 737, loss: 0.028\n",
      "model: 3, epoch: 738, loss: 0.028\n",
      "model: 3, epoch: 739, loss: 0.028\n",
      "model: 3, epoch: 740, loss: 0.028\n",
      "model: 3, epoch: 741, loss: 0.028\n",
      "model: 3, epoch: 742, loss: 0.028\n",
      "model: 3, epoch: 743, loss: 0.028\n",
      "model: 3, epoch: 744, loss: 0.028\n",
      "model: 3, epoch: 745, loss: 0.028\n",
      "model: 3, epoch: 746, loss: 0.028\n",
      "model: 3, epoch: 747, loss: 0.028\n",
      "model: 3, epoch: 748, loss: 0.028\n",
      "model: 3, epoch: 749, loss: 0.028\n",
      "model: 3, epoch: 750, loss: 0.028\n",
      "model: 3, epoch: 751, loss: 0.028\n",
      "model: 3, epoch: 752, loss: 0.028\n",
      "model: 3, epoch: 753, loss: 0.028\n",
      "model: 3, epoch: 754, loss: 0.028\n",
      "model: 3, epoch: 755, loss: 0.028\n",
      "model: 3, epoch: 756, loss: 0.028\n",
      "model: 3, epoch: 757, loss: 0.028\n",
      "model: 3, epoch: 758, loss: 0.028\n",
      "model: 3, epoch: 759, loss: 0.028\n",
      "model: 3, epoch: 760, loss: 0.028\n",
      "model: 3, epoch: 761, loss: 0.028\n",
      "model: 3, epoch: 762, loss: 0.028\n",
      "model: 3, epoch: 763, loss: 0.028\n",
      "model: 3, epoch: 764, loss: 0.028\n",
      "model: 3, epoch: 765, loss: 0.028\n",
      "model: 3, epoch: 766, loss: 0.028\n",
      "model: 3, epoch: 767, loss: 0.028\n",
      "model: 3, epoch: 768, loss: 0.028\n",
      "model: 3, epoch: 769, loss: 0.028\n",
      "model: 3, epoch: 770, loss: 0.028\n",
      "model: 3, epoch: 771, loss: 0.028\n",
      "model: 3, epoch: 772, loss: 0.028\n",
      "model: 3, epoch: 773, loss: 0.028\n",
      "model: 3, epoch: 774, loss: 0.028\n",
      "model: 3, epoch: 775, loss: 0.028\n",
      "model: 3, epoch: 776, loss: 0.028\n",
      "model: 3, epoch: 777, loss: 0.028\n",
      "model: 3, epoch: 778, loss: 0.028\n",
      "model: 3, epoch: 779, loss: 0.028\n",
      "model: 3, epoch: 780, loss: 0.028\n",
      "model: 3, epoch: 781, loss: 0.028\n",
      "model: 3, epoch: 782, loss: 0.028\n",
      "model: 3, epoch: 783, loss: 0.028\n",
      "model: 3, epoch: 784, loss: 0.028\n",
      "model: 3, epoch: 785, loss: 0.028\n",
      "model: 3, epoch: 786, loss: 0.028\n",
      "model: 3, epoch: 787, loss: 0.028\n",
      "model: 3, epoch: 788, loss: 0.028\n",
      "model: 3, epoch: 789, loss: 0.028\n",
      "model: 3, epoch: 790, loss: 0.028\n",
      "model: 3, epoch: 791, loss: 0.028\n",
      "model: 3, epoch: 792, loss: 0.028\n",
      "model: 3, epoch: 793, loss: 0.028\n",
      "model: 3, epoch: 794, loss: 0.028\n",
      "model: 3, epoch: 795, loss: 0.028\n",
      "model: 3, epoch: 796, loss: 0.028\n",
      "model: 3, epoch: 797, loss: 0.028\n",
      "model: 3, epoch: 798, loss: 0.028\n",
      "model: 3, epoch: 799, loss: 0.028\n",
      "model: 3, epoch: 800, loss: 0.028\n",
      "model: 3, epoch: 801, loss: 0.028\n",
      "model: 3, epoch: 802, loss: 0.028\n",
      "model: 3, epoch: 803, loss: 0.028\n",
      "model: 3, epoch: 804, loss: 0.028\n",
      "model: 3, epoch: 805, loss: 0.028\n",
      "model: 3, epoch: 806, loss: 0.028\n",
      "model: 3, epoch: 807, loss: 0.028\n",
      "model: 3, epoch: 808, loss: 0.028\n",
      "model: 3, epoch: 809, loss: 0.028\n",
      "model: 3, epoch: 810, loss: 0.028\n",
      "model: 3, epoch: 811, loss: 0.028\n",
      "model: 3, epoch: 812, loss: 0.028\n",
      "model: 3, epoch: 813, loss: 0.028\n",
      "model: 3, epoch: 814, loss: 0.028\n",
      "model: 3, epoch: 815, loss: 0.028\n",
      "model: 3, epoch: 816, loss: 0.028\n",
      "model: 3, epoch: 817, loss: 0.028\n",
      "model: 3, epoch: 818, loss: 0.028\n",
      "model: 3, epoch: 819, loss: 0.028\n",
      "model: 3, epoch: 820, loss: 0.028\n",
      "model: 3, epoch: 821, loss: 0.028\n",
      "model: 3, epoch: 822, loss: 0.028\n",
      "model: 3, epoch: 823, loss: 0.028\n",
      "model: 3, epoch: 824, loss: 0.028\n",
      "model: 3, epoch: 825, loss: 0.028\n",
      "model: 3, epoch: 826, loss: 0.028\n",
      "model: 3, epoch: 827, loss: 0.028\n",
      "model: 3, epoch: 828, loss: 0.028\n",
      "model: 3, epoch: 829, loss: 0.028\n",
      "model: 3, epoch: 830, loss: 0.028\n",
      "model: 3, epoch: 831, loss: 0.028\n",
      "model: 3, epoch: 832, loss: 0.028\n",
      "model: 3, epoch: 833, loss: 0.028\n",
      "model: 3, epoch: 834, loss: 0.028\n",
      "model: 3, epoch: 835, loss: 0.028\n",
      "model: 3, epoch: 836, loss: 0.028\n",
      "model: 3, epoch: 837, loss: 0.028\n",
      "model: 3, epoch: 838, loss: 0.028\n",
      "model: 3, epoch: 839, loss: 0.028\n",
      "model: 3, epoch: 840, loss: 0.028\n",
      "model: 3, epoch: 841, loss: 0.028\n",
      "model: 3, epoch: 842, loss: 0.028\n",
      "model: 3, epoch: 843, loss: 0.028\n",
      "model: 3, epoch: 844, loss: 0.028\n",
      "model: 3, epoch: 845, loss: 0.028\n",
      "model: 3, epoch: 846, loss: 0.028\n",
      "model: 3, epoch: 847, loss: 0.028\n",
      "model: 3, epoch: 848, loss: 0.028\n",
      "model: 3, epoch: 849, loss: 0.028\n",
      "model: 3, epoch: 850, loss: 0.028\n",
      "model: 3, epoch: 851, loss: 0.028\n",
      "model: 3, epoch: 852, loss: 0.028\n",
      "model: 3, epoch: 853, loss: 0.028\n",
      "model: 3, epoch: 854, loss: 0.028\n",
      "model: 3, epoch: 855, loss: 0.028\n",
      "model: 3, epoch: 856, loss: 0.028\n",
      "model: 3, epoch: 857, loss: 0.028\n",
      "model: 3, epoch: 858, loss: 0.028\n",
      "model: 3, epoch: 859, loss: 0.028\n",
      "model: 3, epoch: 860, loss: 0.028\n",
      "model: 3, epoch: 861, loss: 0.028\n",
      "model: 3, epoch: 862, loss: 0.028\n",
      "model: 3, epoch: 863, loss: 0.028\n",
      "model: 3, epoch: 864, loss: 0.028\n",
      "model: 3, epoch: 865, loss: 0.028\n",
      "model: 3, epoch: 866, loss: 0.028\n",
      "model: 3, epoch: 867, loss: 0.028\n",
      "model: 3, epoch: 868, loss: 0.028\n",
      "model: 3, epoch: 869, loss: 0.028\n",
      "model: 3, epoch: 870, loss: 0.028\n",
      "model: 3, epoch: 871, loss: 0.028\n",
      "model: 3, epoch: 872, loss: 0.028\n",
      "model: 3, epoch: 873, loss: 0.028\n",
      "model: 3, epoch: 874, loss: 0.028\n",
      "model: 3, epoch: 875, loss: 0.028\n",
      "model: 3, epoch: 876, loss: 0.028\n",
      "model: 3, epoch: 877, loss: 0.028\n",
      "model: 3, epoch: 878, loss: 0.028\n",
      "model: 3, epoch: 879, loss: 0.028\n",
      "model: 3, epoch: 880, loss: 0.028\n",
      "model: 3, epoch: 881, loss: 0.028\n",
      "model: 3, epoch: 882, loss: 0.028\n",
      "model: 3, epoch: 883, loss: 0.028\n",
      "model: 3, epoch: 884, loss: 0.028\n",
      "model: 3, epoch: 885, loss: 0.028\n",
      "model: 3, epoch: 886, loss: 0.028\n",
      "model: 3, epoch: 887, loss: 0.028\n",
      "model: 3, epoch: 888, loss: 0.028\n",
      "model: 3, epoch: 889, loss: 0.028\n",
      "model: 3, epoch: 890, loss: 0.028\n",
      "model: 3, epoch: 891, loss: 0.028\n",
      "model: 3, epoch: 892, loss: 0.028\n",
      "model: 3, epoch: 893, loss: 0.028\n",
      "model: 3, epoch: 894, loss: 0.028\n",
      "model: 3, epoch: 895, loss: 0.028\n",
      "model: 3, epoch: 896, loss: 0.028\n",
      "model: 3, epoch: 897, loss: 0.028\n",
      "model: 3, epoch: 898, loss: 0.028\n",
      "model: 3, epoch: 899, loss: 0.028\n",
      "model: 3, epoch: 900, loss: 0.028\n",
      "model: 3, epoch: 901, loss: 0.028\n",
      "model: 3, epoch: 902, loss: 0.028\n",
      "model: 3, epoch: 903, loss: 0.028\n",
      "model: 3, epoch: 904, loss: 0.028\n",
      "model: 3, epoch: 905, loss: 0.028\n",
      "model: 3, epoch: 906, loss: 0.028\n",
      "model: 3, epoch: 907, loss: 0.028\n",
      "model: 3, epoch: 908, loss: 0.028\n",
      "model: 3, epoch: 909, loss: 0.028\n",
      "model: 3, epoch: 910, loss: 0.028\n",
      "model: 3, epoch: 911, loss: 0.028\n",
      "model: 3, epoch: 912, loss: 0.028\n",
      "model: 3, epoch: 913, loss: 0.028\n",
      "model: 3, epoch: 914, loss: 0.028\n",
      "model: 3, epoch: 915, loss: 0.028\n",
      "model: 3, epoch: 916, loss: 0.028\n",
      "model: 3, epoch: 917, loss: 0.028\n",
      "model: 3, epoch: 918, loss: 0.028\n",
      "model: 3, epoch: 919, loss: 0.028\n",
      "model: 3, epoch: 920, loss: 0.028\n",
      "model: 3, epoch: 921, loss: 0.028\n",
      "model: 3, epoch: 922, loss: 0.028\n",
      "model: 3, epoch: 923, loss: 0.028\n",
      "model: 3, epoch: 924, loss: 0.028\n",
      "model: 3, epoch: 925, loss: 0.028\n",
      "model: 3, epoch: 926, loss: 0.028\n",
      "model: 3, epoch: 927, loss: 0.028\n",
      "model: 3, epoch: 928, loss: 0.028\n",
      "model: 3, epoch: 929, loss: 0.028\n",
      "model: 3, epoch: 930, loss: 0.028\n",
      "model: 3, epoch: 931, loss: 0.028\n",
      "model: 3, epoch: 932, loss: 0.028\n",
      "model: 3, epoch: 933, loss: 0.028\n",
      "model: 3, epoch: 934, loss: 0.028\n",
      "model: 3, epoch: 935, loss: 0.028\n",
      "model: 3, epoch: 936, loss: 0.028\n",
      "model: 3, epoch: 937, loss: 0.028\n",
      "model: 3, epoch: 938, loss: 0.028\n",
      "model: 3, epoch: 939, loss: 0.028\n",
      "model: 3, epoch: 940, loss: 0.028\n",
      "model: 3, epoch: 941, loss: 0.028\n",
      "model: 3, epoch: 942, loss: 0.028\n",
      "model: 3, epoch: 943, loss: 0.028\n",
      "model: 3, epoch: 944, loss: 0.028\n",
      "model: 3, epoch: 945, loss: 0.028\n",
      "model: 3, epoch: 946, loss: 0.028\n",
      "model: 3, epoch: 947, loss: 0.028\n",
      "model: 3, epoch: 948, loss: 0.028\n",
      "model: 3, epoch: 949, loss: 0.028\n",
      "model: 3, epoch: 950, loss: 0.028\n",
      "model: 3, epoch: 951, loss: 0.028\n",
      "model: 3, epoch: 952, loss: 0.028\n",
      "model: 3, epoch: 953, loss: 0.028\n",
      "model: 3, epoch: 954, loss: 0.028\n",
      "model: 3, epoch: 955, loss: 0.028\n",
      "model: 3, epoch: 956, loss: 0.028\n",
      "model: 3, epoch: 957, loss: 0.028\n",
      "model: 3, epoch: 958, loss: 0.028\n",
      "model: 3, epoch: 959, loss: 0.028\n",
      "model: 3, epoch: 960, loss: 0.028\n",
      "model: 3, epoch: 961, loss: 0.028\n",
      "model: 3, epoch: 962, loss: 0.028\n",
      "model: 3, epoch: 963, loss: 0.028\n",
      "model: 3, epoch: 964, loss: 0.028\n",
      "model: 3, epoch: 965, loss: 0.028\n",
      "model: 3, epoch: 966, loss: 0.028\n",
      "model: 3, epoch: 967, loss: 0.028\n",
      "model: 3, epoch: 968, loss: 0.028\n",
      "model: 3, epoch: 969, loss: 0.028\n",
      "model: 3, epoch: 970, loss: 0.028\n",
      "model: 3, epoch: 971, loss: 0.028\n",
      "model: 3, epoch: 972, loss: 0.028\n",
      "model: 3, epoch: 973, loss: 0.028\n",
      "model: 3, epoch: 974, loss: 0.028\n",
      "model: 3, epoch: 975, loss: 0.028\n",
      "model: 3, epoch: 976, loss: 0.028\n",
      "model: 3, epoch: 977, loss: 0.028\n",
      "model: 3, epoch: 978, loss: 0.028\n",
      "model: 3, epoch: 979, loss: 0.028\n",
      "model: 3, epoch: 980, loss: 0.028\n",
      "model: 3, epoch: 981, loss: 0.028\n",
      "model: 3, epoch: 982, loss: 0.028\n",
      "model: 3, epoch: 983, loss: 0.028\n",
      "model: 3, epoch: 984, loss: 0.028\n",
      "model: 3, epoch: 985, loss: 0.028\n",
      "model: 3, epoch: 986, loss: 0.028\n",
      "model: 3, epoch: 987, loss: 0.028\n",
      "model: 3, epoch: 988, loss: 0.028\n",
      "model: 3, epoch: 989, loss: 0.028\n",
      "model: 3, epoch: 990, loss: 0.028\n",
      "model: 3, epoch: 991, loss: 0.028\n",
      "model: 3, epoch: 992, loss: 0.028\n",
      "model: 3, epoch: 993, loss: 0.028\n",
      "model: 3, epoch: 994, loss: 0.028\n",
      "model: 3, epoch: 995, loss: 0.028\n",
      "model: 3, epoch: 996, loss: 0.028\n",
      "model: 3, epoch: 997, loss: 0.028\n",
      "model: 3, epoch: 998, loss: 0.028\n",
      "model: 3, epoch: 999, loss: 0.028\n",
      "model: 4, epoch: 0, loss: 0.362\n",
      "model: 4, epoch: 1, loss: 0.332\n",
      "model: 4, epoch: 2, loss: 0.305\n",
      "model: 4, epoch: 3, loss: 0.279\n",
      "model: 4, epoch: 4, loss: 0.255\n",
      "model: 4, epoch: 5, loss: 0.234\n",
      "model: 4, epoch: 6, loss: 0.214\n",
      "model: 4, epoch: 7, loss: 0.195\n",
      "model: 4, epoch: 8, loss: 0.179\n",
      "model: 4, epoch: 9, loss: 0.164\n",
      "model: 4, epoch: 10, loss: 0.150\n",
      "model: 4, epoch: 11, loss: 0.138\n",
      "model: 4, epoch: 12, loss: 0.128\n",
      "model: 4, epoch: 13, loss: 0.118\n",
      "model: 4, epoch: 14, loss: 0.109\n",
      "model: 4, epoch: 15, loss: 0.102\n",
      "model: 4, epoch: 16, loss: 0.095\n",
      "model: 4, epoch: 17, loss: 0.089\n",
      "model: 4, epoch: 18, loss: 0.083\n",
      "model: 4, epoch: 19, loss: 0.078\n",
      "model: 4, epoch: 20, loss: 0.074\n",
      "model: 4, epoch: 21, loss: 0.069\n",
      "model: 4, epoch: 22, loss: 0.065\n",
      "model: 4, epoch: 23, loss: 0.061\n",
      "model: 4, epoch: 24, loss: 0.058\n",
      "model: 4, epoch: 25, loss: 0.054\n",
      "model: 4, epoch: 26, loss: 0.051\n",
      "model: 4, epoch: 27, loss: 0.048\n",
      "model: 4, epoch: 28, loss: 0.045\n",
      "model: 4, epoch: 29, loss: 0.043\n",
      "model: 4, epoch: 30, loss: 0.040\n",
      "model: 4, epoch: 31, loss: 0.038\n",
      "model: 4, epoch: 32, loss: 0.035\n",
      "model: 4, epoch: 33, loss: 0.034\n",
      "model: 4, epoch: 34, loss: 0.032\n",
      "model: 4, epoch: 35, loss: 0.030\n",
      "model: 4, epoch: 36, loss: 0.029\n",
      "model: 4, epoch: 37, loss: 0.028\n",
      "model: 4, epoch: 38, loss: 0.027\n",
      "model: 4, epoch: 39, loss: 0.026\n",
      "model: 4, epoch: 40, loss: 0.025\n",
      "model: 4, epoch: 41, loss: 0.025\n",
      "model: 4, epoch: 42, loss: 0.024\n",
      "model: 4, epoch: 43, loss: 0.024\n",
      "model: 4, epoch: 44, loss: 0.024\n",
      "model: 4, epoch: 45, loss: 0.024\n",
      "model: 4, epoch: 46, loss: 0.024\n",
      "model: 4, epoch: 47, loss: 0.024\n",
      "model: 4, epoch: 48, loss: 0.024\n",
      "model: 4, epoch: 49, loss: 0.024\n",
      "model: 4, epoch: 50, loss: 0.024\n",
      "model: 4, epoch: 51, loss: 0.024\n",
      "model: 4, epoch: 52, loss: 0.024\n",
      "model: 4, epoch: 53, loss: 0.023\n",
      "model: 4, epoch: 54, loss: 0.023\n",
      "model: 4, epoch: 55, loss: 0.023\n",
      "model: 4, epoch: 56, loss: 0.023\n",
      "model: 4, epoch: 57, loss: 0.023\n",
      "model: 4, epoch: 58, loss: 0.023\n",
      "model: 4, epoch: 59, loss: 0.023\n",
      "model: 4, epoch: 60, loss: 0.023\n",
      "model: 4, epoch: 61, loss: 0.023\n",
      "model: 4, epoch: 62, loss: 0.023\n",
      "model: 4, epoch: 63, loss: 0.023\n",
      "model: 4, epoch: 64, loss: 0.022\n",
      "model: 4, epoch: 65, loss: 0.022\n",
      "model: 4, epoch: 66, loss: 0.022\n",
      "model: 4, epoch: 67, loss: 0.022\n",
      "model: 4, epoch: 68, loss: 0.022\n",
      "model: 4, epoch: 69, loss: 0.022\n",
      "model: 4, epoch: 70, loss: 0.022\n",
      "model: 4, epoch: 71, loss: 0.022\n",
      "model: 4, epoch: 72, loss: 0.022\n",
      "model: 4, epoch: 73, loss: 0.021\n",
      "model: 4, epoch: 74, loss: 0.021\n",
      "model: 4, epoch: 75, loss: 0.021\n",
      "model: 4, epoch: 76, loss: 0.021\n",
      "model: 4, epoch: 77, loss: 0.021\n",
      "model: 4, epoch: 78, loss: 0.021\n",
      "model: 4, epoch: 79, loss: 0.021\n",
      "model: 4, epoch: 80, loss: 0.021\n",
      "model: 4, epoch: 81, loss: 0.021\n",
      "model: 4, epoch: 82, loss: 0.020\n",
      "model: 4, epoch: 83, loss: 0.020\n",
      "model: 4, epoch: 84, loss: 0.020\n",
      "model: 4, epoch: 85, loss: 0.020\n",
      "model: 4, epoch: 86, loss: 0.020\n",
      "model: 4, epoch: 87, loss: 0.020\n",
      "model: 4, epoch: 88, loss: 0.020\n",
      "model: 4, epoch: 89, loss: 0.020\n",
      "model: 4, epoch: 90, loss: 0.020\n",
      "model: 4, epoch: 91, loss: 0.019\n",
      "model: 4, epoch: 92, loss: 0.019\n",
      "model: 4, epoch: 93, loss: 0.019\n",
      "model: 4, epoch: 94, loss: 0.019\n",
      "model: 4, epoch: 95, loss: 0.019\n",
      "model: 4, epoch: 96, loss: 0.019\n",
      "model: 4, epoch: 97, loss: 0.019\n",
      "model: 4, epoch: 98, loss: 0.019\n",
      "model: 4, epoch: 99, loss: 0.019\n",
      "model: 4, epoch: 100, loss: 0.019\n",
      "model: 4, epoch: 101, loss: 0.018\n",
      "model: 4, epoch: 102, loss: 0.018\n",
      "model: 4, epoch: 103, loss: 0.018\n",
      "model: 4, epoch: 104, loss: 0.018\n",
      "model: 4, epoch: 105, loss: 0.018\n",
      "model: 4, epoch: 106, loss: 0.018\n",
      "model: 4, epoch: 107, loss: 0.018\n",
      "model: 4, epoch: 108, loss: 0.018\n",
      "model: 4, epoch: 109, loss: 0.018\n",
      "model: 4, epoch: 110, loss: 0.018\n",
      "model: 4, epoch: 111, loss: 0.017\n",
      "model: 4, epoch: 112, loss: 0.017\n",
      "model: 4, epoch: 113, loss: 0.017\n",
      "model: 4, epoch: 114, loss: 0.017\n",
      "model: 4, epoch: 115, loss: 0.017\n",
      "model: 4, epoch: 116, loss: 0.017\n",
      "model: 4, epoch: 117, loss: 0.017\n",
      "model: 4, epoch: 118, loss: 0.017\n",
      "model: 4, epoch: 119, loss: 0.017\n",
      "model: 4, epoch: 120, loss: 0.017\n",
      "model: 4, epoch: 121, loss: 0.016\n",
      "model: 4, epoch: 122, loss: 0.016\n",
      "model: 4, epoch: 123, loss: 0.016\n",
      "model: 4, epoch: 124, loss: 0.016\n",
      "model: 4, epoch: 125, loss: 0.016\n",
      "model: 4, epoch: 126, loss: 0.016\n",
      "model: 4, epoch: 127, loss: 0.016\n",
      "model: 4, epoch: 128, loss: 0.016\n",
      "model: 4, epoch: 129, loss: 0.016\n",
      "model: 4, epoch: 130, loss: 0.016\n",
      "model: 4, epoch: 131, loss: 0.016\n",
      "model: 4, epoch: 132, loss: 0.015\n",
      "model: 4, epoch: 133, loss: 0.015\n",
      "model: 4, epoch: 134, loss: 0.015\n",
      "model: 4, epoch: 135, loss: 0.015\n",
      "model: 4, epoch: 136, loss: 0.015\n",
      "model: 4, epoch: 137, loss: 0.015\n",
      "model: 4, epoch: 138, loss: 0.015\n",
      "model: 4, epoch: 139, loss: 0.015\n",
      "model: 4, epoch: 140, loss: 0.015\n",
      "model: 4, epoch: 141, loss: 0.015\n",
      "model: 4, epoch: 142, loss: 0.015\n",
      "model: 4, epoch: 143, loss: 0.015\n",
      "model: 4, epoch: 144, loss: 0.014\n",
      "model: 4, epoch: 145, loss: 0.014\n",
      "model: 4, epoch: 146, loss: 0.014\n",
      "model: 4, epoch: 147, loss: 0.014\n",
      "model: 4, epoch: 148, loss: 0.014\n",
      "model: 4, epoch: 149, loss: 0.014\n",
      "model: 4, epoch: 150, loss: 0.014\n",
      "model: 4, epoch: 151, loss: 0.014\n",
      "model: 4, epoch: 152, loss: 0.014\n",
      "model: 4, epoch: 153, loss: 0.014\n",
      "model: 4, epoch: 154, loss: 0.014\n",
      "model: 4, epoch: 155, loss: 0.014\n",
      "model: 4, epoch: 156, loss: 0.013\n",
      "model: 4, epoch: 157, loss: 0.013\n",
      "model: 4, epoch: 158, loss: 0.013\n",
      "model: 4, epoch: 159, loss: 0.013\n",
      "model: 4, epoch: 160, loss: 0.013\n",
      "model: 4, epoch: 161, loss: 0.013\n",
      "model: 4, epoch: 162, loss: 0.013\n",
      "model: 4, epoch: 163, loss: 0.013\n",
      "model: 4, epoch: 164, loss: 0.013\n",
      "model: 4, epoch: 165, loss: 0.013\n",
      "model: 4, epoch: 166, loss: 0.013\n",
      "model: 4, epoch: 167, loss: 0.013\n",
      "model: 4, epoch: 168, loss: 0.013\n",
      "model: 4, epoch: 169, loss: 0.013\n",
      "model: 4, epoch: 170, loss: 0.013\n",
      "model: 4, epoch: 171, loss: 0.012\n",
      "model: 4, epoch: 172, loss: 0.012\n",
      "model: 4, epoch: 173, loss: 0.012\n",
      "model: 4, epoch: 174, loss: 0.012\n",
      "model: 4, epoch: 175, loss: 0.012\n",
      "model: 4, epoch: 176, loss: 0.012\n",
      "model: 4, epoch: 177, loss: 0.012\n",
      "model: 4, epoch: 178, loss: 0.012\n",
      "model: 4, epoch: 179, loss: 0.012\n",
      "model: 4, epoch: 180, loss: 0.012\n",
      "model: 4, epoch: 181, loss: 0.012\n",
      "model: 4, epoch: 182, loss: 0.012\n",
      "model: 4, epoch: 183, loss: 0.012\n",
      "model: 4, epoch: 184, loss: 0.012\n",
      "model: 4, epoch: 185, loss: 0.012\n",
      "model: 4, epoch: 186, loss: 0.012\n",
      "model: 4, epoch: 187, loss: 0.011\n",
      "model: 4, epoch: 188, loss: 0.011\n",
      "model: 4, epoch: 189, loss: 0.011\n",
      "model: 4, epoch: 190, loss: 0.011\n",
      "model: 4, epoch: 191, loss: 0.011\n",
      "model: 4, epoch: 192, loss: 0.011\n",
      "model: 4, epoch: 193, loss: 0.011\n",
      "model: 4, epoch: 194, loss: 0.011\n",
      "model: 4, epoch: 195, loss: 0.011\n",
      "model: 4, epoch: 196, loss: 0.011\n",
      "model: 4, epoch: 197, loss: 0.011\n",
      "model: 4, epoch: 198, loss: 0.011\n",
      "model: 4, epoch: 199, loss: 0.011\n",
      "model: 4, epoch: 200, loss: 0.011\n",
      "model: 4, epoch: 201, loss: 0.011\n",
      "model: 4, epoch: 202, loss: 0.011\n",
      "model: 4, epoch: 203, loss: 0.011\n",
      "model: 4, epoch: 204, loss: 0.011\n",
      "model: 4, epoch: 205, loss: 0.011\n",
      "model: 4, epoch: 206, loss: 0.011\n",
      "model: 4, epoch: 207, loss: 0.010\n",
      "model: 4, epoch: 208, loss: 0.010\n",
      "model: 4, epoch: 209, loss: 0.010\n",
      "model: 4, epoch: 210, loss: 0.010\n",
      "model: 4, epoch: 211, loss: 0.010\n",
      "model: 4, epoch: 212, loss: 0.010\n",
      "model: 4, epoch: 213, loss: 0.010\n",
      "model: 4, epoch: 214, loss: 0.010\n",
      "model: 4, epoch: 215, loss: 0.010\n",
      "model: 4, epoch: 216, loss: 0.010\n",
      "model: 4, epoch: 217, loss: 0.010\n",
      "model: 4, epoch: 218, loss: 0.010\n",
      "model: 4, epoch: 219, loss: 0.010\n",
      "model: 4, epoch: 220, loss: 0.010\n",
      "model: 4, epoch: 221, loss: 0.010\n",
      "model: 4, epoch: 222, loss: 0.010\n",
      "model: 4, epoch: 223, loss: 0.010\n",
      "model: 4, epoch: 224, loss: 0.010\n",
      "model: 4, epoch: 225, loss: 0.010\n",
      "model: 4, epoch: 226, loss: 0.010\n",
      "model: 4, epoch: 227, loss: 0.010\n",
      "model: 4, epoch: 228, loss: 0.010\n",
      "model: 4, epoch: 229, loss: 0.010\n",
      "model: 4, epoch: 230, loss: 0.010\n",
      "model: 4, epoch: 231, loss: 0.010\n",
      "model: 4, epoch: 232, loss: 0.010\n",
      "model: 4, epoch: 233, loss: 0.010\n",
      "model: 4, epoch: 234, loss: 0.009\n",
      "model: 4, epoch: 235, loss: 0.009\n",
      "model: 4, epoch: 236, loss: 0.009\n",
      "model: 4, epoch: 237, loss: 0.009\n",
      "model: 4, epoch: 238, loss: 0.009\n",
      "model: 4, epoch: 239, loss: 0.009\n",
      "model: 4, epoch: 240, loss: 0.009\n",
      "model: 4, epoch: 241, loss: 0.009\n",
      "model: 4, epoch: 242, loss: 0.009\n",
      "model: 4, epoch: 243, loss: 0.009\n",
      "model: 4, epoch: 244, loss: 0.009\n",
      "model: 4, epoch: 245, loss: 0.009\n",
      "model: 4, epoch: 246, loss: 0.009\n",
      "model: 4, epoch: 247, loss: 0.009\n",
      "model: 4, epoch: 248, loss: 0.009\n",
      "model: 4, epoch: 249, loss: 0.009\n",
      "model: 4, epoch: 250, loss: 0.009\n",
      "model: 4, epoch: 251, loss: 0.009\n",
      "model: 4, epoch: 252, loss: 0.009\n",
      "model: 4, epoch: 253, loss: 0.009\n",
      "model: 4, epoch: 254, loss: 0.009\n",
      "model: 4, epoch: 255, loss: 0.009\n",
      "model: 4, epoch: 256, loss: 0.009\n",
      "model: 4, epoch: 257, loss: 0.009\n",
      "model: 4, epoch: 258, loss: 0.009\n",
      "model: 4, epoch: 259, loss: 0.009\n",
      "model: 4, epoch: 260, loss: 0.009\n",
      "model: 4, epoch: 261, loss: 0.009\n",
      "model: 4, epoch: 262, loss: 0.009\n",
      "model: 4, epoch: 263, loss: 0.009\n",
      "model: 4, epoch: 264, loss: 0.009\n",
      "model: 4, epoch: 265, loss: 0.009\n",
      "model: 4, epoch: 266, loss: 0.009\n",
      "model: 4, epoch: 267, loss: 0.009\n",
      "model: 4, epoch: 268, loss: 0.009\n",
      "model: 4, epoch: 269, loss: 0.009\n",
      "model: 4, epoch: 270, loss: 0.009\n",
      "model: 4, epoch: 271, loss: 0.009\n",
      "model: 4, epoch: 272, loss: 0.009\n",
      "model: 4, epoch: 273, loss: 0.008\n",
      "model: 4, epoch: 274, loss: 0.008\n",
      "model: 4, epoch: 275, loss: 0.008\n",
      "model: 4, epoch: 276, loss: 0.008\n",
      "model: 4, epoch: 277, loss: 0.008\n",
      "model: 4, epoch: 278, loss: 0.008\n",
      "model: 4, epoch: 279, loss: 0.008\n",
      "model: 4, epoch: 280, loss: 0.008\n",
      "model: 4, epoch: 281, loss: 0.008\n",
      "model: 4, epoch: 282, loss: 0.008\n",
      "model: 4, epoch: 283, loss: 0.008\n",
      "model: 4, epoch: 284, loss: 0.008\n",
      "model: 4, epoch: 285, loss: 0.008\n",
      "model: 4, epoch: 286, loss: 0.008\n",
      "model: 4, epoch: 287, loss: 0.008\n",
      "model: 4, epoch: 288, loss: 0.008\n",
      "model: 4, epoch: 289, loss: 0.008\n",
      "model: 4, epoch: 290, loss: 0.008\n",
      "model: 4, epoch: 291, loss: 0.008\n",
      "model: 4, epoch: 292, loss: 0.008\n",
      "model: 4, epoch: 293, loss: 0.008\n",
      "model: 4, epoch: 294, loss: 0.008\n",
      "model: 4, epoch: 295, loss: 0.008\n",
      "model: 4, epoch: 296, loss: 0.008\n",
      "model: 4, epoch: 297, loss: 0.008\n",
      "model: 4, epoch: 298, loss: 0.008\n",
      "model: 4, epoch: 299, loss: 0.008\n",
      "model: 4, epoch: 300, loss: 0.008\n",
      "model: 4, epoch: 301, loss: 0.008\n",
      "model: 4, epoch: 302, loss: 0.008\n",
      "model: 4, epoch: 303, loss: 0.008\n",
      "model: 4, epoch: 304, loss: 0.008\n",
      "model: 4, epoch: 305, loss: 0.008\n",
      "model: 4, epoch: 306, loss: 0.008\n",
      "model: 4, epoch: 307, loss: 0.008\n",
      "model: 4, epoch: 308, loss: 0.008\n",
      "model: 4, epoch: 309, loss: 0.008\n",
      "model: 4, epoch: 310, loss: 0.008\n",
      "model: 4, epoch: 311, loss: 0.008\n",
      "model: 4, epoch: 312, loss: 0.008\n",
      "model: 4, epoch: 313, loss: 0.008\n",
      "model: 4, epoch: 314, loss: 0.008\n",
      "model: 4, epoch: 315, loss: 0.008\n",
      "model: 4, epoch: 316, loss: 0.008\n",
      "model: 4, epoch: 317, loss: 0.008\n",
      "model: 4, epoch: 318, loss: 0.008\n",
      "model: 4, epoch: 319, loss: 0.008\n",
      "model: 4, epoch: 320, loss: 0.008\n",
      "model: 4, epoch: 321, loss: 0.008\n",
      "model: 4, epoch: 322, loss: 0.008\n",
      "model: 4, epoch: 323, loss: 0.008\n",
      "model: 4, epoch: 324, loss: 0.008\n",
      "model: 4, epoch: 325, loss: 0.008\n",
      "model: 4, epoch: 326, loss: 0.008\n",
      "model: 4, epoch: 327, loss: 0.008\n",
      "model: 4, epoch: 328, loss: 0.008\n",
      "model: 4, epoch: 329, loss: 0.008\n",
      "model: 4, epoch: 330, loss: 0.008\n",
      "model: 4, epoch: 331, loss: 0.008\n",
      "model: 4, epoch: 332, loss: 0.008\n",
      "model: 4, epoch: 333, loss: 0.008\n",
      "model: 4, epoch: 334, loss: 0.008\n",
      "model: 4, epoch: 335, loss: 0.008\n",
      "model: 4, epoch: 336, loss: 0.008\n",
      "model: 4, epoch: 337, loss: 0.008\n",
      "model: 4, epoch: 338, loss: 0.008\n",
      "model: 4, epoch: 339, loss: 0.008\n",
      "model: 4, epoch: 340, loss: 0.008\n",
      "model: 4, epoch: 341, loss: 0.008\n",
      "model: 4, epoch: 342, loss: 0.008\n",
      "model: 4, epoch: 343, loss: 0.008\n",
      "model: 4, epoch: 344, loss: 0.008\n",
      "model: 4, epoch: 345, loss: 0.008\n",
      "model: 4, epoch: 346, loss: 0.008\n",
      "model: 4, epoch: 347, loss: 0.008\n",
      "model: 4, epoch: 348, loss: 0.008\n",
      "model: 4, epoch: 349, loss: 0.008\n",
      "model: 4, epoch: 350, loss: 0.008\n",
      "model: 4, epoch: 351, loss: 0.007\n",
      "model: 4, epoch: 352, loss: 0.007\n",
      "model: 4, epoch: 353, loss: 0.007\n",
      "model: 4, epoch: 354, loss: 0.007\n",
      "model: 4, epoch: 355, loss: 0.007\n",
      "model: 4, epoch: 356, loss: 0.007\n",
      "model: 4, epoch: 357, loss: 0.007\n",
      "model: 4, epoch: 358, loss: 0.007\n",
      "model: 4, epoch: 359, loss: 0.007\n",
      "model: 4, epoch: 360, loss: 0.007\n",
      "model: 4, epoch: 361, loss: 0.007\n",
      "model: 4, epoch: 362, loss: 0.007\n",
      "model: 4, epoch: 363, loss: 0.007\n",
      "model: 4, epoch: 364, loss: 0.007\n",
      "model: 4, epoch: 365, loss: 0.007\n",
      "model: 4, epoch: 366, loss: 0.007\n",
      "model: 4, epoch: 367, loss: 0.007\n",
      "model: 4, epoch: 368, loss: 0.007\n",
      "model: 4, epoch: 369, loss: 0.007\n",
      "model: 4, epoch: 370, loss: 0.007\n",
      "model: 4, epoch: 371, loss: 0.007\n",
      "model: 4, epoch: 372, loss: 0.007\n",
      "model: 4, epoch: 373, loss: 0.007\n",
      "model: 4, epoch: 374, loss: 0.007\n",
      "model: 4, epoch: 375, loss: 0.007\n",
      "model: 4, epoch: 376, loss: 0.007\n",
      "model: 4, epoch: 377, loss: 0.007\n",
      "model: 4, epoch: 378, loss: 0.007\n",
      "model: 4, epoch: 379, loss: 0.007\n",
      "model: 4, epoch: 380, loss: 0.007\n",
      "model: 4, epoch: 381, loss: 0.007\n",
      "model: 4, epoch: 382, loss: 0.007\n",
      "model: 4, epoch: 383, loss: 0.007\n",
      "model: 4, epoch: 384, loss: 0.007\n",
      "model: 4, epoch: 385, loss: 0.007\n",
      "model: 4, epoch: 386, loss: 0.007\n",
      "model: 4, epoch: 387, loss: 0.007\n",
      "model: 4, epoch: 388, loss: 0.007\n",
      "model: 4, epoch: 389, loss: 0.007\n",
      "model: 4, epoch: 390, loss: 0.007\n",
      "model: 4, epoch: 391, loss: 0.007\n",
      "model: 4, epoch: 392, loss: 0.007\n",
      "model: 4, epoch: 393, loss: 0.007\n",
      "model: 4, epoch: 394, loss: 0.007\n",
      "model: 4, epoch: 395, loss: 0.007\n",
      "model: 4, epoch: 396, loss: 0.007\n",
      "model: 4, epoch: 397, loss: 0.007\n",
      "model: 4, epoch: 398, loss: 0.007\n",
      "model: 4, epoch: 399, loss: 0.007\n",
      "model: 4, epoch: 400, loss: 0.007\n",
      "model: 4, epoch: 401, loss: 0.007\n",
      "model: 4, epoch: 402, loss: 0.007\n",
      "model: 4, epoch: 403, loss: 0.007\n",
      "model: 4, epoch: 404, loss: 0.007\n",
      "model: 4, epoch: 405, loss: 0.007\n",
      "model: 4, epoch: 406, loss: 0.007\n",
      "model: 4, epoch: 407, loss: 0.007\n",
      "model: 4, epoch: 408, loss: 0.007\n",
      "model: 4, epoch: 409, loss: 0.007\n",
      "model: 4, epoch: 410, loss: 0.007\n",
      "model: 4, epoch: 411, loss: 0.007\n",
      "model: 4, epoch: 412, loss: 0.007\n",
      "model: 4, epoch: 413, loss: 0.007\n",
      "model: 4, epoch: 414, loss: 0.007\n",
      "model: 4, epoch: 415, loss: 0.007\n",
      "model: 4, epoch: 416, loss: 0.007\n",
      "model: 4, epoch: 417, loss: 0.007\n",
      "model: 4, epoch: 418, loss: 0.007\n",
      "model: 4, epoch: 419, loss: 0.007\n",
      "model: 4, epoch: 420, loss: 0.007\n",
      "model: 4, epoch: 421, loss: 0.007\n",
      "model: 4, epoch: 422, loss: 0.007\n",
      "model: 4, epoch: 423, loss: 0.007\n",
      "model: 4, epoch: 424, loss: 0.007\n",
      "model: 4, epoch: 425, loss: 0.007\n",
      "model: 4, epoch: 426, loss: 0.007\n",
      "model: 4, epoch: 427, loss: 0.007\n",
      "model: 4, epoch: 428, loss: 0.007\n",
      "model: 4, epoch: 429, loss: 0.007\n",
      "model: 4, epoch: 430, loss: 0.007\n",
      "model: 4, epoch: 431, loss: 0.007\n",
      "model: 4, epoch: 432, loss: 0.007\n",
      "model: 4, epoch: 433, loss: 0.007\n",
      "model: 4, epoch: 434, loss: 0.007\n",
      "model: 4, epoch: 435, loss: 0.007\n",
      "model: 4, epoch: 436, loss: 0.007\n",
      "model: 4, epoch: 437, loss: 0.007\n",
      "model: 4, epoch: 438, loss: 0.007\n",
      "model: 4, epoch: 439, loss: 0.007\n",
      "model: 4, epoch: 440, loss: 0.007\n",
      "model: 4, epoch: 441, loss: 0.007\n",
      "model: 4, epoch: 442, loss: 0.007\n",
      "model: 4, epoch: 443, loss: 0.007\n",
      "model: 4, epoch: 444, loss: 0.007\n",
      "model: 4, epoch: 445, loss: 0.007\n",
      "model: 4, epoch: 446, loss: 0.007\n",
      "model: 4, epoch: 447, loss: 0.007\n",
      "model: 4, epoch: 448, loss: 0.007\n",
      "model: 4, epoch: 449, loss: 0.007\n",
      "model: 4, epoch: 450, loss: 0.007\n",
      "model: 4, epoch: 451, loss: 0.007\n",
      "model: 4, epoch: 452, loss: 0.007\n",
      "model: 4, epoch: 453, loss: 0.007\n",
      "model: 4, epoch: 454, loss: 0.007\n",
      "model: 4, epoch: 455, loss: 0.007\n",
      "model: 4, epoch: 456, loss: 0.007\n",
      "model: 4, epoch: 457, loss: 0.007\n",
      "model: 4, epoch: 458, loss: 0.007\n",
      "model: 4, epoch: 459, loss: 0.007\n",
      "model: 4, epoch: 460, loss: 0.007\n",
      "model: 4, epoch: 461, loss: 0.007\n",
      "model: 4, epoch: 462, loss: 0.007\n",
      "model: 4, epoch: 463, loss: 0.007\n",
      "model: 4, epoch: 464, loss: 0.007\n",
      "model: 4, epoch: 465, loss: 0.007\n",
      "model: 4, epoch: 466, loss: 0.007\n",
      "model: 4, epoch: 467, loss: 0.007\n",
      "model: 4, epoch: 468, loss: 0.007\n",
      "model: 4, epoch: 469, loss: 0.007\n",
      "model: 4, epoch: 470, loss: 0.007\n",
      "model: 4, epoch: 471, loss: 0.007\n",
      "model: 4, epoch: 472, loss: 0.007\n",
      "model: 4, epoch: 473, loss: 0.007\n",
      "model: 4, epoch: 474, loss: 0.007\n",
      "model: 4, epoch: 475, loss: 0.007\n",
      "model: 4, epoch: 476, loss: 0.007\n",
      "model: 4, epoch: 477, loss: 0.007\n",
      "model: 4, epoch: 478, loss: 0.007\n",
      "model: 4, epoch: 479, loss: 0.007\n",
      "model: 4, epoch: 480, loss: 0.007\n",
      "model: 4, epoch: 481, loss: 0.007\n",
      "model: 4, epoch: 482, loss: 0.007\n",
      "model: 4, epoch: 483, loss: 0.007\n",
      "model: 4, epoch: 484, loss: 0.007\n",
      "model: 4, epoch: 485, loss: 0.007\n",
      "model: 4, epoch: 486, loss: 0.007\n",
      "model: 4, epoch: 487, loss: 0.007\n",
      "model: 4, epoch: 488, loss: 0.007\n",
      "model: 4, epoch: 489, loss: 0.007\n",
      "model: 4, epoch: 490, loss: 0.007\n",
      "model: 4, epoch: 491, loss: 0.007\n",
      "model: 4, epoch: 492, loss: 0.007\n",
      "model: 4, epoch: 493, loss: 0.007\n",
      "model: 4, epoch: 494, loss: 0.007\n",
      "model: 4, epoch: 495, loss: 0.007\n",
      "model: 4, epoch: 496, loss: 0.007\n",
      "model: 4, epoch: 497, loss: 0.007\n",
      "model: 4, epoch: 498, loss: 0.007\n",
      "model: 4, epoch: 499, loss: 0.007\n",
      "model: 4, epoch: 500, loss: 0.007\n",
      "model: 4, epoch: 501, loss: 0.007\n",
      "model: 4, epoch: 502, loss: 0.007\n",
      "model: 4, epoch: 503, loss: 0.007\n",
      "model: 4, epoch: 504, loss: 0.007\n",
      "model: 4, epoch: 505, loss: 0.007\n",
      "model: 4, epoch: 506, loss: 0.007\n",
      "model: 4, epoch: 507, loss: 0.007\n",
      "model: 4, epoch: 508, loss: 0.007\n",
      "model: 4, epoch: 509, loss: 0.007\n",
      "model: 4, epoch: 510, loss: 0.007\n",
      "model: 4, epoch: 511, loss: 0.007\n",
      "model: 4, epoch: 512, loss: 0.007\n",
      "model: 4, epoch: 513, loss: 0.007\n",
      "model: 4, epoch: 514, loss: 0.007\n",
      "model: 4, epoch: 515, loss: 0.007\n",
      "model: 4, epoch: 516, loss: 0.007\n",
      "model: 4, epoch: 517, loss: 0.007\n",
      "model: 4, epoch: 518, loss: 0.007\n",
      "model: 4, epoch: 519, loss: 0.007\n",
      "model: 4, epoch: 520, loss: 0.007\n",
      "model: 4, epoch: 521, loss: 0.007\n",
      "model: 4, epoch: 522, loss: 0.007\n",
      "model: 4, epoch: 523, loss: 0.007\n",
      "model: 4, epoch: 524, loss: 0.007\n",
      "model: 4, epoch: 525, loss: 0.007\n",
      "model: 4, epoch: 526, loss: 0.007\n",
      "model: 4, epoch: 527, loss: 0.007\n",
      "model: 4, epoch: 528, loss: 0.007\n",
      "model: 4, epoch: 529, loss: 0.007\n",
      "model: 4, epoch: 530, loss: 0.007\n",
      "model: 4, epoch: 531, loss: 0.007\n",
      "model: 4, epoch: 532, loss: 0.007\n",
      "model: 4, epoch: 533, loss: 0.007\n",
      "model: 4, epoch: 534, loss: 0.007\n",
      "model: 4, epoch: 535, loss: 0.007\n",
      "model: 4, epoch: 536, loss: 0.007\n",
      "model: 4, epoch: 537, loss: 0.007\n",
      "model: 4, epoch: 538, loss: 0.007\n",
      "model: 4, epoch: 539, loss: 0.007\n",
      "model: 4, epoch: 540, loss: 0.007\n",
      "model: 4, epoch: 541, loss: 0.007\n",
      "model: 4, epoch: 542, loss: 0.007\n",
      "model: 4, epoch: 543, loss: 0.007\n",
      "model: 4, epoch: 544, loss: 0.007\n",
      "model: 4, epoch: 545, loss: 0.007\n",
      "model: 4, epoch: 546, loss: 0.007\n",
      "model: 4, epoch: 547, loss: 0.007\n",
      "model: 4, epoch: 548, loss: 0.007\n",
      "model: 4, epoch: 549, loss: 0.007\n",
      "model: 4, epoch: 550, loss: 0.007\n",
      "model: 4, epoch: 551, loss: 0.007\n",
      "model: 4, epoch: 552, loss: 0.007\n",
      "model: 4, epoch: 553, loss: 0.007\n",
      "model: 4, epoch: 554, loss: 0.007\n",
      "model: 4, epoch: 555, loss: 0.007\n",
      "model: 4, epoch: 556, loss: 0.007\n",
      "model: 4, epoch: 557, loss: 0.007\n",
      "model: 4, epoch: 558, loss: 0.007\n",
      "model: 4, epoch: 559, loss: 0.007\n",
      "model: 4, epoch: 560, loss: 0.007\n",
      "model: 4, epoch: 561, loss: 0.007\n",
      "model: 4, epoch: 562, loss: 0.007\n",
      "model: 4, epoch: 563, loss: 0.007\n",
      "model: 4, epoch: 564, loss: 0.007\n",
      "model: 4, epoch: 565, loss: 0.007\n",
      "model: 4, epoch: 566, loss: 0.007\n",
      "model: 4, epoch: 567, loss: 0.007\n",
      "model: 4, epoch: 568, loss: 0.007\n",
      "model: 4, epoch: 569, loss: 0.007\n",
      "model: 4, epoch: 570, loss: 0.007\n",
      "model: 4, epoch: 571, loss: 0.007\n",
      "model: 4, epoch: 572, loss: 0.007\n",
      "model: 4, epoch: 573, loss: 0.007\n",
      "model: 4, epoch: 574, loss: 0.007\n",
      "model: 4, epoch: 575, loss: 0.007\n",
      "model: 4, epoch: 576, loss: 0.007\n",
      "model: 4, epoch: 577, loss: 0.007\n",
      "model: 4, epoch: 578, loss: 0.007\n",
      "model: 4, epoch: 579, loss: 0.007\n",
      "model: 4, epoch: 580, loss: 0.007\n",
      "model: 4, epoch: 581, loss: 0.007\n",
      "model: 4, epoch: 582, loss: 0.007\n",
      "model: 4, epoch: 583, loss: 0.007\n",
      "model: 4, epoch: 584, loss: 0.007\n",
      "model: 4, epoch: 585, loss: 0.007\n",
      "model: 4, epoch: 586, loss: 0.007\n",
      "model: 4, epoch: 587, loss: 0.007\n",
      "model: 4, epoch: 588, loss: 0.007\n",
      "model: 4, epoch: 589, loss: 0.007\n",
      "model: 4, epoch: 590, loss: 0.007\n",
      "model: 4, epoch: 591, loss: 0.007\n",
      "model: 4, epoch: 592, loss: 0.007\n",
      "model: 4, epoch: 593, loss: 0.007\n",
      "model: 4, epoch: 594, loss: 0.007\n",
      "model: 4, epoch: 595, loss: 0.007\n",
      "model: 4, epoch: 596, loss: 0.007\n",
      "model: 4, epoch: 597, loss: 0.007\n",
      "model: 4, epoch: 598, loss: 0.007\n",
      "model: 4, epoch: 599, loss: 0.007\n",
      "model: 4, epoch: 600, loss: 0.007\n",
      "model: 4, epoch: 601, loss: 0.007\n",
      "model: 4, epoch: 602, loss: 0.007\n",
      "model: 4, epoch: 603, loss: 0.007\n",
      "model: 4, epoch: 604, loss: 0.007\n",
      "model: 4, epoch: 605, loss: 0.007\n",
      "model: 4, epoch: 606, loss: 0.007\n",
      "model: 4, epoch: 607, loss: 0.007\n",
      "model: 4, epoch: 608, loss: 0.007\n",
      "model: 4, epoch: 609, loss: 0.007\n",
      "model: 4, epoch: 610, loss: 0.007\n",
      "model: 4, epoch: 611, loss: 0.007\n",
      "model: 4, epoch: 612, loss: 0.007\n",
      "model: 4, epoch: 613, loss: 0.007\n",
      "model: 4, epoch: 614, loss: 0.007\n",
      "model: 4, epoch: 615, loss: 0.007\n",
      "model: 4, epoch: 616, loss: 0.007\n",
      "model: 4, epoch: 617, loss: 0.007\n",
      "model: 4, epoch: 618, loss: 0.007\n",
      "model: 4, epoch: 619, loss: 0.007\n",
      "model: 4, epoch: 620, loss: 0.007\n",
      "model: 4, epoch: 621, loss: 0.007\n",
      "model: 4, epoch: 622, loss: 0.007\n",
      "model: 4, epoch: 623, loss: 0.007\n",
      "model: 4, epoch: 624, loss: 0.007\n",
      "model: 4, epoch: 625, loss: 0.007\n",
      "model: 4, epoch: 626, loss: 0.007\n",
      "model: 4, epoch: 627, loss: 0.007\n",
      "model: 4, epoch: 628, loss: 0.007\n",
      "model: 4, epoch: 629, loss: 0.007\n",
      "model: 4, epoch: 630, loss: 0.007\n",
      "model: 4, epoch: 631, loss: 0.007\n",
      "model: 4, epoch: 632, loss: 0.007\n",
      "model: 4, epoch: 633, loss: 0.007\n",
      "model: 4, epoch: 634, loss: 0.007\n",
      "model: 4, epoch: 635, loss: 0.007\n",
      "model: 4, epoch: 636, loss: 0.007\n",
      "model: 4, epoch: 637, loss: 0.007\n",
      "model: 4, epoch: 638, loss: 0.007\n",
      "model: 4, epoch: 639, loss: 0.007\n",
      "model: 4, epoch: 640, loss: 0.007\n",
      "model: 4, epoch: 641, loss: 0.007\n",
      "model: 4, epoch: 642, loss: 0.007\n",
      "model: 4, epoch: 643, loss: 0.007\n",
      "model: 4, epoch: 644, loss: 0.007\n",
      "model: 4, epoch: 645, loss: 0.007\n",
      "model: 4, epoch: 646, loss: 0.007\n",
      "model: 4, epoch: 647, loss: 0.007\n",
      "model: 4, epoch: 648, loss: 0.007\n",
      "model: 4, epoch: 649, loss: 0.007\n",
      "model: 4, epoch: 650, loss: 0.007\n",
      "model: 4, epoch: 651, loss: 0.007\n",
      "model: 4, epoch: 652, loss: 0.007\n",
      "model: 4, epoch: 653, loss: 0.007\n",
      "model: 4, epoch: 654, loss: 0.007\n",
      "model: 4, epoch: 655, loss: 0.007\n",
      "model: 4, epoch: 656, loss: 0.007\n",
      "model: 4, epoch: 657, loss: 0.007\n",
      "model: 4, epoch: 658, loss: 0.007\n",
      "model: 4, epoch: 659, loss: 0.007\n",
      "model: 4, epoch: 660, loss: 0.007\n",
      "model: 4, epoch: 661, loss: 0.007\n",
      "model: 4, epoch: 662, loss: 0.007\n",
      "model: 4, epoch: 663, loss: 0.007\n",
      "model: 4, epoch: 664, loss: 0.007\n",
      "model: 4, epoch: 665, loss: 0.007\n",
      "model: 4, epoch: 666, loss: 0.006\n",
      "model: 4, epoch: 667, loss: 0.006\n",
      "model: 4, epoch: 668, loss: 0.006\n",
      "model: 4, epoch: 669, loss: 0.006\n",
      "model: 4, epoch: 670, loss: 0.006\n",
      "model: 4, epoch: 671, loss: 0.006\n",
      "model: 4, epoch: 672, loss: 0.006\n",
      "model: 4, epoch: 673, loss: 0.006\n",
      "model: 4, epoch: 674, loss: 0.006\n",
      "model: 4, epoch: 675, loss: 0.006\n",
      "model: 4, epoch: 676, loss: 0.006\n",
      "model: 4, epoch: 677, loss: 0.006\n",
      "model: 4, epoch: 678, loss: 0.006\n",
      "model: 4, epoch: 679, loss: 0.006\n",
      "model: 4, epoch: 680, loss: 0.006\n",
      "model: 4, epoch: 681, loss: 0.006\n",
      "model: 4, epoch: 682, loss: 0.006\n",
      "model: 4, epoch: 683, loss: 0.006\n",
      "model: 4, epoch: 684, loss: 0.006\n",
      "model: 4, epoch: 685, loss: 0.006\n",
      "model: 4, epoch: 686, loss: 0.006\n",
      "model: 4, epoch: 687, loss: 0.006\n",
      "model: 4, epoch: 688, loss: 0.006\n",
      "model: 4, epoch: 689, loss: 0.006\n",
      "model: 4, epoch: 690, loss: 0.006\n",
      "model: 4, epoch: 691, loss: 0.006\n",
      "model: 4, epoch: 692, loss: 0.006\n",
      "model: 4, epoch: 693, loss: 0.006\n",
      "model: 4, epoch: 694, loss: 0.006\n",
      "model: 4, epoch: 695, loss: 0.006\n",
      "model: 4, epoch: 696, loss: 0.006\n",
      "model: 4, epoch: 697, loss: 0.006\n",
      "model: 4, epoch: 698, loss: 0.006\n",
      "model: 4, epoch: 699, loss: 0.006\n",
      "model: 4, epoch: 700, loss: 0.006\n",
      "model: 4, epoch: 701, loss: 0.006\n",
      "model: 4, epoch: 702, loss: 0.006\n",
      "model: 4, epoch: 703, loss: 0.006\n",
      "model: 4, epoch: 704, loss: 0.006\n",
      "model: 4, epoch: 705, loss: 0.006\n",
      "model: 4, epoch: 706, loss: 0.006\n",
      "model: 4, epoch: 707, loss: 0.006\n",
      "model: 4, epoch: 708, loss: 0.006\n",
      "model: 4, epoch: 709, loss: 0.006\n",
      "model: 4, epoch: 710, loss: 0.006\n",
      "model: 4, epoch: 711, loss: 0.006\n",
      "model: 4, epoch: 712, loss: 0.006\n",
      "model: 4, epoch: 713, loss: 0.006\n",
      "model: 4, epoch: 714, loss: 0.006\n",
      "model: 4, epoch: 715, loss: 0.006\n",
      "model: 4, epoch: 716, loss: 0.006\n",
      "model: 4, epoch: 717, loss: 0.006\n",
      "model: 4, epoch: 718, loss: 0.006\n",
      "model: 4, epoch: 719, loss: 0.006\n",
      "model: 4, epoch: 720, loss: 0.006\n",
      "model: 4, epoch: 721, loss: 0.006\n",
      "model: 4, epoch: 722, loss: 0.006\n",
      "model: 4, epoch: 723, loss: 0.006\n",
      "model: 4, epoch: 724, loss: 0.006\n",
      "model: 4, epoch: 725, loss: 0.006\n",
      "model: 4, epoch: 726, loss: 0.006\n",
      "model: 4, epoch: 727, loss: 0.006\n",
      "model: 4, epoch: 728, loss: 0.006\n",
      "model: 4, epoch: 729, loss: 0.006\n",
      "model: 4, epoch: 730, loss: 0.006\n",
      "model: 4, epoch: 731, loss: 0.006\n",
      "model: 4, epoch: 732, loss: 0.006\n",
      "model: 4, epoch: 733, loss: 0.006\n",
      "model: 4, epoch: 734, loss: 0.006\n",
      "model: 4, epoch: 735, loss: 0.006\n",
      "model: 4, epoch: 736, loss: 0.006\n",
      "model: 4, epoch: 737, loss: 0.006\n",
      "model: 4, epoch: 738, loss: 0.006\n",
      "model: 4, epoch: 739, loss: 0.006\n",
      "model: 4, epoch: 740, loss: 0.006\n",
      "model: 4, epoch: 741, loss: 0.006\n",
      "model: 4, epoch: 742, loss: 0.006\n",
      "model: 4, epoch: 743, loss: 0.006\n",
      "model: 4, epoch: 744, loss: 0.006\n",
      "model: 4, epoch: 745, loss: 0.006\n",
      "model: 4, epoch: 746, loss: 0.006\n",
      "model: 4, epoch: 747, loss: 0.006\n",
      "model: 4, epoch: 748, loss: 0.006\n",
      "model: 4, epoch: 749, loss: 0.006\n",
      "model: 4, epoch: 750, loss: 0.006\n",
      "model: 4, epoch: 751, loss: 0.006\n",
      "model: 4, epoch: 752, loss: 0.006\n",
      "model: 4, epoch: 753, loss: 0.006\n",
      "model: 4, epoch: 754, loss: 0.006\n",
      "model: 4, epoch: 755, loss: 0.006\n",
      "model: 4, epoch: 756, loss: 0.006\n",
      "model: 4, epoch: 757, loss: 0.006\n",
      "model: 4, epoch: 758, loss: 0.006\n",
      "model: 4, epoch: 759, loss: 0.006\n",
      "model: 4, epoch: 760, loss: 0.006\n",
      "model: 4, epoch: 761, loss: 0.006\n",
      "model: 4, epoch: 762, loss: 0.006\n",
      "model: 4, epoch: 763, loss: 0.006\n",
      "model: 4, epoch: 764, loss: 0.006\n",
      "model: 4, epoch: 765, loss: 0.006\n",
      "model: 4, epoch: 766, loss: 0.006\n",
      "model: 4, epoch: 767, loss: 0.006\n",
      "model: 4, epoch: 768, loss: 0.006\n",
      "model: 4, epoch: 769, loss: 0.006\n",
      "model: 4, epoch: 770, loss: 0.006\n",
      "model: 4, epoch: 771, loss: 0.006\n",
      "model: 4, epoch: 772, loss: 0.006\n",
      "model: 4, epoch: 773, loss: 0.006\n",
      "model: 4, epoch: 774, loss: 0.006\n",
      "model: 4, epoch: 775, loss: 0.006\n",
      "model: 4, epoch: 776, loss: 0.006\n",
      "model: 4, epoch: 777, loss: 0.006\n",
      "model: 4, epoch: 778, loss: 0.006\n",
      "model: 4, epoch: 779, loss: 0.006\n",
      "model: 4, epoch: 780, loss: 0.006\n",
      "model: 4, epoch: 781, loss: 0.006\n",
      "model: 4, epoch: 782, loss: 0.006\n",
      "model: 4, epoch: 783, loss: 0.006\n",
      "model: 4, epoch: 784, loss: 0.006\n",
      "model: 4, epoch: 785, loss: 0.006\n",
      "model: 4, epoch: 786, loss: 0.006\n",
      "model: 4, epoch: 787, loss: 0.006\n",
      "model: 4, epoch: 788, loss: 0.006\n",
      "model: 4, epoch: 789, loss: 0.006\n",
      "model: 4, epoch: 790, loss: 0.006\n",
      "model: 4, epoch: 791, loss: 0.006\n",
      "model: 4, epoch: 792, loss: 0.006\n",
      "model: 4, epoch: 793, loss: 0.006\n",
      "model: 4, epoch: 794, loss: 0.006\n",
      "model: 4, epoch: 795, loss: 0.006\n",
      "model: 4, epoch: 796, loss: 0.006\n",
      "model: 4, epoch: 797, loss: 0.006\n",
      "model: 4, epoch: 798, loss: 0.006\n",
      "model: 4, epoch: 799, loss: 0.006\n",
      "model: 4, epoch: 800, loss: 0.006\n",
      "model: 4, epoch: 801, loss: 0.006\n",
      "model: 4, epoch: 802, loss: 0.006\n",
      "model: 4, epoch: 803, loss: 0.006\n",
      "model: 4, epoch: 804, loss: 0.006\n",
      "model: 4, epoch: 805, loss: 0.006\n",
      "model: 4, epoch: 806, loss: 0.006\n",
      "model: 4, epoch: 807, loss: 0.006\n",
      "model: 4, epoch: 808, loss: 0.006\n",
      "model: 4, epoch: 809, loss: 0.006\n",
      "model: 4, epoch: 810, loss: 0.006\n",
      "model: 4, epoch: 811, loss: 0.006\n",
      "model: 4, epoch: 812, loss: 0.006\n",
      "model: 4, epoch: 813, loss: 0.006\n",
      "model: 4, epoch: 814, loss: 0.006\n",
      "model: 4, epoch: 815, loss: 0.006\n",
      "model: 4, epoch: 816, loss: 0.006\n",
      "model: 4, epoch: 817, loss: 0.006\n",
      "model: 4, epoch: 818, loss: 0.006\n",
      "model: 4, epoch: 819, loss: 0.006\n",
      "model: 4, epoch: 820, loss: 0.006\n",
      "model: 4, epoch: 821, loss: 0.006\n",
      "model: 4, epoch: 822, loss: 0.006\n",
      "model: 4, epoch: 823, loss: 0.006\n",
      "model: 4, epoch: 824, loss: 0.006\n",
      "model: 4, epoch: 825, loss: 0.006\n",
      "model: 4, epoch: 826, loss: 0.006\n",
      "model: 4, epoch: 827, loss: 0.006\n",
      "model: 4, epoch: 828, loss: 0.006\n",
      "model: 4, epoch: 829, loss: 0.006\n",
      "model: 4, epoch: 830, loss: 0.006\n",
      "model: 4, epoch: 831, loss: 0.006\n",
      "model: 4, epoch: 832, loss: 0.006\n",
      "model: 4, epoch: 833, loss: 0.006\n",
      "model: 4, epoch: 834, loss: 0.006\n",
      "model: 4, epoch: 835, loss: 0.006\n",
      "model: 4, epoch: 836, loss: 0.006\n",
      "model: 4, epoch: 837, loss: 0.006\n",
      "model: 4, epoch: 838, loss: 0.006\n",
      "model: 4, epoch: 839, loss: 0.006\n",
      "model: 4, epoch: 840, loss: 0.006\n",
      "model: 4, epoch: 841, loss: 0.006\n",
      "model: 4, epoch: 842, loss: 0.006\n",
      "model: 4, epoch: 843, loss: 0.006\n",
      "model: 4, epoch: 844, loss: 0.006\n",
      "model: 4, epoch: 845, loss: 0.006\n",
      "model: 4, epoch: 846, loss: 0.006\n",
      "model: 4, epoch: 847, loss: 0.006\n",
      "model: 4, epoch: 848, loss: 0.006\n",
      "model: 4, epoch: 849, loss: 0.006\n",
      "model: 4, epoch: 850, loss: 0.006\n",
      "model: 4, epoch: 851, loss: 0.006\n",
      "model: 4, epoch: 852, loss: 0.006\n",
      "model: 4, epoch: 853, loss: 0.006\n",
      "model: 4, epoch: 854, loss: 0.006\n",
      "model: 4, epoch: 855, loss: 0.006\n",
      "model: 4, epoch: 856, loss: 0.006\n",
      "model: 4, epoch: 857, loss: 0.006\n",
      "model: 4, epoch: 858, loss: 0.006\n",
      "model: 4, epoch: 859, loss: 0.006\n",
      "model: 4, epoch: 860, loss: 0.006\n",
      "model: 4, epoch: 861, loss: 0.006\n",
      "model: 4, epoch: 862, loss: 0.006\n",
      "model: 4, epoch: 863, loss: 0.006\n",
      "model: 4, epoch: 864, loss: 0.006\n",
      "model: 4, epoch: 865, loss: 0.006\n",
      "model: 4, epoch: 866, loss: 0.006\n",
      "model: 4, epoch: 867, loss: 0.006\n",
      "model: 4, epoch: 868, loss: 0.006\n",
      "model: 4, epoch: 869, loss: 0.006\n",
      "model: 4, epoch: 870, loss: 0.006\n",
      "model: 4, epoch: 871, loss: 0.006\n",
      "model: 4, epoch: 872, loss: 0.006\n",
      "model: 4, epoch: 873, loss: 0.006\n",
      "model: 4, epoch: 874, loss: 0.006\n",
      "model: 4, epoch: 875, loss: 0.006\n",
      "model: 4, epoch: 876, loss: 0.006\n",
      "model: 4, epoch: 877, loss: 0.006\n",
      "model: 4, epoch: 878, loss: 0.006\n",
      "model: 4, epoch: 879, loss: 0.006\n",
      "model: 4, epoch: 880, loss: 0.006\n",
      "model: 4, epoch: 881, loss: 0.006\n",
      "model: 4, epoch: 882, loss: 0.006\n",
      "model: 4, epoch: 883, loss: 0.006\n",
      "model: 4, epoch: 884, loss: 0.006\n",
      "model: 4, epoch: 885, loss: 0.006\n",
      "model: 4, epoch: 886, loss: 0.006\n",
      "model: 4, epoch: 887, loss: 0.006\n",
      "model: 4, epoch: 888, loss: 0.006\n",
      "model: 4, epoch: 889, loss: 0.006\n",
      "model: 4, epoch: 890, loss: 0.006\n",
      "model: 4, epoch: 891, loss: 0.006\n",
      "model: 4, epoch: 892, loss: 0.006\n",
      "model: 4, epoch: 893, loss: 0.006\n",
      "model: 4, epoch: 894, loss: 0.006\n",
      "model: 4, epoch: 895, loss: 0.006\n",
      "model: 4, epoch: 896, loss: 0.006\n",
      "model: 4, epoch: 897, loss: 0.006\n",
      "model: 4, epoch: 898, loss: 0.006\n",
      "model: 4, epoch: 899, loss: 0.006\n",
      "model: 4, epoch: 900, loss: 0.006\n",
      "model: 4, epoch: 901, loss: 0.006\n",
      "model: 4, epoch: 902, loss: 0.006\n",
      "model: 4, epoch: 903, loss: 0.006\n",
      "model: 4, epoch: 904, loss: 0.006\n",
      "model: 4, epoch: 905, loss: 0.006\n",
      "model: 4, epoch: 906, loss: 0.006\n",
      "model: 4, epoch: 907, loss: 0.006\n",
      "model: 4, epoch: 908, loss: 0.006\n",
      "model: 4, epoch: 909, loss: 0.006\n",
      "model: 4, epoch: 910, loss: 0.006\n",
      "model: 4, epoch: 911, loss: 0.006\n",
      "model: 4, epoch: 912, loss: 0.006\n",
      "model: 4, epoch: 913, loss: 0.006\n",
      "model: 4, epoch: 914, loss: 0.006\n",
      "model: 4, epoch: 915, loss: 0.006\n",
      "model: 4, epoch: 916, loss: 0.006\n",
      "model: 4, epoch: 917, loss: 0.006\n",
      "model: 4, epoch: 918, loss: 0.006\n",
      "model: 4, epoch: 919, loss: 0.006\n",
      "model: 4, epoch: 920, loss: 0.006\n",
      "model: 4, epoch: 921, loss: 0.006\n",
      "model: 4, epoch: 922, loss: 0.006\n",
      "model: 4, epoch: 923, loss: 0.006\n",
      "model: 4, epoch: 924, loss: 0.006\n",
      "model: 4, epoch: 925, loss: 0.006\n",
      "model: 4, epoch: 926, loss: 0.006\n",
      "model: 4, epoch: 927, loss: 0.006\n",
      "model: 4, epoch: 928, loss: 0.006\n",
      "model: 4, epoch: 929, loss: 0.006\n",
      "model: 4, epoch: 930, loss: 0.006\n",
      "model: 4, epoch: 931, loss: 0.006\n",
      "model: 4, epoch: 932, loss: 0.006\n",
      "model: 4, epoch: 933, loss: 0.006\n",
      "model: 4, epoch: 934, loss: 0.006\n",
      "model: 4, epoch: 935, loss: 0.006\n",
      "model: 4, epoch: 936, loss: 0.006\n",
      "model: 4, epoch: 937, loss: 0.006\n",
      "model: 4, epoch: 938, loss: 0.006\n",
      "model: 4, epoch: 939, loss: 0.006\n",
      "model: 4, epoch: 940, loss: 0.006\n",
      "model: 4, epoch: 941, loss: 0.006\n",
      "model: 4, epoch: 942, loss: 0.006\n",
      "model: 4, epoch: 943, loss: 0.006\n",
      "model: 4, epoch: 944, loss: 0.006\n",
      "model: 4, epoch: 945, loss: 0.006\n",
      "model: 4, epoch: 946, loss: 0.006\n",
      "model: 4, epoch: 947, loss: 0.006\n",
      "model: 4, epoch: 948, loss: 0.006\n",
      "model: 4, epoch: 949, loss: 0.006\n",
      "model: 4, epoch: 950, loss: 0.006\n",
      "model: 4, epoch: 951, loss: 0.006\n",
      "model: 4, epoch: 952, loss: 0.006\n",
      "model: 4, epoch: 953, loss: 0.006\n",
      "model: 4, epoch: 954, loss: 0.006\n",
      "model: 4, epoch: 955, loss: 0.006\n",
      "model: 4, epoch: 956, loss: 0.006\n",
      "model: 4, epoch: 957, loss: 0.006\n",
      "model: 4, epoch: 958, loss: 0.006\n",
      "model: 4, epoch: 959, loss: 0.006\n",
      "model: 4, epoch: 960, loss: 0.006\n",
      "model: 4, epoch: 961, loss: 0.006\n",
      "model: 4, epoch: 962, loss: 0.006\n",
      "model: 4, epoch: 963, loss: 0.006\n",
      "model: 4, epoch: 964, loss: 0.006\n",
      "model: 4, epoch: 965, loss: 0.006\n",
      "model: 4, epoch: 966, loss: 0.006\n",
      "model: 4, epoch: 967, loss: 0.006\n",
      "model: 4, epoch: 968, loss: 0.006\n",
      "model: 4, epoch: 969, loss: 0.006\n",
      "model: 4, epoch: 970, loss: 0.006\n",
      "model: 4, epoch: 971, loss: 0.006\n",
      "model: 4, epoch: 972, loss: 0.006\n",
      "model: 4, epoch: 973, loss: 0.006\n",
      "model: 4, epoch: 974, loss: 0.006\n",
      "model: 4, epoch: 975, loss: 0.006\n",
      "model: 4, epoch: 976, loss: 0.006\n",
      "model: 4, epoch: 977, loss: 0.006\n",
      "model: 4, epoch: 978, loss: 0.006\n",
      "model: 4, epoch: 979, loss: 0.006\n",
      "model: 4, epoch: 980, loss: 0.006\n",
      "model: 4, epoch: 981, loss: 0.006\n",
      "model: 4, epoch: 982, loss: 0.006\n",
      "model: 4, epoch: 983, loss: 0.006\n",
      "model: 4, epoch: 984, loss: 0.006\n",
      "model: 4, epoch: 985, loss: 0.006\n",
      "model: 4, epoch: 986, loss: 0.006\n",
      "model: 4, epoch: 987, loss: 0.006\n",
      "model: 4, epoch: 988, loss: 0.006\n",
      "model: 4, epoch: 989, loss: 0.006\n",
      "model: 4, epoch: 990, loss: 0.006\n",
      "model: 4, epoch: 991, loss: 0.006\n",
      "model: 4, epoch: 992, loss: 0.006\n",
      "model: 4, epoch: 993, loss: 0.006\n",
      "model: 4, epoch: 994, loss: 0.006\n",
      "model: 4, epoch: 995, loss: 0.006\n",
      "model: 4, epoch: 996, loss: 0.006\n",
      "model: 4, epoch: 997, loss: 0.006\n",
      "model: 4, epoch: 998, loss: 0.006\n",
      "model: 4, epoch: 999, loss: 0.006\n",
      "model: 5, epoch: 0, loss: 0.098\n",
      "model: 5, epoch: 1, loss: 0.087\n",
      "model: 5, epoch: 2, loss: 0.078\n",
      "model: 5, epoch: 3, loss: 0.070\n",
      "model: 5, epoch: 4, loss: 0.063\n",
      "model: 5, epoch: 5, loss: 0.056\n",
      "model: 5, epoch: 6, loss: 0.050\n",
      "model: 5, epoch: 7, loss: 0.045\n",
      "model: 5, epoch: 8, loss: 0.041\n",
      "model: 5, epoch: 9, loss: 0.038\n",
      "model: 5, epoch: 10, loss: 0.035\n",
      "model: 5, epoch: 11, loss: 0.033\n",
      "model: 5, epoch: 12, loss: 0.031\n",
      "model: 5, epoch: 13, loss: 0.031\n",
      "model: 5, epoch: 14, loss: 0.030\n",
      "model: 5, epoch: 15, loss: 0.030\n",
      "model: 5, epoch: 16, loss: 0.030\n",
      "model: 5, epoch: 17, loss: 0.030\n",
      "model: 5, epoch: 18, loss: 0.031\n",
      "model: 5, epoch: 19, loss: 0.031\n",
      "model: 5, epoch: 20, loss: 0.032\n",
      "model: 5, epoch: 21, loss: 0.032\n",
      "model: 5, epoch: 22, loss: 0.033\n",
      "model: 5, epoch: 23, loss: 0.033\n",
      "model: 5, epoch: 24, loss: 0.033\n",
      "model: 5, epoch: 25, loss: 0.033\n",
      "model: 5, epoch: 26, loss: 0.033\n",
      "model: 5, epoch: 27, loss: 0.032\n",
      "model: 5, epoch: 28, loss: 0.032\n",
      "model: 5, epoch: 29, loss: 0.032\n",
      "model: 5, epoch: 30, loss: 0.031\n",
      "model: 5, epoch: 31, loss: 0.031\n",
      "model: 5, epoch: 32, loss: 0.031\n",
      "model: 5, epoch: 33, loss: 0.030\n",
      "model: 5, epoch: 34, loss: 0.030\n",
      "model: 5, epoch: 35, loss: 0.030\n",
      "model: 5, epoch: 36, loss: 0.030\n",
      "model: 5, epoch: 37, loss: 0.030\n",
      "model: 5, epoch: 38, loss: 0.030\n",
      "model: 5, epoch: 39, loss: 0.030\n",
      "model: 5, epoch: 40, loss: 0.030\n",
      "model: 5, epoch: 41, loss: 0.030\n",
      "model: 5, epoch: 42, loss: 0.030\n",
      "model: 5, epoch: 43, loss: 0.030\n",
      "model: 5, epoch: 44, loss: 0.030\n",
      "model: 5, epoch: 45, loss: 0.030\n",
      "model: 5, epoch: 46, loss: 0.030\n",
      "model: 5, epoch: 47, loss: 0.030\n",
      "model: 5, epoch: 48, loss: 0.030\n",
      "model: 5, epoch: 49, loss: 0.030\n",
      "model: 5, epoch: 50, loss: 0.030\n",
      "model: 5, epoch: 51, loss: 0.030\n",
      "model: 5, epoch: 52, loss: 0.030\n",
      "model: 5, epoch: 53, loss: 0.030\n",
      "model: 5, epoch: 54, loss: 0.030\n",
      "model: 5, epoch: 55, loss: 0.030\n",
      "model: 5, epoch: 56, loss: 0.029\n",
      "model: 5, epoch: 57, loss: 0.029\n",
      "model: 5, epoch: 58, loss: 0.029\n",
      "model: 5, epoch: 59, loss: 0.029\n",
      "model: 5, epoch: 60, loss: 0.029\n",
      "model: 5, epoch: 61, loss: 0.029\n",
      "model: 5, epoch: 62, loss: 0.029\n",
      "model: 5, epoch: 63, loss: 0.029\n",
      "model: 5, epoch: 64, loss: 0.029\n",
      "model: 5, epoch: 65, loss: 0.029\n",
      "model: 5, epoch: 66, loss: 0.029\n",
      "model: 5, epoch: 67, loss: 0.029\n",
      "model: 5, epoch: 68, loss: 0.029\n",
      "model: 5, epoch: 69, loss: 0.029\n",
      "model: 5, epoch: 70, loss: 0.029\n",
      "model: 5, epoch: 71, loss: 0.029\n",
      "model: 5, epoch: 72, loss: 0.029\n",
      "model: 5, epoch: 73, loss: 0.029\n",
      "model: 5, epoch: 74, loss: 0.029\n",
      "model: 5, epoch: 75, loss: 0.029\n",
      "model: 5, epoch: 76, loss: 0.029\n",
      "model: 5, epoch: 77, loss: 0.029\n",
      "model: 5, epoch: 78, loss: 0.029\n",
      "model: 5, epoch: 79, loss: 0.029\n",
      "model: 5, epoch: 80, loss: 0.029\n",
      "model: 5, epoch: 81, loss: 0.029\n",
      "model: 5, epoch: 82, loss: 0.029\n",
      "model: 5, epoch: 83, loss: 0.029\n",
      "model: 5, epoch: 84, loss: 0.029\n",
      "model: 5, epoch: 85, loss: 0.029\n",
      "model: 5, epoch: 86, loss: 0.029\n",
      "model: 5, epoch: 87, loss: 0.029\n",
      "model: 5, epoch: 88, loss: 0.029\n",
      "model: 5, epoch: 89, loss: 0.029\n",
      "model: 5, epoch: 90, loss: 0.029\n",
      "model: 5, epoch: 91, loss: 0.029\n",
      "model: 5, epoch: 92, loss: 0.029\n",
      "model: 5, epoch: 93, loss: 0.029\n",
      "model: 5, epoch: 94, loss: 0.029\n",
      "model: 5, epoch: 95, loss: 0.029\n",
      "model: 5, epoch: 96, loss: 0.029\n",
      "model: 5, epoch: 97, loss: 0.029\n",
      "model: 5, epoch: 98, loss: 0.029\n",
      "model: 5, epoch: 99, loss: 0.029\n",
      "model: 5, epoch: 100, loss: 0.029\n",
      "model: 5, epoch: 101, loss: 0.029\n",
      "model: 5, epoch: 102, loss: 0.029\n",
      "model: 5, epoch: 103, loss: 0.029\n",
      "model: 5, epoch: 104, loss: 0.029\n",
      "model: 5, epoch: 105, loss: 0.029\n",
      "model: 5, epoch: 106, loss: 0.029\n",
      "model: 5, epoch: 107, loss: 0.029\n",
      "model: 5, epoch: 108, loss: 0.029\n",
      "model: 5, epoch: 109, loss: 0.029\n",
      "model: 5, epoch: 110, loss: 0.029\n",
      "model: 5, epoch: 111, loss: 0.029\n",
      "model: 5, epoch: 112, loss: 0.029\n",
      "model: 5, epoch: 113, loss: 0.029\n",
      "model: 5, epoch: 114, loss: 0.029\n",
      "model: 5, epoch: 115, loss: 0.029\n",
      "model: 5, epoch: 116, loss: 0.029\n",
      "model: 5, epoch: 117, loss: 0.029\n",
      "model: 5, epoch: 118, loss: 0.029\n",
      "model: 5, epoch: 119, loss: 0.029\n",
      "model: 5, epoch: 120, loss: 0.029\n",
      "model: 5, epoch: 121, loss: 0.029\n",
      "model: 5, epoch: 122, loss: 0.029\n",
      "model: 5, epoch: 123, loss: 0.029\n",
      "model: 5, epoch: 124, loss: 0.029\n",
      "model: 5, epoch: 125, loss: 0.029\n",
      "model: 5, epoch: 126, loss: 0.029\n",
      "model: 5, epoch: 127, loss: 0.029\n",
      "model: 5, epoch: 128, loss: 0.029\n",
      "model: 5, epoch: 129, loss: 0.029\n",
      "model: 5, epoch: 130, loss: 0.029\n",
      "model: 5, epoch: 131, loss: 0.028\n",
      "model: 5, epoch: 132, loss: 0.028\n",
      "model: 5, epoch: 133, loss: 0.028\n",
      "model: 5, epoch: 134, loss: 0.028\n",
      "model: 5, epoch: 135, loss: 0.028\n",
      "model: 5, epoch: 136, loss: 0.028\n",
      "model: 5, epoch: 137, loss: 0.028\n",
      "model: 5, epoch: 138, loss: 0.028\n",
      "model: 5, epoch: 139, loss: 0.028\n",
      "model: 5, epoch: 140, loss: 0.028\n",
      "model: 5, epoch: 141, loss: 0.028\n",
      "model: 5, epoch: 142, loss: 0.028\n",
      "model: 5, epoch: 143, loss: 0.028\n",
      "model: 5, epoch: 144, loss: 0.028\n",
      "model: 5, epoch: 145, loss: 0.028\n",
      "model: 5, epoch: 146, loss: 0.028\n",
      "model: 5, epoch: 147, loss: 0.028\n",
      "model: 5, epoch: 148, loss: 0.028\n",
      "model: 5, epoch: 149, loss: 0.028\n",
      "model: 5, epoch: 150, loss: 0.028\n",
      "model: 5, epoch: 151, loss: 0.028\n",
      "model: 5, epoch: 152, loss: 0.028\n",
      "model: 5, epoch: 153, loss: 0.028\n",
      "model: 5, epoch: 154, loss: 0.028\n",
      "model: 5, epoch: 155, loss: 0.028\n",
      "model: 5, epoch: 156, loss: 0.028\n",
      "model: 5, epoch: 157, loss: 0.028\n",
      "model: 5, epoch: 158, loss: 0.028\n",
      "model: 5, epoch: 159, loss: 0.028\n",
      "model: 5, epoch: 160, loss: 0.028\n",
      "model: 5, epoch: 161, loss: 0.028\n",
      "model: 5, epoch: 162, loss: 0.028\n",
      "model: 5, epoch: 163, loss: 0.028\n",
      "model: 5, epoch: 164, loss: 0.028\n",
      "model: 5, epoch: 165, loss: 0.028\n",
      "model: 5, epoch: 166, loss: 0.028\n",
      "model: 5, epoch: 167, loss: 0.028\n",
      "model: 5, epoch: 168, loss: 0.028\n",
      "model: 5, epoch: 169, loss: 0.028\n",
      "model: 5, epoch: 170, loss: 0.028\n",
      "model: 5, epoch: 171, loss: 0.028\n",
      "model: 5, epoch: 172, loss: 0.028\n",
      "model: 5, epoch: 173, loss: 0.028\n",
      "model: 5, epoch: 174, loss: 0.028\n",
      "model: 5, epoch: 175, loss: 0.028\n",
      "model: 5, epoch: 176, loss: 0.028\n",
      "model: 5, epoch: 177, loss: 0.028\n",
      "model: 5, epoch: 178, loss: 0.028\n",
      "model: 5, epoch: 179, loss: 0.028\n",
      "model: 5, epoch: 180, loss: 0.028\n",
      "model: 5, epoch: 181, loss: 0.028\n",
      "model: 5, epoch: 182, loss: 0.028\n",
      "model: 5, epoch: 183, loss: 0.028\n",
      "model: 5, epoch: 184, loss: 0.028\n",
      "model: 5, epoch: 185, loss: 0.028\n",
      "model: 5, epoch: 186, loss: 0.028\n",
      "model: 5, epoch: 187, loss: 0.028\n",
      "model: 5, epoch: 188, loss: 0.028\n",
      "model: 5, epoch: 189, loss: 0.028\n",
      "model: 5, epoch: 190, loss: 0.028\n",
      "model: 5, epoch: 191, loss: 0.028\n",
      "model: 5, epoch: 192, loss: 0.027\n",
      "model: 5, epoch: 193, loss: 0.027\n",
      "model: 5, epoch: 194, loss: 0.027\n",
      "model: 5, epoch: 195, loss: 0.027\n",
      "model: 5, epoch: 196, loss: 0.027\n",
      "model: 5, epoch: 197, loss: 0.027\n",
      "model: 5, epoch: 198, loss: 0.027\n",
      "model: 5, epoch: 199, loss: 0.027\n",
      "model: 5, epoch: 200, loss: 0.027\n",
      "model: 5, epoch: 201, loss: 0.027\n",
      "model: 5, epoch: 202, loss: 0.027\n",
      "model: 5, epoch: 203, loss: 0.027\n",
      "model: 5, epoch: 204, loss: 0.027\n",
      "model: 5, epoch: 205, loss: 0.027\n",
      "model: 5, epoch: 206, loss: 0.027\n",
      "model: 5, epoch: 207, loss: 0.027\n",
      "model: 5, epoch: 208, loss: 0.027\n",
      "model: 5, epoch: 209, loss: 0.027\n",
      "model: 5, epoch: 210, loss: 0.027\n",
      "model: 5, epoch: 211, loss: 0.027\n",
      "model: 5, epoch: 212, loss: 0.027\n",
      "model: 5, epoch: 213, loss: 0.027\n",
      "model: 5, epoch: 214, loss: 0.027\n",
      "model: 5, epoch: 215, loss: 0.027\n",
      "model: 5, epoch: 216, loss: 0.027\n",
      "model: 5, epoch: 217, loss: 0.027\n",
      "model: 5, epoch: 218, loss: 0.027\n",
      "model: 5, epoch: 219, loss: 0.027\n",
      "model: 5, epoch: 220, loss: 0.027\n",
      "model: 5, epoch: 221, loss: 0.027\n",
      "model: 5, epoch: 222, loss: 0.027\n",
      "model: 5, epoch: 223, loss: 0.027\n",
      "model: 5, epoch: 224, loss: 0.027\n",
      "model: 5, epoch: 225, loss: 0.027\n",
      "model: 5, epoch: 226, loss: 0.027\n",
      "model: 5, epoch: 227, loss: 0.027\n",
      "model: 5, epoch: 228, loss: 0.027\n",
      "model: 5, epoch: 229, loss: 0.027\n",
      "model: 5, epoch: 230, loss: 0.027\n",
      "model: 5, epoch: 231, loss: 0.027\n",
      "model: 5, epoch: 232, loss: 0.027\n",
      "model: 5, epoch: 233, loss: 0.027\n",
      "model: 5, epoch: 234, loss: 0.027\n",
      "model: 5, epoch: 235, loss: 0.027\n",
      "model: 5, epoch: 236, loss: 0.027\n",
      "model: 5, epoch: 237, loss: 0.027\n",
      "model: 5, epoch: 238, loss: 0.027\n",
      "model: 5, epoch: 239, loss: 0.027\n",
      "model: 5, epoch: 240, loss: 0.027\n",
      "model: 5, epoch: 241, loss: 0.027\n",
      "model: 5, epoch: 242, loss: 0.027\n",
      "model: 5, epoch: 243, loss: 0.026\n",
      "model: 5, epoch: 244, loss: 0.026\n",
      "model: 5, epoch: 245, loss: 0.026\n",
      "model: 5, epoch: 246, loss: 0.026\n",
      "model: 5, epoch: 247, loss: 0.026\n",
      "model: 5, epoch: 248, loss: 0.026\n",
      "model: 5, epoch: 249, loss: 0.026\n",
      "model: 5, epoch: 250, loss: 0.026\n",
      "model: 5, epoch: 251, loss: 0.026\n",
      "model: 5, epoch: 252, loss: 0.026\n",
      "model: 5, epoch: 253, loss: 0.026\n",
      "model: 5, epoch: 254, loss: 0.026\n",
      "model: 5, epoch: 255, loss: 0.026\n",
      "model: 5, epoch: 256, loss: 0.026\n",
      "model: 5, epoch: 257, loss: 0.026\n",
      "model: 5, epoch: 258, loss: 0.026\n",
      "model: 5, epoch: 259, loss: 0.026\n",
      "model: 5, epoch: 260, loss: 0.026\n",
      "model: 5, epoch: 261, loss: 0.026\n",
      "model: 5, epoch: 262, loss: 0.026\n",
      "model: 5, epoch: 263, loss: 0.026\n",
      "model: 5, epoch: 264, loss: 0.026\n",
      "model: 5, epoch: 265, loss: 0.026\n",
      "model: 5, epoch: 266, loss: 0.026\n",
      "model: 5, epoch: 267, loss: 0.026\n",
      "model: 5, epoch: 268, loss: 0.026\n",
      "model: 5, epoch: 269, loss: 0.026\n",
      "model: 5, epoch: 270, loss: 0.026\n",
      "model: 5, epoch: 271, loss: 0.026\n",
      "model: 5, epoch: 272, loss: 0.026\n",
      "model: 5, epoch: 273, loss: 0.026\n",
      "model: 5, epoch: 274, loss: 0.026\n",
      "model: 5, epoch: 275, loss: 0.026\n",
      "model: 5, epoch: 276, loss: 0.026\n",
      "model: 5, epoch: 277, loss: 0.026\n",
      "model: 5, epoch: 278, loss: 0.026\n",
      "model: 5, epoch: 279, loss: 0.026\n",
      "model: 5, epoch: 280, loss: 0.026\n",
      "model: 5, epoch: 281, loss: 0.026\n",
      "model: 5, epoch: 282, loss: 0.026\n",
      "model: 5, epoch: 283, loss: 0.026\n",
      "model: 5, epoch: 284, loss: 0.026\n",
      "model: 5, epoch: 285, loss: 0.026\n",
      "model: 5, epoch: 286, loss: 0.025\n",
      "model: 5, epoch: 287, loss: 0.025\n",
      "model: 5, epoch: 288, loss: 0.025\n",
      "model: 5, epoch: 289, loss: 0.025\n",
      "model: 5, epoch: 290, loss: 0.025\n",
      "model: 5, epoch: 291, loss: 0.025\n",
      "model: 5, epoch: 292, loss: 0.025\n",
      "model: 5, epoch: 293, loss: 0.025\n",
      "model: 5, epoch: 294, loss: 0.025\n",
      "model: 5, epoch: 295, loss: 0.025\n",
      "model: 5, epoch: 296, loss: 0.025\n",
      "model: 5, epoch: 297, loss: 0.025\n",
      "model: 5, epoch: 298, loss: 0.025\n",
      "model: 5, epoch: 299, loss: 0.025\n",
      "model: 5, epoch: 300, loss: 0.025\n",
      "model: 5, epoch: 301, loss: 0.025\n",
      "model: 5, epoch: 302, loss: 0.025\n",
      "model: 5, epoch: 303, loss: 0.025\n",
      "model: 5, epoch: 304, loss: 0.025\n",
      "model: 5, epoch: 305, loss: 0.025\n",
      "model: 5, epoch: 306, loss: 0.025\n",
      "model: 5, epoch: 307, loss: 0.025\n",
      "model: 5, epoch: 308, loss: 0.025\n",
      "model: 5, epoch: 309, loss: 0.025\n",
      "model: 5, epoch: 310, loss: 0.025\n",
      "model: 5, epoch: 311, loss: 0.025\n",
      "model: 5, epoch: 312, loss: 0.025\n",
      "model: 5, epoch: 313, loss: 0.025\n",
      "model: 5, epoch: 314, loss: 0.025\n",
      "model: 5, epoch: 315, loss: 0.025\n",
      "model: 5, epoch: 316, loss: 0.025\n",
      "model: 5, epoch: 317, loss: 0.025\n",
      "model: 5, epoch: 318, loss: 0.025\n",
      "model: 5, epoch: 319, loss: 0.025\n",
      "model: 5, epoch: 320, loss: 0.025\n",
      "model: 5, epoch: 321, loss: 0.025\n",
      "model: 5, epoch: 322, loss: 0.024\n",
      "model: 5, epoch: 323, loss: 0.024\n",
      "model: 5, epoch: 324, loss: 0.024\n",
      "model: 5, epoch: 325, loss: 0.024\n",
      "model: 5, epoch: 326, loss: 0.024\n",
      "model: 5, epoch: 327, loss: 0.024\n",
      "model: 5, epoch: 328, loss: 0.024\n",
      "model: 5, epoch: 329, loss: 0.024\n",
      "model: 5, epoch: 330, loss: 0.024\n",
      "model: 5, epoch: 331, loss: 0.024\n",
      "model: 5, epoch: 332, loss: 0.024\n",
      "model: 5, epoch: 333, loss: 0.024\n",
      "model: 5, epoch: 334, loss: 0.024\n",
      "model: 5, epoch: 335, loss: 0.024\n",
      "model: 5, epoch: 336, loss: 0.024\n",
      "model: 5, epoch: 337, loss: 0.024\n",
      "model: 5, epoch: 338, loss: 0.024\n",
      "model: 5, epoch: 339, loss: 0.024\n",
      "model: 5, epoch: 340, loss: 0.024\n",
      "model: 5, epoch: 341, loss: 0.024\n",
      "model: 5, epoch: 342, loss: 0.024\n",
      "model: 5, epoch: 343, loss: 0.024\n",
      "model: 5, epoch: 344, loss: 0.024\n",
      "model: 5, epoch: 345, loss: 0.024\n",
      "model: 5, epoch: 346, loss: 0.024\n",
      "model: 5, epoch: 347, loss: 0.024\n",
      "model: 5, epoch: 348, loss: 0.024\n",
      "model: 5, epoch: 349, loss: 0.024\n",
      "model: 5, epoch: 350, loss: 0.024\n",
      "model: 5, epoch: 351, loss: 0.024\n",
      "model: 5, epoch: 352, loss: 0.024\n",
      "model: 5, epoch: 353, loss: 0.023\n",
      "model: 5, epoch: 354, loss: 0.023\n",
      "model: 5, epoch: 355, loss: 0.023\n",
      "model: 5, epoch: 356, loss: 0.023\n",
      "model: 5, epoch: 357, loss: 0.023\n",
      "model: 5, epoch: 358, loss: 0.023\n",
      "model: 5, epoch: 359, loss: 0.023\n",
      "model: 5, epoch: 360, loss: 0.023\n",
      "model: 5, epoch: 361, loss: 0.023\n",
      "model: 5, epoch: 362, loss: 0.023\n",
      "model: 5, epoch: 363, loss: 0.023\n",
      "model: 5, epoch: 364, loss: 0.023\n",
      "model: 5, epoch: 365, loss: 0.023\n",
      "model: 5, epoch: 366, loss: 0.023\n",
      "model: 5, epoch: 367, loss: 0.023\n",
      "model: 5, epoch: 368, loss: 0.023\n",
      "model: 5, epoch: 369, loss: 0.023\n",
      "model: 5, epoch: 370, loss: 0.023\n",
      "model: 5, epoch: 371, loss: 0.023\n",
      "model: 5, epoch: 372, loss: 0.023\n",
      "model: 5, epoch: 373, loss: 0.023\n",
      "model: 5, epoch: 374, loss: 0.023\n",
      "model: 5, epoch: 375, loss: 0.023\n",
      "model: 5, epoch: 376, loss: 0.023\n",
      "model: 5, epoch: 377, loss: 0.023\n",
      "model: 5, epoch: 378, loss: 0.023\n",
      "model: 5, epoch: 379, loss: 0.023\n",
      "model: 5, epoch: 380, loss: 0.023\n",
      "model: 5, epoch: 381, loss: 0.022\n",
      "model: 5, epoch: 382, loss: 0.022\n",
      "model: 5, epoch: 383, loss: 0.022\n",
      "model: 5, epoch: 384, loss: 0.022\n",
      "model: 5, epoch: 385, loss: 0.022\n",
      "model: 5, epoch: 386, loss: 0.022\n",
      "model: 5, epoch: 387, loss: 0.022\n",
      "model: 5, epoch: 388, loss: 0.022\n",
      "model: 5, epoch: 389, loss: 0.022\n",
      "model: 5, epoch: 390, loss: 0.022\n",
      "model: 5, epoch: 391, loss: 0.022\n",
      "model: 5, epoch: 392, loss: 0.022\n",
      "model: 5, epoch: 393, loss: 0.022\n",
      "model: 5, epoch: 394, loss: 0.022\n",
      "model: 5, epoch: 395, loss: 0.022\n",
      "model: 5, epoch: 396, loss: 0.022\n",
      "model: 5, epoch: 397, loss: 0.022\n",
      "model: 5, epoch: 398, loss: 0.022\n",
      "model: 5, epoch: 399, loss: 0.022\n",
      "model: 5, epoch: 400, loss: 0.022\n",
      "model: 5, epoch: 401, loss: 0.022\n",
      "model: 5, epoch: 402, loss: 0.022\n",
      "model: 5, epoch: 403, loss: 0.022\n",
      "model: 5, epoch: 404, loss: 0.022\n",
      "model: 5, epoch: 405, loss: 0.022\n",
      "model: 5, epoch: 406, loss: 0.021\n",
      "model: 5, epoch: 407, loss: 0.021\n",
      "model: 5, epoch: 408, loss: 0.021\n",
      "model: 5, epoch: 409, loss: 0.021\n",
      "model: 5, epoch: 410, loss: 0.021\n",
      "model: 5, epoch: 411, loss: 0.021\n",
      "model: 5, epoch: 412, loss: 0.021\n",
      "model: 5, epoch: 413, loss: 0.021\n",
      "model: 5, epoch: 414, loss: 0.021\n",
      "model: 5, epoch: 415, loss: 0.021\n",
      "model: 5, epoch: 416, loss: 0.021\n",
      "model: 5, epoch: 417, loss: 0.021\n",
      "model: 5, epoch: 418, loss: 0.021\n",
      "model: 5, epoch: 419, loss: 0.021\n",
      "model: 5, epoch: 420, loss: 0.021\n",
      "model: 5, epoch: 421, loss: 0.021\n",
      "model: 5, epoch: 422, loss: 0.021\n",
      "model: 5, epoch: 423, loss: 0.021\n",
      "model: 5, epoch: 424, loss: 0.021\n",
      "model: 5, epoch: 425, loss: 0.021\n",
      "model: 5, epoch: 426, loss: 0.021\n",
      "model: 5, epoch: 427, loss: 0.021\n",
      "model: 5, epoch: 428, loss: 0.021\n",
      "model: 5, epoch: 429, loss: 0.020\n",
      "model: 5, epoch: 430, loss: 0.020\n",
      "model: 5, epoch: 431, loss: 0.020\n",
      "model: 5, epoch: 432, loss: 0.020\n",
      "model: 5, epoch: 433, loss: 0.020\n",
      "model: 5, epoch: 434, loss: 0.020\n",
      "model: 5, epoch: 435, loss: 0.020\n",
      "model: 5, epoch: 436, loss: 0.020\n",
      "model: 5, epoch: 437, loss: 0.020\n",
      "model: 5, epoch: 438, loss: 0.020\n",
      "model: 5, epoch: 439, loss: 0.020\n",
      "model: 5, epoch: 440, loss: 0.020\n",
      "model: 5, epoch: 441, loss: 0.020\n",
      "model: 5, epoch: 442, loss: 0.020\n",
      "model: 5, epoch: 443, loss: 0.020\n",
      "model: 5, epoch: 444, loss: 0.020\n",
      "model: 5, epoch: 445, loss: 0.020\n",
      "model: 5, epoch: 446, loss: 0.020\n",
      "model: 5, epoch: 447, loss: 0.020\n",
      "model: 5, epoch: 448, loss: 0.020\n",
      "model: 5, epoch: 449, loss: 0.020\n",
      "model: 5, epoch: 450, loss: 0.020\n",
      "model: 5, epoch: 451, loss: 0.019\n",
      "model: 5, epoch: 452, loss: 0.019\n",
      "model: 5, epoch: 453, loss: 0.019\n",
      "model: 5, epoch: 454, loss: 0.019\n",
      "model: 5, epoch: 455, loss: 0.019\n",
      "model: 5, epoch: 456, loss: 0.019\n",
      "model: 5, epoch: 457, loss: 0.019\n",
      "model: 5, epoch: 458, loss: 0.019\n",
      "model: 5, epoch: 459, loss: 0.019\n",
      "model: 5, epoch: 460, loss: 0.019\n",
      "model: 5, epoch: 461, loss: 0.019\n",
      "model: 5, epoch: 462, loss: 0.019\n",
      "model: 5, epoch: 463, loss: 0.019\n",
      "model: 5, epoch: 464, loss: 0.019\n",
      "model: 5, epoch: 465, loss: 0.019\n",
      "model: 5, epoch: 466, loss: 0.019\n",
      "model: 5, epoch: 467, loss: 0.019\n",
      "model: 5, epoch: 468, loss: 0.019\n",
      "model: 5, epoch: 469, loss: 0.019\n",
      "model: 5, epoch: 470, loss: 0.019\n",
      "model: 5, epoch: 471, loss: 0.018\n",
      "model: 5, epoch: 472, loss: 0.018\n",
      "model: 5, epoch: 473, loss: 0.018\n",
      "model: 5, epoch: 474, loss: 0.018\n",
      "model: 5, epoch: 475, loss: 0.018\n",
      "model: 5, epoch: 476, loss: 0.018\n",
      "model: 5, epoch: 477, loss: 0.018\n",
      "model: 5, epoch: 478, loss: 0.018\n",
      "model: 5, epoch: 479, loss: 0.018\n",
      "model: 5, epoch: 480, loss: 0.018\n",
      "model: 5, epoch: 481, loss: 0.018\n",
      "model: 5, epoch: 482, loss: 0.018\n",
      "model: 5, epoch: 483, loss: 0.018\n",
      "model: 5, epoch: 484, loss: 0.018\n",
      "model: 5, epoch: 485, loss: 0.018\n",
      "model: 5, epoch: 486, loss: 0.018\n",
      "model: 5, epoch: 487, loss: 0.018\n",
      "model: 5, epoch: 488, loss: 0.018\n",
      "model: 5, epoch: 489, loss: 0.018\n",
      "model: 5, epoch: 490, loss: 0.017\n",
      "model: 5, epoch: 491, loss: 0.017\n",
      "model: 5, epoch: 492, loss: 0.017\n",
      "model: 5, epoch: 493, loss: 0.017\n",
      "model: 5, epoch: 494, loss: 0.017\n",
      "model: 5, epoch: 495, loss: 0.017\n",
      "model: 5, epoch: 496, loss: 0.017\n",
      "model: 5, epoch: 497, loss: 0.017\n",
      "model: 5, epoch: 498, loss: 0.017\n",
      "model: 5, epoch: 499, loss: 0.017\n",
      "model: 5, epoch: 500, loss: 0.017\n",
      "model: 5, epoch: 501, loss: 0.017\n",
      "model: 5, epoch: 502, loss: 0.017\n",
      "model: 5, epoch: 503, loss: 0.017\n",
      "model: 5, epoch: 504, loss: 0.017\n",
      "model: 5, epoch: 505, loss: 0.017\n",
      "model: 5, epoch: 506, loss: 0.017\n",
      "model: 5, epoch: 507, loss: 0.017\n",
      "model: 5, epoch: 508, loss: 0.016\n",
      "model: 5, epoch: 509, loss: 0.016\n",
      "model: 5, epoch: 510, loss: 0.016\n",
      "model: 5, epoch: 511, loss: 0.016\n",
      "model: 5, epoch: 512, loss: 0.016\n",
      "model: 5, epoch: 513, loss: 0.016\n",
      "model: 5, epoch: 514, loss: 0.016\n",
      "model: 5, epoch: 515, loss: 0.016\n",
      "model: 5, epoch: 516, loss: 0.016\n",
      "model: 5, epoch: 517, loss: 0.016\n",
      "model: 5, epoch: 518, loss: 0.016\n",
      "model: 5, epoch: 519, loss: 0.016\n",
      "model: 5, epoch: 520, loss: 0.016\n",
      "model: 5, epoch: 521, loss: 0.016\n",
      "model: 5, epoch: 522, loss: 0.016\n",
      "model: 5, epoch: 523, loss: 0.016\n",
      "model: 5, epoch: 524, loss: 0.016\n",
      "model: 5, epoch: 525, loss: 0.016\n",
      "model: 5, epoch: 526, loss: 0.016\n",
      "model: 5, epoch: 527, loss: 0.015\n",
      "model: 5, epoch: 528, loss: 0.015\n",
      "model: 5, epoch: 529, loss: 0.015\n",
      "model: 5, epoch: 530, loss: 0.015\n",
      "model: 5, epoch: 531, loss: 0.015\n",
      "model: 5, epoch: 532, loss: 0.015\n",
      "model: 5, epoch: 533, loss: 0.015\n",
      "model: 5, epoch: 534, loss: 0.015\n",
      "model: 5, epoch: 535, loss: 0.015\n",
      "model: 5, epoch: 536, loss: 0.015\n",
      "model: 5, epoch: 537, loss: 0.015\n",
      "model: 5, epoch: 538, loss: 0.015\n",
      "model: 5, epoch: 539, loss: 0.015\n",
      "model: 5, epoch: 540, loss: 0.015\n",
      "model: 5, epoch: 541, loss: 0.015\n",
      "model: 5, epoch: 542, loss: 0.015\n",
      "model: 5, epoch: 543, loss: 0.015\n",
      "model: 5, epoch: 544, loss: 0.015\n",
      "model: 5, epoch: 545, loss: 0.014\n",
      "model: 5, epoch: 546, loss: 0.014\n",
      "model: 5, epoch: 547, loss: 0.014\n",
      "model: 5, epoch: 548, loss: 0.014\n",
      "model: 5, epoch: 549, loss: 0.014\n",
      "model: 5, epoch: 550, loss: 0.014\n",
      "model: 5, epoch: 551, loss: 0.014\n",
      "model: 5, epoch: 552, loss: 0.014\n",
      "model: 5, epoch: 553, loss: 0.014\n",
      "model: 5, epoch: 554, loss: 0.014\n",
      "model: 5, epoch: 555, loss: 0.014\n",
      "model: 5, epoch: 556, loss: 0.014\n",
      "model: 5, epoch: 557, loss: 0.014\n",
      "model: 5, epoch: 558, loss: 0.014\n",
      "model: 5, epoch: 559, loss: 0.014\n",
      "model: 5, epoch: 560, loss: 0.014\n",
      "model: 5, epoch: 561, loss: 0.014\n",
      "model: 5, epoch: 562, loss: 0.014\n",
      "model: 5, epoch: 563, loss: 0.013\n",
      "model: 5, epoch: 564, loss: 0.013\n",
      "model: 5, epoch: 565, loss: 0.013\n",
      "model: 5, epoch: 566, loss: 0.013\n",
      "model: 5, epoch: 567, loss: 0.013\n",
      "model: 5, epoch: 568, loss: 0.013\n",
      "model: 5, epoch: 569, loss: 0.013\n",
      "model: 5, epoch: 570, loss: 0.013\n",
      "model: 5, epoch: 571, loss: 0.013\n",
      "model: 5, epoch: 572, loss: 0.013\n",
      "model: 5, epoch: 573, loss: 0.013\n",
      "model: 5, epoch: 574, loss: 0.013\n",
      "model: 5, epoch: 575, loss: 0.013\n",
      "model: 5, epoch: 576, loss: 0.013\n",
      "model: 5, epoch: 577, loss: 0.013\n",
      "model: 5, epoch: 578, loss: 0.013\n",
      "model: 5, epoch: 579, loss: 0.013\n",
      "model: 5, epoch: 580, loss: 0.013\n",
      "model: 5, epoch: 581, loss: 0.013\n",
      "model: 5, epoch: 582, loss: 0.012\n",
      "model: 5, epoch: 583, loss: 0.012\n",
      "model: 5, epoch: 584, loss: 0.012\n",
      "model: 5, epoch: 585, loss: 0.012\n",
      "model: 5, epoch: 586, loss: 0.012\n",
      "model: 5, epoch: 587, loss: 0.012\n",
      "model: 5, epoch: 588, loss: 0.012\n",
      "model: 5, epoch: 589, loss: 0.012\n",
      "model: 5, epoch: 590, loss: 0.012\n",
      "model: 5, epoch: 591, loss: 0.012\n",
      "model: 5, epoch: 592, loss: 0.012\n",
      "model: 5, epoch: 593, loss: 0.012\n",
      "model: 5, epoch: 594, loss: 0.012\n",
      "model: 5, epoch: 595, loss: 0.012\n",
      "model: 5, epoch: 596, loss: 0.012\n",
      "model: 5, epoch: 597, loss: 0.012\n",
      "model: 5, epoch: 598, loss: 0.012\n",
      "model: 5, epoch: 599, loss: 0.012\n",
      "model: 5, epoch: 600, loss: 0.012\n",
      "model: 5, epoch: 601, loss: 0.012\n",
      "model: 5, epoch: 602, loss: 0.011\n",
      "model: 5, epoch: 603, loss: 0.011\n",
      "model: 5, epoch: 604, loss: 0.011\n",
      "model: 5, epoch: 605, loss: 0.011\n",
      "model: 5, epoch: 606, loss: 0.011\n",
      "model: 5, epoch: 607, loss: 0.011\n",
      "model: 5, epoch: 608, loss: 0.011\n",
      "model: 5, epoch: 609, loss: 0.011\n",
      "model: 5, epoch: 610, loss: 0.011\n",
      "model: 5, epoch: 611, loss: 0.011\n",
      "model: 5, epoch: 612, loss: 0.011\n",
      "model: 5, epoch: 613, loss: 0.011\n",
      "model: 5, epoch: 614, loss: 0.011\n",
      "model: 5, epoch: 615, loss: 0.011\n",
      "model: 5, epoch: 616, loss: 0.011\n",
      "model: 5, epoch: 617, loss: 0.011\n",
      "model: 5, epoch: 618, loss: 0.011\n",
      "model: 5, epoch: 619, loss: 0.011\n",
      "model: 5, epoch: 620, loss: 0.011\n",
      "model: 5, epoch: 621, loss: 0.011\n",
      "model: 5, epoch: 622, loss: 0.011\n",
      "model: 5, epoch: 623, loss: 0.011\n",
      "model: 5, epoch: 624, loss: 0.010\n",
      "model: 5, epoch: 625, loss: 0.010\n",
      "model: 5, epoch: 626, loss: 0.010\n",
      "model: 5, epoch: 627, loss: 0.010\n",
      "model: 5, epoch: 628, loss: 0.010\n",
      "model: 5, epoch: 629, loss: 0.010\n",
      "model: 5, epoch: 630, loss: 0.010\n",
      "model: 5, epoch: 631, loss: 0.010\n",
      "model: 5, epoch: 632, loss: 0.010\n",
      "model: 5, epoch: 633, loss: 0.010\n",
      "model: 5, epoch: 634, loss: 0.010\n",
      "model: 5, epoch: 635, loss: 0.010\n",
      "model: 5, epoch: 636, loss: 0.010\n",
      "model: 5, epoch: 637, loss: 0.010\n",
      "model: 5, epoch: 638, loss: 0.010\n",
      "model: 5, epoch: 639, loss: 0.010\n",
      "model: 5, epoch: 640, loss: 0.010\n",
      "model: 5, epoch: 641, loss: 0.010\n",
      "model: 5, epoch: 642, loss: 0.010\n",
      "model: 5, epoch: 643, loss: 0.010\n",
      "model: 5, epoch: 644, loss: 0.010\n",
      "model: 5, epoch: 645, loss: 0.010\n",
      "model: 5, epoch: 646, loss: 0.010\n",
      "model: 5, epoch: 647, loss: 0.010\n",
      "model: 5, epoch: 648, loss: 0.010\n",
      "model: 5, epoch: 649, loss: 0.010\n",
      "model: 5, epoch: 650, loss: 0.009\n",
      "model: 5, epoch: 651, loss: 0.009\n",
      "model: 5, epoch: 652, loss: 0.009\n",
      "model: 5, epoch: 653, loss: 0.009\n",
      "model: 5, epoch: 654, loss: 0.009\n",
      "model: 5, epoch: 655, loss: 0.009\n",
      "model: 5, epoch: 656, loss: 0.009\n",
      "model: 5, epoch: 657, loss: 0.009\n",
      "model: 5, epoch: 658, loss: 0.009\n",
      "model: 5, epoch: 659, loss: 0.009\n",
      "model: 5, epoch: 660, loss: 0.009\n",
      "model: 5, epoch: 661, loss: 0.009\n",
      "model: 5, epoch: 662, loss: 0.009\n",
      "model: 5, epoch: 663, loss: 0.009\n",
      "model: 5, epoch: 664, loss: 0.009\n",
      "model: 5, epoch: 665, loss: 0.009\n",
      "model: 5, epoch: 666, loss: 0.009\n",
      "model: 5, epoch: 667, loss: 0.009\n",
      "model: 5, epoch: 668, loss: 0.009\n",
      "model: 5, epoch: 669, loss: 0.009\n",
      "model: 5, epoch: 670, loss: 0.009\n",
      "model: 5, epoch: 671, loss: 0.009\n",
      "model: 5, epoch: 672, loss: 0.009\n",
      "model: 5, epoch: 673, loss: 0.009\n",
      "model: 5, epoch: 674, loss: 0.009\n",
      "model: 5, epoch: 675, loss: 0.009\n",
      "model: 5, epoch: 676, loss: 0.009\n",
      "model: 5, epoch: 677, loss: 0.009\n",
      "model: 5, epoch: 678, loss: 0.009\n",
      "model: 5, epoch: 679, loss: 0.009\n",
      "model: 5, epoch: 680, loss: 0.009\n",
      "model: 5, epoch: 681, loss: 0.009\n",
      "model: 5, epoch: 682, loss: 0.009\n",
      "model: 5, epoch: 683, loss: 0.008\n",
      "model: 5, epoch: 684, loss: 0.008\n",
      "model: 5, epoch: 685, loss: 0.008\n",
      "model: 5, epoch: 686, loss: 0.008\n",
      "model: 5, epoch: 687, loss: 0.008\n",
      "model: 5, epoch: 688, loss: 0.008\n",
      "model: 5, epoch: 689, loss: 0.008\n",
      "model: 5, epoch: 690, loss: 0.008\n",
      "model: 5, epoch: 691, loss: 0.008\n",
      "model: 5, epoch: 692, loss: 0.008\n",
      "model: 5, epoch: 693, loss: 0.008\n",
      "model: 5, epoch: 694, loss: 0.008\n",
      "model: 5, epoch: 695, loss: 0.008\n",
      "model: 5, epoch: 696, loss: 0.008\n",
      "model: 5, epoch: 697, loss: 0.008\n",
      "model: 5, epoch: 698, loss: 0.008\n",
      "model: 5, epoch: 699, loss: 0.008\n",
      "model: 5, epoch: 700, loss: 0.008\n",
      "model: 5, epoch: 701, loss: 0.008\n",
      "model: 5, epoch: 702, loss: 0.008\n",
      "model: 5, epoch: 703, loss: 0.008\n",
      "model: 5, epoch: 704, loss: 0.008\n",
      "model: 5, epoch: 705, loss: 0.008\n",
      "model: 5, epoch: 706, loss: 0.008\n",
      "model: 5, epoch: 707, loss: 0.008\n",
      "model: 5, epoch: 708, loss: 0.008\n",
      "model: 5, epoch: 709, loss: 0.008\n",
      "model: 5, epoch: 710, loss: 0.008\n",
      "model: 5, epoch: 711, loss: 0.008\n",
      "model: 5, epoch: 712, loss: 0.008\n",
      "model: 5, epoch: 713, loss: 0.008\n",
      "model: 5, epoch: 714, loss: 0.008\n",
      "model: 5, epoch: 715, loss: 0.008\n",
      "model: 5, epoch: 716, loss: 0.008\n",
      "model: 5, epoch: 717, loss: 0.008\n",
      "model: 5, epoch: 718, loss: 0.008\n",
      "model: 5, epoch: 719, loss: 0.008\n",
      "model: 5, epoch: 720, loss: 0.008\n",
      "model: 5, epoch: 721, loss: 0.008\n",
      "model: 5, epoch: 722, loss: 0.008\n",
      "model: 5, epoch: 723, loss: 0.008\n",
      "model: 5, epoch: 724, loss: 0.008\n",
      "model: 5, epoch: 725, loss: 0.008\n",
      "model: 5, epoch: 726, loss: 0.008\n",
      "model: 5, epoch: 727, loss: 0.008\n",
      "model: 5, epoch: 728, loss: 0.008\n",
      "model: 5, epoch: 729, loss: 0.008\n",
      "model: 5, epoch: 730, loss: 0.008\n",
      "model: 5, epoch: 731, loss: 0.008\n",
      "model: 5, epoch: 732, loss: 0.008\n",
      "model: 5, epoch: 733, loss: 0.008\n",
      "model: 5, epoch: 734, loss: 0.008\n",
      "model: 5, epoch: 735, loss: 0.008\n",
      "model: 5, epoch: 736, loss: 0.008\n",
      "model: 5, epoch: 737, loss: 0.007\n",
      "model: 5, epoch: 738, loss: 0.007\n",
      "model: 5, epoch: 739, loss: 0.007\n",
      "model: 5, epoch: 740, loss: 0.007\n",
      "model: 5, epoch: 741, loss: 0.007\n",
      "model: 5, epoch: 742, loss: 0.007\n",
      "model: 5, epoch: 743, loss: 0.007\n",
      "model: 5, epoch: 744, loss: 0.007\n",
      "model: 5, epoch: 745, loss: 0.007\n",
      "model: 5, epoch: 746, loss: 0.007\n",
      "model: 5, epoch: 747, loss: 0.007\n",
      "model: 5, epoch: 748, loss: 0.007\n",
      "model: 5, epoch: 749, loss: 0.007\n",
      "model: 5, epoch: 750, loss: 0.007\n",
      "model: 5, epoch: 751, loss: 0.007\n",
      "model: 5, epoch: 752, loss: 0.007\n",
      "model: 5, epoch: 753, loss: 0.007\n",
      "model: 5, epoch: 754, loss: 0.007\n",
      "model: 5, epoch: 755, loss: 0.007\n",
      "model: 5, epoch: 756, loss: 0.007\n",
      "model: 5, epoch: 757, loss: 0.007\n",
      "model: 5, epoch: 758, loss: 0.007\n",
      "model: 5, epoch: 759, loss: 0.007\n",
      "model: 5, epoch: 760, loss: 0.007\n",
      "model: 5, epoch: 761, loss: 0.007\n",
      "model: 5, epoch: 762, loss: 0.007\n",
      "model: 5, epoch: 763, loss: 0.007\n",
      "model: 5, epoch: 764, loss: 0.007\n",
      "model: 5, epoch: 765, loss: 0.007\n",
      "model: 5, epoch: 766, loss: 0.007\n",
      "model: 5, epoch: 767, loss: 0.007\n",
      "model: 5, epoch: 768, loss: 0.007\n",
      "model: 5, epoch: 769, loss: 0.007\n",
      "model: 5, epoch: 770, loss: 0.007\n",
      "model: 5, epoch: 771, loss: 0.007\n",
      "model: 5, epoch: 772, loss: 0.007\n",
      "model: 5, epoch: 773, loss: 0.007\n",
      "model: 5, epoch: 774, loss: 0.007\n",
      "model: 5, epoch: 775, loss: 0.007\n",
      "model: 5, epoch: 776, loss: 0.007\n",
      "model: 5, epoch: 777, loss: 0.007\n",
      "model: 5, epoch: 778, loss: 0.007\n",
      "model: 5, epoch: 779, loss: 0.007\n",
      "model: 5, epoch: 780, loss: 0.007\n",
      "model: 5, epoch: 781, loss: 0.007\n",
      "model: 5, epoch: 782, loss: 0.007\n",
      "model: 5, epoch: 783, loss: 0.007\n",
      "model: 5, epoch: 784, loss: 0.007\n",
      "model: 5, epoch: 785, loss: 0.007\n",
      "model: 5, epoch: 786, loss: 0.007\n",
      "model: 5, epoch: 787, loss: 0.007\n",
      "model: 5, epoch: 788, loss: 0.007\n",
      "model: 5, epoch: 789, loss: 0.007\n",
      "model: 5, epoch: 790, loss: 0.007\n",
      "model: 5, epoch: 791, loss: 0.007\n",
      "model: 5, epoch: 792, loss: 0.007\n",
      "model: 5, epoch: 793, loss: 0.007\n",
      "model: 5, epoch: 794, loss: 0.007\n",
      "model: 5, epoch: 795, loss: 0.007\n",
      "model: 5, epoch: 796, loss: 0.007\n",
      "model: 5, epoch: 797, loss: 0.007\n",
      "model: 5, epoch: 798, loss: 0.007\n",
      "model: 5, epoch: 799, loss: 0.007\n",
      "model: 5, epoch: 800, loss: 0.007\n",
      "model: 5, epoch: 801, loss: 0.007\n",
      "model: 5, epoch: 802, loss: 0.007\n",
      "model: 5, epoch: 803, loss: 0.007\n",
      "model: 5, epoch: 804, loss: 0.007\n",
      "model: 5, epoch: 805, loss: 0.007\n",
      "model: 5, epoch: 806, loss: 0.007\n",
      "model: 5, epoch: 807, loss: 0.007\n",
      "model: 5, epoch: 808, loss: 0.007\n",
      "model: 5, epoch: 809, loss: 0.007\n",
      "model: 5, epoch: 810, loss: 0.007\n",
      "model: 5, epoch: 811, loss: 0.007\n",
      "model: 5, epoch: 812, loss: 0.007\n",
      "model: 5, epoch: 813, loss: 0.007\n",
      "model: 5, epoch: 814, loss: 0.007\n",
      "model: 5, epoch: 815, loss: 0.007\n",
      "model: 5, epoch: 816, loss: 0.007\n",
      "model: 5, epoch: 817, loss: 0.007\n",
      "model: 5, epoch: 818, loss: 0.007\n",
      "model: 5, epoch: 819, loss: 0.007\n",
      "model: 5, epoch: 820, loss: 0.007\n",
      "model: 5, epoch: 821, loss: 0.007\n",
      "model: 5, epoch: 822, loss: 0.007\n",
      "model: 5, epoch: 823, loss: 0.007\n",
      "model: 5, epoch: 824, loss: 0.007\n",
      "model: 5, epoch: 825, loss: 0.007\n",
      "model: 5, epoch: 826, loss: 0.007\n",
      "model: 5, epoch: 827, loss: 0.007\n",
      "model: 5, epoch: 828, loss: 0.007\n",
      "model: 5, epoch: 829, loss: 0.007\n",
      "model: 5, epoch: 830, loss: 0.007\n",
      "model: 5, epoch: 831, loss: 0.007\n",
      "model: 5, epoch: 832, loss: 0.007\n",
      "model: 5, epoch: 833, loss: 0.007\n",
      "model: 5, epoch: 834, loss: 0.007\n",
      "model: 5, epoch: 835, loss: 0.007\n",
      "model: 5, epoch: 836, loss: 0.007\n",
      "model: 5, epoch: 837, loss: 0.007\n",
      "model: 5, epoch: 838, loss: 0.007\n",
      "model: 5, epoch: 839, loss: 0.007\n",
      "model: 5, epoch: 840, loss: 0.007\n",
      "model: 5, epoch: 841, loss: 0.007\n",
      "model: 5, epoch: 842, loss: 0.007\n",
      "model: 5, epoch: 843, loss: 0.007\n",
      "model: 5, epoch: 844, loss: 0.007\n",
      "model: 5, epoch: 845, loss: 0.007\n",
      "model: 5, epoch: 846, loss: 0.007\n",
      "model: 5, epoch: 847, loss: 0.007\n",
      "model: 5, epoch: 848, loss: 0.007\n",
      "model: 5, epoch: 849, loss: 0.007\n",
      "model: 5, epoch: 850, loss: 0.007\n",
      "model: 5, epoch: 851, loss: 0.007\n",
      "model: 5, epoch: 852, loss: 0.007\n",
      "model: 5, epoch: 853, loss: 0.007\n",
      "model: 5, epoch: 854, loss: 0.007\n",
      "model: 5, epoch: 855, loss: 0.007\n",
      "model: 5, epoch: 856, loss: 0.007\n",
      "model: 5, epoch: 857, loss: 0.007\n",
      "model: 5, epoch: 858, loss: 0.007\n",
      "model: 5, epoch: 859, loss: 0.007\n",
      "model: 5, epoch: 860, loss: 0.007\n",
      "model: 5, epoch: 861, loss: 0.007\n",
      "model: 5, epoch: 862, loss: 0.007\n",
      "model: 5, epoch: 863, loss: 0.007\n",
      "model: 5, epoch: 864, loss: 0.007\n",
      "model: 5, epoch: 865, loss: 0.007\n",
      "model: 5, epoch: 866, loss: 0.007\n",
      "model: 5, epoch: 867, loss: 0.007\n",
      "model: 5, epoch: 868, loss: 0.007\n",
      "model: 5, epoch: 869, loss: 0.007\n",
      "model: 5, epoch: 870, loss: 0.007\n",
      "model: 5, epoch: 871, loss: 0.007\n",
      "model: 5, epoch: 872, loss: 0.007\n",
      "model: 5, epoch: 873, loss: 0.007\n",
      "model: 5, epoch: 874, loss: 0.007\n",
      "model: 5, epoch: 875, loss: 0.007\n",
      "model: 5, epoch: 876, loss: 0.007\n",
      "model: 5, epoch: 877, loss: 0.007\n",
      "model: 5, epoch: 878, loss: 0.007\n",
      "model: 5, epoch: 879, loss: 0.007\n",
      "model: 5, epoch: 880, loss: 0.007\n",
      "model: 5, epoch: 881, loss: 0.007\n",
      "model: 5, epoch: 882, loss: 0.007\n",
      "model: 5, epoch: 883, loss: 0.007\n",
      "model: 5, epoch: 884, loss: 0.007\n",
      "model: 5, epoch: 885, loss: 0.007\n",
      "model: 5, epoch: 886, loss: 0.007\n",
      "model: 5, epoch: 887, loss: 0.007\n",
      "model: 5, epoch: 888, loss: 0.007\n",
      "model: 5, epoch: 889, loss: 0.007\n",
      "model: 5, epoch: 890, loss: 0.007\n",
      "model: 5, epoch: 891, loss: 0.007\n",
      "model: 5, epoch: 892, loss: 0.007\n",
      "model: 5, epoch: 893, loss: 0.007\n",
      "model: 5, epoch: 894, loss: 0.007\n",
      "model: 5, epoch: 895, loss: 0.007\n",
      "model: 5, epoch: 896, loss: 0.007\n",
      "model: 5, epoch: 897, loss: 0.007\n",
      "model: 5, epoch: 898, loss: 0.007\n",
      "model: 5, epoch: 899, loss: 0.007\n",
      "model: 5, epoch: 900, loss: 0.007\n",
      "model: 5, epoch: 901, loss: 0.007\n",
      "model: 5, epoch: 902, loss: 0.007\n",
      "model: 5, epoch: 903, loss: 0.007\n",
      "model: 5, epoch: 904, loss: 0.007\n",
      "model: 5, epoch: 905, loss: 0.007\n",
      "model: 5, epoch: 906, loss: 0.007\n",
      "model: 5, epoch: 907, loss: 0.007\n",
      "model: 5, epoch: 908, loss: 0.007\n",
      "model: 5, epoch: 909, loss: 0.007\n",
      "model: 5, epoch: 910, loss: 0.007\n",
      "model: 5, epoch: 911, loss: 0.007\n",
      "model: 5, epoch: 912, loss: 0.007\n",
      "model: 5, epoch: 913, loss: 0.007\n",
      "model: 5, epoch: 914, loss: 0.007\n",
      "model: 5, epoch: 915, loss: 0.007\n",
      "model: 5, epoch: 916, loss: 0.007\n",
      "model: 5, epoch: 917, loss: 0.007\n",
      "model: 5, epoch: 918, loss: 0.007\n",
      "model: 5, epoch: 919, loss: 0.007\n",
      "model: 5, epoch: 920, loss: 0.007\n",
      "model: 5, epoch: 921, loss: 0.007\n",
      "model: 5, epoch: 922, loss: 0.007\n",
      "model: 5, epoch: 923, loss: 0.007\n",
      "model: 5, epoch: 924, loss: 0.007\n",
      "model: 5, epoch: 925, loss: 0.007\n",
      "model: 5, epoch: 926, loss: 0.007\n",
      "model: 5, epoch: 927, loss: 0.007\n",
      "model: 5, epoch: 928, loss: 0.007\n",
      "model: 5, epoch: 929, loss: 0.007\n",
      "model: 5, epoch: 930, loss: 0.007\n",
      "model: 5, epoch: 931, loss: 0.007\n",
      "model: 5, epoch: 932, loss: 0.007\n",
      "model: 5, epoch: 933, loss: 0.007\n",
      "model: 5, epoch: 934, loss: 0.007\n",
      "model: 5, epoch: 935, loss: 0.007\n",
      "model: 5, epoch: 936, loss: 0.007\n",
      "model: 5, epoch: 937, loss: 0.007\n",
      "model: 5, epoch: 938, loss: 0.007\n",
      "model: 5, epoch: 939, loss: 0.007\n",
      "model: 5, epoch: 940, loss: 0.007\n",
      "model: 5, epoch: 941, loss: 0.007\n",
      "model: 5, epoch: 942, loss: 0.007\n",
      "model: 5, epoch: 943, loss: 0.007\n",
      "model: 5, epoch: 944, loss: 0.007\n",
      "model: 5, epoch: 945, loss: 0.007\n",
      "model: 5, epoch: 946, loss: 0.007\n",
      "model: 5, epoch: 947, loss: 0.007\n",
      "model: 5, epoch: 948, loss: 0.007\n",
      "model: 5, epoch: 949, loss: 0.007\n",
      "model: 5, epoch: 950, loss: 0.007\n",
      "model: 5, epoch: 951, loss: 0.007\n",
      "model: 5, epoch: 952, loss: 0.007\n",
      "model: 5, epoch: 953, loss: 0.007\n",
      "model: 5, epoch: 954, loss: 0.007\n",
      "model: 5, epoch: 955, loss: 0.007\n",
      "model: 5, epoch: 956, loss: 0.007\n",
      "model: 5, epoch: 957, loss: 0.007\n",
      "model: 5, epoch: 958, loss: 0.007\n",
      "model: 5, epoch: 959, loss: 0.007\n",
      "model: 5, epoch: 960, loss: 0.007\n",
      "model: 5, epoch: 961, loss: 0.007\n",
      "model: 5, epoch: 962, loss: 0.007\n",
      "model: 5, epoch: 963, loss: 0.007\n",
      "model: 5, epoch: 964, loss: 0.007\n",
      "model: 5, epoch: 965, loss: 0.007\n",
      "model: 5, epoch: 966, loss: 0.007\n",
      "model: 5, epoch: 967, loss: 0.007\n",
      "model: 5, epoch: 968, loss: 0.007\n",
      "model: 5, epoch: 969, loss: 0.007\n",
      "model: 5, epoch: 970, loss: 0.007\n",
      "model: 5, epoch: 971, loss: 0.007\n",
      "model: 5, epoch: 972, loss: 0.007\n",
      "model: 5, epoch: 973, loss: 0.007\n",
      "model: 5, epoch: 974, loss: 0.007\n",
      "model: 5, epoch: 975, loss: 0.007\n",
      "model: 5, epoch: 976, loss: 0.007\n",
      "model: 5, epoch: 977, loss: 0.007\n",
      "model: 5, epoch: 978, loss: 0.007\n",
      "model: 5, epoch: 979, loss: 0.007\n",
      "model: 5, epoch: 980, loss: 0.007\n",
      "model: 5, epoch: 981, loss: 0.007\n",
      "model: 5, epoch: 982, loss: 0.007\n",
      "model: 5, epoch: 983, loss: 0.007\n",
      "model: 5, epoch: 984, loss: 0.007\n",
      "model: 5, epoch: 985, loss: 0.007\n",
      "model: 5, epoch: 986, loss: 0.007\n",
      "model: 5, epoch: 987, loss: 0.007\n",
      "model: 5, epoch: 988, loss: 0.007\n",
      "model: 5, epoch: 989, loss: 0.007\n",
      "model: 5, epoch: 990, loss: 0.007\n",
      "model: 5, epoch: 991, loss: 0.007\n",
      "model: 5, epoch: 992, loss: 0.007\n",
      "model: 5, epoch: 993, loss: 0.007\n",
      "model: 5, epoch: 994, loss: 0.007\n",
      "model: 5, epoch: 995, loss: 0.007\n",
      "model: 5, epoch: 996, loss: 0.007\n",
      "model: 5, epoch: 997, loss: 0.007\n",
      "model: 5, epoch: 998, loss: 0.007\n",
      "model: 5, epoch: 999, loss: 0.007\n",
      "model: 6, epoch: 0, loss: 0.244\n",
      "model: 6, epoch: 1, loss: 0.210\n",
      "model: 6, epoch: 2, loss: 0.182\n",
      "model: 6, epoch: 3, loss: 0.158\n",
      "model: 6, epoch: 4, loss: 0.139\n",
      "model: 6, epoch: 5, loss: 0.125\n",
      "model: 6, epoch: 6, loss: 0.116\n",
      "model: 6, epoch: 7, loss: 0.110\n",
      "model: 6, epoch: 8, loss: 0.107\n",
      "model: 6, epoch: 9, loss: 0.107\n",
      "model: 6, epoch: 10, loss: 0.108\n",
      "model: 6, epoch: 11, loss: 0.110\n",
      "model: 6, epoch: 12, loss: 0.111\n",
      "model: 6, epoch: 13, loss: 0.112\n",
      "model: 6, epoch: 14, loss: 0.112\n",
      "model: 6, epoch: 15, loss: 0.111\n",
      "model: 6, epoch: 16, loss: 0.109\n",
      "model: 6, epoch: 17, loss: 0.107\n",
      "model: 6, epoch: 18, loss: 0.104\n",
      "model: 6, epoch: 19, loss: 0.100\n",
      "model: 6, epoch: 20, loss: 0.097\n",
      "model: 6, epoch: 21, loss: 0.093\n",
      "model: 6, epoch: 22, loss: 0.090\n",
      "model: 6, epoch: 23, loss: 0.088\n",
      "model: 6, epoch: 24, loss: 0.086\n",
      "model: 6, epoch: 25, loss: 0.084\n",
      "model: 6, epoch: 26, loss: 0.083\n",
      "model: 6, epoch: 27, loss: 0.082\n",
      "model: 6, epoch: 28, loss: 0.081\n",
      "model: 6, epoch: 29, loss: 0.080\n",
      "model: 6, epoch: 30, loss: 0.079\n",
      "model: 6, epoch: 31, loss: 0.078\n",
      "model: 6, epoch: 32, loss: 0.077\n",
      "model: 6, epoch: 33, loss: 0.075\n",
      "model: 6, epoch: 34, loss: 0.074\n",
      "model: 6, epoch: 35, loss: 0.072\n",
      "model: 6, epoch: 36, loss: 0.071\n",
      "model: 6, epoch: 37, loss: 0.069\n",
      "model: 6, epoch: 38, loss: 0.068\n",
      "model: 6, epoch: 39, loss: 0.066\n",
      "model: 6, epoch: 40, loss: 0.065\n",
      "model: 6, epoch: 41, loss: 0.064\n",
      "model: 6, epoch: 42, loss: 0.062\n",
      "model: 6, epoch: 43, loss: 0.061\n",
      "model: 6, epoch: 44, loss: 0.060\n",
      "model: 6, epoch: 45, loss: 0.059\n",
      "model: 6, epoch: 46, loss: 0.059\n",
      "model: 6, epoch: 47, loss: 0.058\n",
      "model: 6, epoch: 48, loss: 0.057\n",
      "model: 6, epoch: 49, loss: 0.056\n",
      "model: 6, epoch: 50, loss: 0.055\n",
      "model: 6, epoch: 51, loss: 0.054\n",
      "model: 6, epoch: 52, loss: 0.053\n",
      "model: 6, epoch: 53, loss: 0.052\n",
      "model: 6, epoch: 54, loss: 0.051\n",
      "model: 6, epoch: 55, loss: 0.050\n",
      "model: 6, epoch: 56, loss: 0.049\n",
      "model: 6, epoch: 57, loss: 0.048\n",
      "model: 6, epoch: 58, loss: 0.048\n",
      "model: 6, epoch: 59, loss: 0.047\n",
      "model: 6, epoch: 60, loss: 0.046\n",
      "model: 6, epoch: 61, loss: 0.045\n",
      "model: 6, epoch: 62, loss: 0.045\n",
      "model: 6, epoch: 63, loss: 0.044\n",
      "model: 6, epoch: 64, loss: 0.043\n",
      "model: 6, epoch: 65, loss: 0.043\n",
      "model: 6, epoch: 66, loss: 0.042\n",
      "model: 6, epoch: 67, loss: 0.042\n",
      "model: 6, epoch: 68, loss: 0.041\n",
      "model: 6, epoch: 69, loss: 0.040\n",
      "model: 6, epoch: 70, loss: 0.040\n",
      "model: 6, epoch: 71, loss: 0.039\n",
      "model: 6, epoch: 72, loss: 0.039\n",
      "model: 6, epoch: 73, loss: 0.038\n",
      "model: 6, epoch: 74, loss: 0.038\n",
      "model: 6, epoch: 75, loss: 0.038\n",
      "model: 6, epoch: 76, loss: 0.037\n",
      "model: 6, epoch: 77, loss: 0.037\n",
      "model: 6, epoch: 78, loss: 0.036\n",
      "model: 6, epoch: 79, loss: 0.036\n",
      "model: 6, epoch: 80, loss: 0.036\n",
      "model: 6, epoch: 81, loss: 0.035\n",
      "model: 6, epoch: 82, loss: 0.035\n",
      "model: 6, epoch: 83, loss: 0.035\n",
      "model: 6, epoch: 84, loss: 0.034\n",
      "model: 6, epoch: 85, loss: 0.034\n",
      "model: 6, epoch: 86, loss: 0.034\n",
      "model: 6, epoch: 87, loss: 0.033\n",
      "model: 6, epoch: 88, loss: 0.033\n",
      "model: 6, epoch: 89, loss: 0.033\n",
      "model: 6, epoch: 90, loss: 0.033\n",
      "model: 6, epoch: 91, loss: 0.032\n",
      "model: 6, epoch: 92, loss: 0.032\n",
      "model: 6, epoch: 93, loss: 0.032\n",
      "model: 6, epoch: 94, loss: 0.032\n",
      "model: 6, epoch: 95, loss: 0.032\n",
      "model: 6, epoch: 96, loss: 0.031\n",
      "model: 6, epoch: 97, loss: 0.031\n",
      "model: 6, epoch: 98, loss: 0.031\n",
      "model: 6, epoch: 99, loss: 0.031\n",
      "model: 6, epoch: 100, loss: 0.031\n",
      "model: 6, epoch: 101, loss: 0.031\n",
      "model: 6, epoch: 102, loss: 0.031\n",
      "model: 6, epoch: 103, loss: 0.030\n",
      "model: 6, epoch: 104, loss: 0.030\n",
      "model: 6, epoch: 105, loss: 0.030\n",
      "model: 6, epoch: 106, loss: 0.030\n",
      "model: 6, epoch: 107, loss: 0.030\n",
      "model: 6, epoch: 108, loss: 0.030\n",
      "model: 6, epoch: 109, loss: 0.030\n",
      "model: 6, epoch: 110, loss: 0.030\n",
      "model: 6, epoch: 111, loss: 0.030\n",
      "model: 6, epoch: 112, loss: 0.030\n",
      "model: 6, epoch: 113, loss: 0.030\n",
      "model: 6, epoch: 114, loss: 0.029\n",
      "model: 6, epoch: 115, loss: 0.029\n",
      "model: 6, epoch: 116, loss: 0.029\n",
      "model: 6, epoch: 117, loss: 0.029\n",
      "model: 6, epoch: 118, loss: 0.029\n",
      "model: 6, epoch: 119, loss: 0.029\n",
      "model: 6, epoch: 120, loss: 0.029\n",
      "model: 6, epoch: 121, loss: 0.029\n",
      "model: 6, epoch: 122, loss: 0.029\n",
      "model: 6, epoch: 123, loss: 0.029\n",
      "model: 6, epoch: 124, loss: 0.029\n",
      "model: 6, epoch: 125, loss: 0.029\n",
      "model: 6, epoch: 126, loss: 0.029\n",
      "model: 6, epoch: 127, loss: 0.029\n",
      "model: 6, epoch: 128, loss: 0.029\n",
      "model: 6, epoch: 129, loss: 0.029\n",
      "model: 6, epoch: 130, loss: 0.029\n",
      "model: 6, epoch: 131, loss: 0.029\n",
      "model: 6, epoch: 132, loss: 0.029\n",
      "model: 6, epoch: 133, loss: 0.029\n",
      "model: 6, epoch: 134, loss: 0.029\n",
      "model: 6, epoch: 135, loss: 0.029\n",
      "model: 6, epoch: 136, loss: 0.029\n",
      "model: 6, epoch: 137, loss: 0.029\n",
      "model: 6, epoch: 138, loss: 0.029\n",
      "model: 6, epoch: 139, loss: 0.029\n",
      "model: 6, epoch: 140, loss: 0.029\n",
      "model: 6, epoch: 141, loss: 0.029\n",
      "model: 6, epoch: 142, loss: 0.029\n",
      "model: 6, epoch: 143, loss: 0.029\n",
      "model: 6, epoch: 144, loss: 0.029\n",
      "model: 6, epoch: 145, loss: 0.029\n",
      "model: 6, epoch: 146, loss: 0.029\n",
      "model: 6, epoch: 147, loss: 0.029\n",
      "model: 6, epoch: 148, loss: 0.029\n",
      "model: 6, epoch: 149, loss: 0.029\n",
      "model: 6, epoch: 150, loss: 0.029\n",
      "model: 6, epoch: 151, loss: 0.029\n",
      "model: 6, epoch: 152, loss: 0.029\n",
      "model: 6, epoch: 153, loss: 0.029\n",
      "model: 6, epoch: 154, loss: 0.029\n",
      "model: 6, epoch: 155, loss: 0.029\n",
      "model: 6, epoch: 156, loss: 0.029\n",
      "model: 6, epoch: 157, loss: 0.029\n",
      "model: 6, epoch: 158, loss: 0.029\n",
      "model: 6, epoch: 159, loss: 0.029\n",
      "model: 6, epoch: 160, loss: 0.029\n",
      "model: 6, epoch: 161, loss: 0.029\n",
      "model: 6, epoch: 162, loss: 0.029\n",
      "model: 6, epoch: 163, loss: 0.029\n",
      "model: 6, epoch: 164, loss: 0.029\n",
      "model: 6, epoch: 165, loss: 0.029\n",
      "model: 6, epoch: 166, loss: 0.029\n",
      "model: 6, epoch: 167, loss: 0.029\n",
      "model: 6, epoch: 168, loss: 0.029\n",
      "model: 6, epoch: 169, loss: 0.029\n",
      "model: 6, epoch: 170, loss: 0.029\n",
      "model: 6, epoch: 171, loss: 0.029\n",
      "model: 6, epoch: 172, loss: 0.029\n",
      "model: 6, epoch: 173, loss: 0.029\n",
      "model: 6, epoch: 174, loss: 0.029\n",
      "model: 6, epoch: 175, loss: 0.029\n",
      "model: 6, epoch: 176, loss: 0.029\n",
      "model: 6, epoch: 177, loss: 0.029\n",
      "model: 6, epoch: 178, loss: 0.029\n",
      "model: 6, epoch: 179, loss: 0.029\n",
      "model: 6, epoch: 180, loss: 0.029\n",
      "model: 6, epoch: 181, loss: 0.029\n",
      "model: 6, epoch: 182, loss: 0.029\n",
      "model: 6, epoch: 183, loss: 0.029\n",
      "model: 6, epoch: 184, loss: 0.029\n",
      "model: 6, epoch: 185, loss: 0.029\n",
      "model: 6, epoch: 186, loss: 0.029\n",
      "model: 6, epoch: 187, loss: 0.029\n",
      "model: 6, epoch: 188, loss: 0.029\n",
      "model: 6, epoch: 189, loss: 0.029\n",
      "model: 6, epoch: 190, loss: 0.029\n",
      "model: 6, epoch: 191, loss: 0.029\n",
      "model: 6, epoch: 192, loss: 0.029\n",
      "model: 6, epoch: 193, loss: 0.029\n",
      "model: 6, epoch: 194, loss: 0.029\n",
      "model: 6, epoch: 195, loss: 0.029\n",
      "model: 6, epoch: 196, loss: 0.029\n",
      "model: 6, epoch: 197, loss: 0.029\n",
      "model: 6, epoch: 198, loss: 0.029\n",
      "model: 6, epoch: 199, loss: 0.029\n",
      "model: 6, epoch: 200, loss: 0.029\n",
      "model: 6, epoch: 201, loss: 0.029\n",
      "model: 6, epoch: 202, loss: 0.029\n",
      "model: 6, epoch: 203, loss: 0.029\n",
      "model: 6, epoch: 204, loss: 0.029\n",
      "model: 6, epoch: 205, loss: 0.029\n",
      "model: 6, epoch: 206, loss: 0.029\n",
      "model: 6, epoch: 207, loss: 0.029\n",
      "model: 6, epoch: 208, loss: 0.029\n",
      "model: 6, epoch: 209, loss: 0.029\n",
      "model: 6, epoch: 210, loss: 0.029\n",
      "model: 6, epoch: 211, loss: 0.029\n",
      "model: 6, epoch: 212, loss: 0.029\n",
      "model: 6, epoch: 213, loss: 0.029\n",
      "model: 6, epoch: 214, loss: 0.029\n",
      "model: 6, epoch: 215, loss: 0.029\n",
      "model: 6, epoch: 216, loss: 0.029\n",
      "model: 6, epoch: 217, loss: 0.029\n",
      "model: 6, epoch: 218, loss: 0.029\n",
      "model: 6, epoch: 219, loss: 0.029\n",
      "model: 6, epoch: 220, loss: 0.029\n",
      "model: 6, epoch: 221, loss: 0.029\n",
      "model: 6, epoch: 222, loss: 0.029\n",
      "model: 6, epoch: 223, loss: 0.029\n",
      "model: 6, epoch: 224, loss: 0.029\n",
      "model: 6, epoch: 225, loss: 0.029\n",
      "model: 6, epoch: 226, loss: 0.029\n",
      "model: 6, epoch: 227, loss: 0.029\n",
      "model: 6, epoch: 228, loss: 0.029\n",
      "model: 6, epoch: 229, loss: 0.029\n",
      "model: 6, epoch: 230, loss: 0.029\n",
      "model: 6, epoch: 231, loss: 0.029\n",
      "model: 6, epoch: 232, loss: 0.029\n",
      "model: 6, epoch: 233, loss: 0.029\n",
      "model: 6, epoch: 234, loss: 0.029\n",
      "model: 6, epoch: 235, loss: 0.029\n",
      "model: 6, epoch: 236, loss: 0.029\n",
      "model: 6, epoch: 237, loss: 0.029\n",
      "model: 6, epoch: 238, loss: 0.029\n",
      "model: 6, epoch: 239, loss: 0.029\n",
      "model: 6, epoch: 240, loss: 0.029\n",
      "model: 6, epoch: 241, loss: 0.029\n",
      "model: 6, epoch: 242, loss: 0.029\n",
      "model: 6, epoch: 243, loss: 0.029\n",
      "model: 6, epoch: 244, loss: 0.029\n",
      "model: 6, epoch: 245, loss: 0.029\n",
      "model: 6, epoch: 246, loss: 0.029\n",
      "model: 6, epoch: 247, loss: 0.029\n",
      "model: 6, epoch: 248, loss: 0.029\n",
      "model: 6, epoch: 249, loss: 0.029\n",
      "model: 6, epoch: 250, loss: 0.029\n",
      "model: 6, epoch: 251, loss: 0.029\n",
      "model: 6, epoch: 252, loss: 0.029\n",
      "model: 6, epoch: 253, loss: 0.029\n",
      "model: 6, epoch: 254, loss: 0.029\n",
      "model: 6, epoch: 255, loss: 0.029\n",
      "model: 6, epoch: 256, loss: 0.029\n",
      "model: 6, epoch: 257, loss: 0.029\n",
      "model: 6, epoch: 258, loss: 0.029\n",
      "model: 6, epoch: 259, loss: 0.029\n",
      "model: 6, epoch: 260, loss: 0.029\n",
      "model: 6, epoch: 261, loss: 0.029\n",
      "model: 6, epoch: 262, loss: 0.029\n",
      "model: 6, epoch: 263, loss: 0.029\n",
      "model: 6, epoch: 264, loss: 0.029\n",
      "model: 6, epoch: 265, loss: 0.029\n",
      "model: 6, epoch: 266, loss: 0.029\n",
      "model: 6, epoch: 267, loss: 0.029\n",
      "model: 6, epoch: 268, loss: 0.029\n",
      "model: 6, epoch: 269, loss: 0.029\n",
      "model: 6, epoch: 270, loss: 0.029\n",
      "model: 6, epoch: 271, loss: 0.029\n",
      "model: 6, epoch: 272, loss: 0.029\n",
      "model: 6, epoch: 273, loss: 0.029\n",
      "model: 6, epoch: 274, loss: 0.029\n",
      "model: 6, epoch: 275, loss: 0.029\n",
      "model: 6, epoch: 276, loss: 0.029\n",
      "model: 6, epoch: 277, loss: 0.029\n",
      "model: 6, epoch: 278, loss: 0.029\n",
      "model: 6, epoch: 279, loss: 0.029\n",
      "model: 6, epoch: 280, loss: 0.029\n",
      "model: 6, epoch: 281, loss: 0.029\n",
      "model: 6, epoch: 282, loss: 0.029\n",
      "model: 6, epoch: 283, loss: 0.029\n",
      "model: 6, epoch: 284, loss: 0.029\n",
      "model: 6, epoch: 285, loss: 0.029\n",
      "model: 6, epoch: 286, loss: 0.029\n",
      "model: 6, epoch: 287, loss: 0.029\n",
      "model: 6, epoch: 288, loss: 0.029\n",
      "model: 6, epoch: 289, loss: 0.029\n",
      "model: 6, epoch: 290, loss: 0.029\n",
      "model: 6, epoch: 291, loss: 0.029\n",
      "model: 6, epoch: 292, loss: 0.029\n",
      "model: 6, epoch: 293, loss: 0.029\n",
      "model: 6, epoch: 294, loss: 0.029\n",
      "model: 6, epoch: 295, loss: 0.029\n",
      "model: 6, epoch: 296, loss: 0.029\n",
      "model: 6, epoch: 297, loss: 0.029\n",
      "model: 6, epoch: 298, loss: 0.029\n",
      "model: 6, epoch: 299, loss: 0.029\n",
      "model: 6, epoch: 300, loss: 0.029\n",
      "model: 6, epoch: 301, loss: 0.029\n",
      "model: 6, epoch: 302, loss: 0.029\n",
      "model: 6, epoch: 303, loss: 0.029\n",
      "model: 6, epoch: 304, loss: 0.029\n",
      "model: 6, epoch: 305, loss: 0.029\n",
      "model: 6, epoch: 306, loss: 0.029\n",
      "model: 6, epoch: 307, loss: 0.029\n",
      "model: 6, epoch: 308, loss: 0.029\n",
      "model: 6, epoch: 309, loss: 0.029\n",
      "model: 6, epoch: 310, loss: 0.029\n",
      "model: 6, epoch: 311, loss: 0.029\n",
      "model: 6, epoch: 312, loss: 0.029\n",
      "model: 6, epoch: 313, loss: 0.029\n",
      "model: 6, epoch: 314, loss: 0.029\n",
      "model: 6, epoch: 315, loss: 0.029\n",
      "model: 6, epoch: 316, loss: 0.029\n",
      "model: 6, epoch: 317, loss: 0.029\n",
      "model: 6, epoch: 318, loss: 0.029\n",
      "model: 6, epoch: 319, loss: 0.029\n",
      "model: 6, epoch: 320, loss: 0.029\n",
      "model: 6, epoch: 321, loss: 0.029\n",
      "model: 6, epoch: 322, loss: 0.029\n",
      "model: 6, epoch: 323, loss: 0.029\n",
      "model: 6, epoch: 324, loss: 0.029\n",
      "model: 6, epoch: 325, loss: 0.029\n",
      "model: 6, epoch: 326, loss: 0.029\n",
      "model: 6, epoch: 327, loss: 0.029\n",
      "model: 6, epoch: 328, loss: 0.029\n",
      "model: 6, epoch: 329, loss: 0.029\n",
      "model: 6, epoch: 330, loss: 0.029\n",
      "model: 6, epoch: 331, loss: 0.029\n",
      "model: 6, epoch: 332, loss: 0.029\n",
      "model: 6, epoch: 333, loss: 0.029\n",
      "model: 6, epoch: 334, loss: 0.029\n",
      "model: 6, epoch: 335, loss: 0.029\n",
      "model: 6, epoch: 336, loss: 0.029\n",
      "model: 6, epoch: 337, loss: 0.029\n",
      "model: 6, epoch: 338, loss: 0.029\n",
      "model: 6, epoch: 339, loss: 0.029\n",
      "model: 6, epoch: 340, loss: 0.029\n",
      "model: 6, epoch: 341, loss: 0.029\n",
      "model: 6, epoch: 342, loss: 0.029\n",
      "model: 6, epoch: 343, loss: 0.029\n",
      "model: 6, epoch: 344, loss: 0.029\n",
      "model: 6, epoch: 345, loss: 0.029\n",
      "model: 6, epoch: 346, loss: 0.029\n",
      "model: 6, epoch: 347, loss: 0.029\n",
      "model: 6, epoch: 348, loss: 0.029\n",
      "model: 6, epoch: 349, loss: 0.029\n",
      "model: 6, epoch: 350, loss: 0.029\n",
      "model: 6, epoch: 351, loss: 0.029\n",
      "model: 6, epoch: 352, loss: 0.029\n",
      "model: 6, epoch: 353, loss: 0.029\n",
      "model: 6, epoch: 354, loss: 0.029\n",
      "model: 6, epoch: 355, loss: 0.029\n",
      "model: 6, epoch: 356, loss: 0.029\n",
      "model: 6, epoch: 357, loss: 0.029\n",
      "model: 6, epoch: 358, loss: 0.029\n",
      "model: 6, epoch: 359, loss: 0.029\n",
      "model: 6, epoch: 360, loss: 0.029\n",
      "model: 6, epoch: 361, loss: 0.029\n",
      "model: 6, epoch: 362, loss: 0.029\n",
      "model: 6, epoch: 363, loss: 0.029\n",
      "model: 6, epoch: 364, loss: 0.029\n",
      "model: 6, epoch: 365, loss: 0.029\n",
      "model: 6, epoch: 366, loss: 0.029\n",
      "model: 6, epoch: 367, loss: 0.029\n",
      "model: 6, epoch: 368, loss: 0.029\n",
      "model: 6, epoch: 369, loss: 0.029\n",
      "model: 6, epoch: 370, loss: 0.029\n",
      "model: 6, epoch: 371, loss: 0.029\n",
      "model: 6, epoch: 372, loss: 0.029\n",
      "model: 6, epoch: 373, loss: 0.029\n",
      "model: 6, epoch: 374, loss: 0.029\n",
      "model: 6, epoch: 375, loss: 0.029\n",
      "model: 6, epoch: 376, loss: 0.029\n",
      "model: 6, epoch: 377, loss: 0.029\n",
      "model: 6, epoch: 378, loss: 0.029\n",
      "model: 6, epoch: 379, loss: 0.029\n",
      "model: 6, epoch: 380, loss: 0.029\n",
      "model: 6, epoch: 381, loss: 0.029\n",
      "model: 6, epoch: 382, loss: 0.029\n",
      "model: 6, epoch: 383, loss: 0.029\n",
      "model: 6, epoch: 384, loss: 0.029\n",
      "model: 6, epoch: 385, loss: 0.029\n",
      "model: 6, epoch: 386, loss: 0.029\n",
      "model: 6, epoch: 387, loss: 0.029\n",
      "model: 6, epoch: 388, loss: 0.029\n",
      "model: 6, epoch: 389, loss: 0.029\n",
      "model: 6, epoch: 390, loss: 0.029\n",
      "model: 6, epoch: 391, loss: 0.029\n",
      "model: 6, epoch: 392, loss: 0.029\n",
      "model: 6, epoch: 393, loss: 0.029\n",
      "model: 6, epoch: 394, loss: 0.029\n",
      "model: 6, epoch: 395, loss: 0.029\n",
      "model: 6, epoch: 396, loss: 0.029\n",
      "model: 6, epoch: 397, loss: 0.029\n",
      "model: 6, epoch: 398, loss: 0.029\n",
      "model: 6, epoch: 399, loss: 0.029\n",
      "model: 6, epoch: 400, loss: 0.029\n",
      "model: 6, epoch: 401, loss: 0.029\n",
      "model: 6, epoch: 402, loss: 0.029\n",
      "model: 6, epoch: 403, loss: 0.029\n",
      "model: 6, epoch: 404, loss: 0.029\n",
      "model: 6, epoch: 405, loss: 0.029\n",
      "model: 6, epoch: 406, loss: 0.029\n",
      "model: 6, epoch: 407, loss: 0.029\n",
      "model: 6, epoch: 408, loss: 0.029\n",
      "model: 6, epoch: 409, loss: 0.029\n",
      "model: 6, epoch: 410, loss: 0.029\n",
      "model: 6, epoch: 411, loss: 0.029\n",
      "model: 6, epoch: 412, loss: 0.029\n",
      "model: 6, epoch: 413, loss: 0.029\n",
      "model: 6, epoch: 414, loss: 0.029\n",
      "model: 6, epoch: 415, loss: 0.029\n",
      "model: 6, epoch: 416, loss: 0.029\n",
      "model: 6, epoch: 417, loss: 0.029\n",
      "model: 6, epoch: 418, loss: 0.029\n",
      "model: 6, epoch: 419, loss: 0.029\n",
      "model: 6, epoch: 420, loss: 0.029\n",
      "model: 6, epoch: 421, loss: 0.029\n",
      "model: 6, epoch: 422, loss: 0.029\n",
      "model: 6, epoch: 423, loss: 0.029\n",
      "model: 6, epoch: 424, loss: 0.029\n",
      "model: 6, epoch: 425, loss: 0.029\n",
      "model: 6, epoch: 426, loss: 0.029\n",
      "model: 6, epoch: 427, loss: 0.029\n",
      "model: 6, epoch: 428, loss: 0.029\n",
      "model: 6, epoch: 429, loss: 0.029\n",
      "model: 6, epoch: 430, loss: 0.029\n",
      "model: 6, epoch: 431, loss: 0.029\n",
      "model: 6, epoch: 432, loss: 0.029\n",
      "model: 6, epoch: 433, loss: 0.029\n",
      "model: 6, epoch: 434, loss: 0.029\n",
      "model: 6, epoch: 435, loss: 0.029\n",
      "model: 6, epoch: 436, loss: 0.029\n",
      "model: 6, epoch: 437, loss: 0.029\n",
      "model: 6, epoch: 438, loss: 0.028\n",
      "model: 6, epoch: 439, loss: 0.028\n",
      "model: 6, epoch: 440, loss: 0.028\n",
      "model: 6, epoch: 441, loss: 0.028\n",
      "model: 6, epoch: 442, loss: 0.028\n",
      "model: 6, epoch: 443, loss: 0.028\n",
      "model: 6, epoch: 444, loss: 0.028\n",
      "model: 6, epoch: 445, loss: 0.028\n",
      "model: 6, epoch: 446, loss: 0.028\n",
      "model: 6, epoch: 447, loss: 0.028\n",
      "model: 6, epoch: 448, loss: 0.028\n",
      "model: 6, epoch: 449, loss: 0.028\n",
      "model: 6, epoch: 450, loss: 0.028\n",
      "model: 6, epoch: 451, loss: 0.028\n",
      "model: 6, epoch: 452, loss: 0.028\n",
      "model: 6, epoch: 453, loss: 0.028\n",
      "model: 6, epoch: 454, loss: 0.028\n",
      "model: 6, epoch: 455, loss: 0.028\n",
      "model: 6, epoch: 456, loss: 0.028\n",
      "model: 6, epoch: 457, loss: 0.028\n",
      "model: 6, epoch: 458, loss: 0.028\n",
      "model: 6, epoch: 459, loss: 0.028\n",
      "model: 6, epoch: 460, loss: 0.028\n",
      "model: 6, epoch: 461, loss: 0.028\n",
      "model: 6, epoch: 462, loss: 0.028\n",
      "model: 6, epoch: 463, loss: 0.028\n",
      "model: 6, epoch: 464, loss: 0.028\n",
      "model: 6, epoch: 465, loss: 0.028\n",
      "model: 6, epoch: 466, loss: 0.028\n",
      "model: 6, epoch: 467, loss: 0.028\n",
      "model: 6, epoch: 468, loss: 0.028\n",
      "model: 6, epoch: 469, loss: 0.028\n",
      "model: 6, epoch: 470, loss: 0.028\n",
      "model: 6, epoch: 471, loss: 0.028\n",
      "model: 6, epoch: 472, loss: 0.028\n",
      "model: 6, epoch: 473, loss: 0.028\n",
      "model: 6, epoch: 474, loss: 0.028\n",
      "model: 6, epoch: 475, loss: 0.028\n",
      "model: 6, epoch: 476, loss: 0.028\n",
      "model: 6, epoch: 477, loss: 0.028\n",
      "model: 6, epoch: 478, loss: 0.028\n",
      "model: 6, epoch: 479, loss: 0.028\n",
      "model: 6, epoch: 480, loss: 0.028\n",
      "model: 6, epoch: 481, loss: 0.028\n",
      "model: 6, epoch: 482, loss: 0.028\n",
      "model: 6, epoch: 483, loss: 0.028\n",
      "model: 6, epoch: 484, loss: 0.028\n",
      "model: 6, epoch: 485, loss: 0.028\n",
      "model: 6, epoch: 486, loss: 0.028\n",
      "model: 6, epoch: 487, loss: 0.028\n",
      "model: 6, epoch: 488, loss: 0.028\n",
      "model: 6, epoch: 489, loss: 0.028\n",
      "model: 6, epoch: 490, loss: 0.028\n",
      "model: 6, epoch: 491, loss: 0.028\n",
      "model: 6, epoch: 492, loss: 0.028\n",
      "model: 6, epoch: 493, loss: 0.028\n",
      "model: 6, epoch: 494, loss: 0.028\n",
      "model: 6, epoch: 495, loss: 0.028\n",
      "model: 6, epoch: 496, loss: 0.028\n",
      "model: 6, epoch: 497, loss: 0.028\n",
      "model: 6, epoch: 498, loss: 0.028\n",
      "model: 6, epoch: 499, loss: 0.028\n",
      "model: 6, epoch: 500, loss: 0.028\n",
      "model: 6, epoch: 501, loss: 0.028\n",
      "model: 6, epoch: 502, loss: 0.028\n",
      "model: 6, epoch: 503, loss: 0.028\n",
      "model: 6, epoch: 504, loss: 0.028\n",
      "model: 6, epoch: 505, loss: 0.028\n",
      "model: 6, epoch: 506, loss: 0.028\n",
      "model: 6, epoch: 507, loss: 0.028\n",
      "model: 6, epoch: 508, loss: 0.028\n",
      "model: 6, epoch: 509, loss: 0.028\n",
      "model: 6, epoch: 510, loss: 0.028\n",
      "model: 6, epoch: 511, loss: 0.028\n",
      "model: 6, epoch: 512, loss: 0.028\n",
      "model: 6, epoch: 513, loss: 0.028\n",
      "model: 6, epoch: 514, loss: 0.028\n",
      "model: 6, epoch: 515, loss: 0.028\n",
      "model: 6, epoch: 516, loss: 0.028\n",
      "model: 6, epoch: 517, loss: 0.028\n",
      "model: 6, epoch: 518, loss: 0.028\n",
      "model: 6, epoch: 519, loss: 0.028\n",
      "model: 6, epoch: 520, loss: 0.028\n",
      "model: 6, epoch: 521, loss: 0.028\n",
      "model: 6, epoch: 522, loss: 0.028\n",
      "model: 6, epoch: 523, loss: 0.028\n",
      "model: 6, epoch: 524, loss: 0.028\n",
      "model: 6, epoch: 525, loss: 0.028\n",
      "model: 6, epoch: 526, loss: 0.028\n",
      "model: 6, epoch: 527, loss: 0.028\n",
      "model: 6, epoch: 528, loss: 0.028\n",
      "model: 6, epoch: 529, loss: 0.028\n",
      "model: 6, epoch: 530, loss: 0.028\n",
      "model: 6, epoch: 531, loss: 0.028\n",
      "model: 6, epoch: 532, loss: 0.028\n",
      "model: 6, epoch: 533, loss: 0.028\n",
      "model: 6, epoch: 534, loss: 0.028\n",
      "model: 6, epoch: 535, loss: 0.028\n",
      "model: 6, epoch: 536, loss: 0.028\n",
      "model: 6, epoch: 537, loss: 0.028\n",
      "model: 6, epoch: 538, loss: 0.028\n",
      "model: 6, epoch: 539, loss: 0.028\n",
      "model: 6, epoch: 540, loss: 0.028\n",
      "model: 6, epoch: 541, loss: 0.028\n",
      "model: 6, epoch: 542, loss: 0.028\n",
      "model: 6, epoch: 543, loss: 0.028\n",
      "model: 6, epoch: 544, loss: 0.028\n",
      "model: 6, epoch: 545, loss: 0.028\n",
      "model: 6, epoch: 546, loss: 0.028\n",
      "model: 6, epoch: 547, loss: 0.028\n",
      "model: 6, epoch: 548, loss: 0.028\n",
      "model: 6, epoch: 549, loss: 0.028\n",
      "model: 6, epoch: 550, loss: 0.028\n",
      "model: 6, epoch: 551, loss: 0.028\n",
      "model: 6, epoch: 552, loss: 0.028\n",
      "model: 6, epoch: 553, loss: 0.028\n",
      "model: 6, epoch: 554, loss: 0.028\n",
      "model: 6, epoch: 555, loss: 0.028\n",
      "model: 6, epoch: 556, loss: 0.028\n",
      "model: 6, epoch: 557, loss: 0.028\n",
      "model: 6, epoch: 558, loss: 0.028\n",
      "model: 6, epoch: 559, loss: 0.028\n",
      "model: 6, epoch: 560, loss: 0.028\n",
      "model: 6, epoch: 561, loss: 0.028\n",
      "model: 6, epoch: 562, loss: 0.028\n",
      "model: 6, epoch: 563, loss: 0.028\n",
      "model: 6, epoch: 564, loss: 0.028\n",
      "model: 6, epoch: 565, loss: 0.028\n",
      "model: 6, epoch: 566, loss: 0.028\n",
      "model: 6, epoch: 567, loss: 0.028\n",
      "model: 6, epoch: 568, loss: 0.028\n",
      "model: 6, epoch: 569, loss: 0.028\n",
      "model: 6, epoch: 570, loss: 0.028\n",
      "model: 6, epoch: 571, loss: 0.028\n",
      "model: 6, epoch: 572, loss: 0.028\n",
      "model: 6, epoch: 573, loss: 0.028\n",
      "model: 6, epoch: 574, loss: 0.028\n",
      "model: 6, epoch: 575, loss: 0.028\n",
      "model: 6, epoch: 576, loss: 0.028\n",
      "model: 6, epoch: 577, loss: 0.028\n",
      "model: 6, epoch: 578, loss: 0.028\n",
      "model: 6, epoch: 579, loss: 0.028\n",
      "model: 6, epoch: 580, loss: 0.028\n",
      "model: 6, epoch: 581, loss: 0.028\n",
      "model: 6, epoch: 582, loss: 0.028\n",
      "model: 6, epoch: 583, loss: 0.028\n",
      "model: 6, epoch: 584, loss: 0.028\n",
      "model: 6, epoch: 585, loss: 0.028\n",
      "model: 6, epoch: 586, loss: 0.028\n",
      "model: 6, epoch: 587, loss: 0.028\n",
      "model: 6, epoch: 588, loss: 0.028\n",
      "model: 6, epoch: 589, loss: 0.028\n",
      "model: 6, epoch: 590, loss: 0.028\n",
      "model: 6, epoch: 591, loss: 0.028\n",
      "model: 6, epoch: 592, loss: 0.028\n",
      "model: 6, epoch: 593, loss: 0.028\n",
      "model: 6, epoch: 594, loss: 0.028\n",
      "model: 6, epoch: 595, loss: 0.028\n",
      "model: 6, epoch: 596, loss: 0.028\n",
      "model: 6, epoch: 597, loss: 0.028\n",
      "model: 6, epoch: 598, loss: 0.028\n",
      "model: 6, epoch: 599, loss: 0.028\n",
      "model: 6, epoch: 600, loss: 0.028\n",
      "model: 6, epoch: 601, loss: 0.028\n",
      "model: 6, epoch: 602, loss: 0.028\n",
      "model: 6, epoch: 603, loss: 0.028\n",
      "model: 6, epoch: 604, loss: 0.028\n",
      "model: 6, epoch: 605, loss: 0.028\n",
      "model: 6, epoch: 606, loss: 0.028\n",
      "model: 6, epoch: 607, loss: 0.028\n",
      "model: 6, epoch: 608, loss: 0.028\n",
      "model: 6, epoch: 609, loss: 0.028\n",
      "model: 6, epoch: 610, loss: 0.028\n",
      "model: 6, epoch: 611, loss: 0.028\n",
      "model: 6, epoch: 612, loss: 0.028\n",
      "model: 6, epoch: 613, loss: 0.028\n",
      "model: 6, epoch: 614, loss: 0.028\n",
      "model: 6, epoch: 615, loss: 0.028\n",
      "model: 6, epoch: 616, loss: 0.028\n",
      "model: 6, epoch: 617, loss: 0.028\n",
      "model: 6, epoch: 618, loss: 0.028\n",
      "model: 6, epoch: 619, loss: 0.028\n",
      "model: 6, epoch: 620, loss: 0.028\n",
      "model: 6, epoch: 621, loss: 0.028\n",
      "model: 6, epoch: 622, loss: 0.028\n",
      "model: 6, epoch: 623, loss: 0.028\n",
      "model: 6, epoch: 624, loss: 0.028\n",
      "model: 6, epoch: 625, loss: 0.028\n",
      "model: 6, epoch: 626, loss: 0.028\n",
      "model: 6, epoch: 627, loss: 0.028\n",
      "model: 6, epoch: 628, loss: 0.028\n",
      "model: 6, epoch: 629, loss: 0.028\n",
      "model: 6, epoch: 630, loss: 0.028\n",
      "model: 6, epoch: 631, loss: 0.028\n",
      "model: 6, epoch: 632, loss: 0.028\n",
      "model: 6, epoch: 633, loss: 0.028\n",
      "model: 6, epoch: 634, loss: 0.028\n",
      "model: 6, epoch: 635, loss: 0.028\n",
      "model: 6, epoch: 636, loss: 0.028\n",
      "model: 6, epoch: 637, loss: 0.028\n",
      "model: 6, epoch: 638, loss: 0.028\n",
      "model: 6, epoch: 639, loss: 0.028\n",
      "model: 6, epoch: 640, loss: 0.028\n",
      "model: 6, epoch: 641, loss: 0.028\n",
      "model: 6, epoch: 642, loss: 0.028\n",
      "model: 6, epoch: 643, loss: 0.028\n",
      "model: 6, epoch: 644, loss: 0.028\n",
      "model: 6, epoch: 645, loss: 0.028\n",
      "model: 6, epoch: 646, loss: 0.028\n",
      "model: 6, epoch: 647, loss: 0.028\n",
      "model: 6, epoch: 648, loss: 0.028\n",
      "model: 6, epoch: 649, loss: 0.028\n",
      "model: 6, epoch: 650, loss: 0.028\n",
      "model: 6, epoch: 651, loss: 0.028\n",
      "model: 6, epoch: 652, loss: 0.028\n",
      "model: 6, epoch: 653, loss: 0.028\n",
      "model: 6, epoch: 654, loss: 0.028\n",
      "model: 6, epoch: 655, loss: 0.028\n",
      "model: 6, epoch: 656, loss: 0.028\n",
      "model: 6, epoch: 657, loss: 0.028\n",
      "model: 6, epoch: 658, loss: 0.028\n",
      "model: 6, epoch: 659, loss: 0.028\n",
      "model: 6, epoch: 660, loss: 0.028\n",
      "model: 6, epoch: 661, loss: 0.028\n",
      "model: 6, epoch: 662, loss: 0.028\n",
      "model: 6, epoch: 663, loss: 0.028\n",
      "model: 6, epoch: 664, loss: 0.028\n",
      "model: 6, epoch: 665, loss: 0.028\n",
      "model: 6, epoch: 666, loss: 0.028\n",
      "model: 6, epoch: 667, loss: 0.028\n",
      "model: 6, epoch: 668, loss: 0.028\n",
      "model: 6, epoch: 669, loss: 0.028\n",
      "model: 6, epoch: 670, loss: 0.028\n",
      "model: 6, epoch: 671, loss: 0.028\n",
      "model: 6, epoch: 672, loss: 0.028\n",
      "model: 6, epoch: 673, loss: 0.028\n",
      "model: 6, epoch: 674, loss: 0.028\n",
      "model: 6, epoch: 675, loss: 0.028\n",
      "model: 6, epoch: 676, loss: 0.028\n",
      "model: 6, epoch: 677, loss: 0.028\n",
      "model: 6, epoch: 678, loss: 0.028\n",
      "model: 6, epoch: 679, loss: 0.028\n",
      "model: 6, epoch: 680, loss: 0.028\n",
      "model: 6, epoch: 681, loss: 0.028\n",
      "model: 6, epoch: 682, loss: 0.028\n",
      "model: 6, epoch: 683, loss: 0.028\n",
      "model: 6, epoch: 684, loss: 0.028\n",
      "model: 6, epoch: 685, loss: 0.028\n",
      "model: 6, epoch: 686, loss: 0.028\n",
      "model: 6, epoch: 687, loss: 0.028\n",
      "model: 6, epoch: 688, loss: 0.028\n",
      "model: 6, epoch: 689, loss: 0.028\n",
      "model: 6, epoch: 690, loss: 0.028\n",
      "model: 6, epoch: 691, loss: 0.028\n",
      "model: 6, epoch: 692, loss: 0.028\n",
      "model: 6, epoch: 693, loss: 0.028\n",
      "model: 6, epoch: 694, loss: 0.028\n",
      "model: 6, epoch: 695, loss: 0.028\n",
      "model: 6, epoch: 696, loss: 0.028\n",
      "model: 6, epoch: 697, loss: 0.028\n",
      "model: 6, epoch: 698, loss: 0.028\n",
      "model: 6, epoch: 699, loss: 0.028\n",
      "model: 6, epoch: 700, loss: 0.028\n",
      "model: 6, epoch: 701, loss: 0.028\n",
      "model: 6, epoch: 702, loss: 0.028\n",
      "model: 6, epoch: 703, loss: 0.028\n",
      "model: 6, epoch: 704, loss: 0.028\n",
      "model: 6, epoch: 705, loss: 0.028\n",
      "model: 6, epoch: 706, loss: 0.028\n",
      "model: 6, epoch: 707, loss: 0.028\n",
      "model: 6, epoch: 708, loss: 0.028\n",
      "model: 6, epoch: 709, loss: 0.028\n",
      "model: 6, epoch: 710, loss: 0.028\n",
      "model: 6, epoch: 711, loss: 0.028\n",
      "model: 6, epoch: 712, loss: 0.028\n",
      "model: 6, epoch: 713, loss: 0.028\n",
      "model: 6, epoch: 714, loss: 0.028\n",
      "model: 6, epoch: 715, loss: 0.028\n",
      "model: 6, epoch: 716, loss: 0.028\n",
      "model: 6, epoch: 717, loss: 0.028\n",
      "model: 6, epoch: 718, loss: 0.028\n",
      "model: 6, epoch: 719, loss: 0.028\n",
      "model: 6, epoch: 720, loss: 0.028\n",
      "model: 6, epoch: 721, loss: 0.028\n",
      "model: 6, epoch: 722, loss: 0.028\n",
      "model: 6, epoch: 723, loss: 0.028\n",
      "model: 6, epoch: 724, loss: 0.028\n",
      "model: 6, epoch: 725, loss: 0.028\n",
      "model: 6, epoch: 726, loss: 0.028\n",
      "model: 6, epoch: 727, loss: 0.028\n",
      "model: 6, epoch: 728, loss: 0.028\n",
      "model: 6, epoch: 729, loss: 0.028\n",
      "model: 6, epoch: 730, loss: 0.028\n",
      "model: 6, epoch: 731, loss: 0.028\n",
      "model: 6, epoch: 732, loss: 0.028\n",
      "model: 6, epoch: 733, loss: 0.028\n",
      "model: 6, epoch: 734, loss: 0.028\n",
      "model: 6, epoch: 735, loss: 0.028\n",
      "model: 6, epoch: 736, loss: 0.028\n",
      "model: 6, epoch: 737, loss: 0.028\n",
      "model: 6, epoch: 738, loss: 0.028\n",
      "model: 6, epoch: 739, loss: 0.028\n",
      "model: 6, epoch: 740, loss: 0.028\n",
      "model: 6, epoch: 741, loss: 0.028\n",
      "model: 6, epoch: 742, loss: 0.028\n",
      "model: 6, epoch: 743, loss: 0.028\n",
      "model: 6, epoch: 744, loss: 0.028\n",
      "model: 6, epoch: 745, loss: 0.028\n",
      "model: 6, epoch: 746, loss: 0.028\n",
      "model: 6, epoch: 747, loss: 0.028\n",
      "model: 6, epoch: 748, loss: 0.028\n",
      "model: 6, epoch: 749, loss: 0.028\n",
      "model: 6, epoch: 750, loss: 0.028\n",
      "model: 6, epoch: 751, loss: 0.028\n",
      "model: 6, epoch: 752, loss: 0.028\n",
      "model: 6, epoch: 753, loss: 0.028\n",
      "model: 6, epoch: 754, loss: 0.028\n",
      "model: 6, epoch: 755, loss: 0.028\n",
      "model: 6, epoch: 756, loss: 0.028\n",
      "model: 6, epoch: 757, loss: 0.028\n",
      "model: 6, epoch: 758, loss: 0.028\n",
      "model: 6, epoch: 759, loss: 0.028\n",
      "model: 6, epoch: 760, loss: 0.028\n",
      "model: 6, epoch: 761, loss: 0.028\n",
      "model: 6, epoch: 762, loss: 0.028\n",
      "model: 6, epoch: 763, loss: 0.028\n",
      "model: 6, epoch: 764, loss: 0.028\n",
      "model: 6, epoch: 765, loss: 0.028\n",
      "model: 6, epoch: 766, loss: 0.028\n",
      "model: 6, epoch: 767, loss: 0.028\n",
      "model: 6, epoch: 768, loss: 0.028\n",
      "model: 6, epoch: 769, loss: 0.028\n",
      "model: 6, epoch: 770, loss: 0.028\n",
      "model: 6, epoch: 771, loss: 0.028\n",
      "model: 6, epoch: 772, loss: 0.028\n",
      "model: 6, epoch: 773, loss: 0.028\n",
      "model: 6, epoch: 774, loss: 0.028\n",
      "model: 6, epoch: 775, loss: 0.028\n",
      "model: 6, epoch: 776, loss: 0.028\n",
      "model: 6, epoch: 777, loss: 0.028\n",
      "model: 6, epoch: 778, loss: 0.028\n",
      "model: 6, epoch: 779, loss: 0.028\n",
      "model: 6, epoch: 780, loss: 0.028\n",
      "model: 6, epoch: 781, loss: 0.028\n",
      "model: 6, epoch: 782, loss: 0.028\n",
      "model: 6, epoch: 783, loss: 0.028\n",
      "model: 6, epoch: 784, loss: 0.028\n",
      "model: 6, epoch: 785, loss: 0.028\n",
      "model: 6, epoch: 786, loss: 0.028\n",
      "model: 6, epoch: 787, loss: 0.028\n",
      "model: 6, epoch: 788, loss: 0.028\n",
      "model: 6, epoch: 789, loss: 0.028\n",
      "model: 6, epoch: 790, loss: 0.028\n",
      "model: 6, epoch: 791, loss: 0.028\n",
      "model: 6, epoch: 792, loss: 0.028\n",
      "model: 6, epoch: 793, loss: 0.028\n",
      "model: 6, epoch: 794, loss: 0.028\n",
      "model: 6, epoch: 795, loss: 0.028\n",
      "model: 6, epoch: 796, loss: 0.028\n",
      "model: 6, epoch: 797, loss: 0.028\n",
      "model: 6, epoch: 798, loss: 0.028\n",
      "model: 6, epoch: 799, loss: 0.028\n",
      "model: 6, epoch: 800, loss: 0.028\n",
      "model: 6, epoch: 801, loss: 0.028\n",
      "model: 6, epoch: 802, loss: 0.028\n",
      "model: 6, epoch: 803, loss: 0.028\n",
      "model: 6, epoch: 804, loss: 0.028\n",
      "model: 6, epoch: 805, loss: 0.028\n",
      "model: 6, epoch: 806, loss: 0.028\n",
      "model: 6, epoch: 807, loss: 0.028\n",
      "model: 6, epoch: 808, loss: 0.028\n",
      "model: 6, epoch: 809, loss: 0.028\n",
      "model: 6, epoch: 810, loss: 0.028\n",
      "model: 6, epoch: 811, loss: 0.028\n",
      "model: 6, epoch: 812, loss: 0.028\n",
      "model: 6, epoch: 813, loss: 0.028\n",
      "model: 6, epoch: 814, loss: 0.028\n",
      "model: 6, epoch: 815, loss: 0.028\n",
      "model: 6, epoch: 816, loss: 0.028\n",
      "model: 6, epoch: 817, loss: 0.028\n",
      "model: 6, epoch: 818, loss: 0.028\n",
      "model: 6, epoch: 819, loss: 0.028\n",
      "model: 6, epoch: 820, loss: 0.028\n",
      "model: 6, epoch: 821, loss: 0.028\n",
      "model: 6, epoch: 822, loss: 0.028\n",
      "model: 6, epoch: 823, loss: 0.028\n",
      "model: 6, epoch: 824, loss: 0.028\n",
      "model: 6, epoch: 825, loss: 0.028\n",
      "model: 6, epoch: 826, loss: 0.028\n",
      "model: 6, epoch: 827, loss: 0.028\n",
      "model: 6, epoch: 828, loss: 0.028\n",
      "model: 6, epoch: 829, loss: 0.028\n",
      "model: 6, epoch: 830, loss: 0.028\n",
      "model: 6, epoch: 831, loss: 0.028\n",
      "model: 6, epoch: 832, loss: 0.028\n",
      "model: 6, epoch: 833, loss: 0.028\n",
      "model: 6, epoch: 834, loss: 0.028\n",
      "model: 6, epoch: 835, loss: 0.028\n",
      "model: 6, epoch: 836, loss: 0.028\n",
      "model: 6, epoch: 837, loss: 0.028\n",
      "model: 6, epoch: 838, loss: 0.028\n",
      "model: 6, epoch: 839, loss: 0.028\n",
      "model: 6, epoch: 840, loss: 0.028\n",
      "model: 6, epoch: 841, loss: 0.028\n",
      "model: 6, epoch: 842, loss: 0.028\n",
      "model: 6, epoch: 843, loss: 0.028\n",
      "model: 6, epoch: 844, loss: 0.028\n",
      "model: 6, epoch: 845, loss: 0.028\n",
      "model: 6, epoch: 846, loss: 0.028\n",
      "model: 6, epoch: 847, loss: 0.028\n",
      "model: 6, epoch: 848, loss: 0.028\n",
      "model: 6, epoch: 849, loss: 0.028\n",
      "model: 6, epoch: 850, loss: 0.028\n",
      "model: 6, epoch: 851, loss: 0.028\n",
      "model: 6, epoch: 852, loss: 0.028\n",
      "model: 6, epoch: 853, loss: 0.028\n",
      "model: 6, epoch: 854, loss: 0.028\n",
      "model: 6, epoch: 855, loss: 0.028\n",
      "model: 6, epoch: 856, loss: 0.028\n",
      "model: 6, epoch: 857, loss: 0.028\n",
      "model: 6, epoch: 858, loss: 0.028\n",
      "model: 6, epoch: 859, loss: 0.028\n",
      "model: 6, epoch: 860, loss: 0.028\n",
      "model: 6, epoch: 861, loss: 0.028\n",
      "model: 6, epoch: 862, loss: 0.028\n",
      "model: 6, epoch: 863, loss: 0.028\n",
      "model: 6, epoch: 864, loss: 0.028\n",
      "model: 6, epoch: 865, loss: 0.028\n",
      "model: 6, epoch: 866, loss: 0.028\n",
      "model: 6, epoch: 867, loss: 0.028\n",
      "model: 6, epoch: 868, loss: 0.028\n",
      "model: 6, epoch: 869, loss: 0.028\n",
      "model: 6, epoch: 870, loss: 0.028\n",
      "model: 6, epoch: 871, loss: 0.028\n",
      "model: 6, epoch: 872, loss: 0.028\n",
      "model: 6, epoch: 873, loss: 0.028\n",
      "model: 6, epoch: 874, loss: 0.028\n",
      "model: 6, epoch: 875, loss: 0.028\n",
      "model: 6, epoch: 876, loss: 0.028\n",
      "model: 6, epoch: 877, loss: 0.028\n",
      "model: 6, epoch: 878, loss: 0.028\n",
      "model: 6, epoch: 879, loss: 0.028\n",
      "model: 6, epoch: 880, loss: 0.028\n",
      "model: 6, epoch: 881, loss: 0.028\n",
      "model: 6, epoch: 882, loss: 0.028\n",
      "model: 6, epoch: 883, loss: 0.028\n",
      "model: 6, epoch: 884, loss: 0.028\n",
      "model: 6, epoch: 885, loss: 0.028\n",
      "model: 6, epoch: 886, loss: 0.028\n",
      "model: 6, epoch: 887, loss: 0.028\n",
      "model: 6, epoch: 888, loss: 0.028\n",
      "model: 6, epoch: 889, loss: 0.028\n",
      "model: 6, epoch: 890, loss: 0.028\n",
      "model: 6, epoch: 891, loss: 0.028\n",
      "model: 6, epoch: 892, loss: 0.028\n",
      "model: 6, epoch: 893, loss: 0.028\n",
      "model: 6, epoch: 894, loss: 0.028\n",
      "model: 6, epoch: 895, loss: 0.028\n",
      "model: 6, epoch: 896, loss: 0.028\n",
      "model: 6, epoch: 897, loss: 0.028\n",
      "model: 6, epoch: 898, loss: 0.028\n",
      "model: 6, epoch: 899, loss: 0.028\n",
      "model: 6, epoch: 900, loss: 0.028\n",
      "model: 6, epoch: 901, loss: 0.028\n",
      "model: 6, epoch: 902, loss: 0.028\n",
      "model: 6, epoch: 903, loss: 0.028\n",
      "model: 6, epoch: 904, loss: 0.028\n",
      "model: 6, epoch: 905, loss: 0.028\n",
      "model: 6, epoch: 906, loss: 0.028\n",
      "model: 6, epoch: 907, loss: 0.028\n",
      "model: 6, epoch: 908, loss: 0.028\n",
      "model: 6, epoch: 909, loss: 0.028\n",
      "model: 6, epoch: 910, loss: 0.028\n",
      "model: 6, epoch: 911, loss: 0.028\n",
      "model: 6, epoch: 912, loss: 0.028\n",
      "model: 6, epoch: 913, loss: 0.028\n",
      "model: 6, epoch: 914, loss: 0.028\n",
      "model: 6, epoch: 915, loss: 0.028\n",
      "model: 6, epoch: 916, loss: 0.028\n",
      "model: 6, epoch: 917, loss: 0.028\n",
      "model: 6, epoch: 918, loss: 0.028\n",
      "model: 6, epoch: 919, loss: 0.028\n",
      "model: 6, epoch: 920, loss: 0.028\n",
      "model: 6, epoch: 921, loss: 0.028\n",
      "model: 6, epoch: 922, loss: 0.028\n",
      "model: 6, epoch: 923, loss: 0.028\n",
      "model: 6, epoch: 924, loss: 0.028\n",
      "model: 6, epoch: 925, loss: 0.028\n",
      "model: 6, epoch: 926, loss: 0.028\n",
      "model: 6, epoch: 927, loss: 0.028\n",
      "model: 6, epoch: 928, loss: 0.028\n",
      "model: 6, epoch: 929, loss: 0.028\n",
      "model: 6, epoch: 930, loss: 0.028\n",
      "model: 6, epoch: 931, loss: 0.028\n",
      "model: 6, epoch: 932, loss: 0.028\n",
      "model: 6, epoch: 933, loss: 0.028\n",
      "model: 6, epoch: 934, loss: 0.028\n",
      "model: 6, epoch: 935, loss: 0.028\n",
      "model: 6, epoch: 936, loss: 0.028\n",
      "model: 6, epoch: 937, loss: 0.028\n",
      "model: 6, epoch: 938, loss: 0.028\n",
      "model: 6, epoch: 939, loss: 0.028\n",
      "model: 6, epoch: 940, loss: 0.028\n",
      "model: 6, epoch: 941, loss: 0.028\n",
      "model: 6, epoch: 942, loss: 0.028\n",
      "model: 6, epoch: 943, loss: 0.028\n",
      "model: 6, epoch: 944, loss: 0.028\n",
      "model: 6, epoch: 945, loss: 0.028\n",
      "model: 6, epoch: 946, loss: 0.028\n",
      "model: 6, epoch: 947, loss: 0.028\n",
      "model: 6, epoch: 948, loss: 0.028\n",
      "model: 6, epoch: 949, loss: 0.028\n",
      "model: 6, epoch: 950, loss: 0.028\n",
      "model: 6, epoch: 951, loss: 0.028\n",
      "model: 6, epoch: 952, loss: 0.028\n",
      "model: 6, epoch: 953, loss: 0.028\n",
      "model: 6, epoch: 954, loss: 0.028\n",
      "model: 6, epoch: 955, loss: 0.028\n",
      "model: 6, epoch: 956, loss: 0.028\n",
      "model: 6, epoch: 957, loss: 0.028\n",
      "model: 6, epoch: 958, loss: 0.028\n",
      "model: 6, epoch: 959, loss: 0.028\n",
      "model: 6, epoch: 960, loss: 0.028\n",
      "model: 6, epoch: 961, loss: 0.028\n",
      "model: 6, epoch: 962, loss: 0.028\n",
      "model: 6, epoch: 963, loss: 0.028\n",
      "model: 6, epoch: 964, loss: 0.028\n",
      "model: 6, epoch: 965, loss: 0.028\n",
      "model: 6, epoch: 966, loss: 0.028\n",
      "model: 6, epoch: 967, loss: 0.028\n",
      "model: 6, epoch: 968, loss: 0.028\n",
      "model: 6, epoch: 969, loss: 0.028\n",
      "model: 6, epoch: 970, loss: 0.028\n",
      "model: 6, epoch: 971, loss: 0.028\n",
      "model: 6, epoch: 972, loss: 0.028\n",
      "model: 6, epoch: 973, loss: 0.028\n",
      "model: 6, epoch: 974, loss: 0.028\n",
      "model: 6, epoch: 975, loss: 0.028\n",
      "model: 6, epoch: 976, loss: 0.028\n",
      "model: 6, epoch: 977, loss: 0.028\n",
      "model: 6, epoch: 978, loss: 0.028\n",
      "model: 6, epoch: 979, loss: 0.028\n",
      "model: 6, epoch: 980, loss: 0.028\n",
      "model: 6, epoch: 981, loss: 0.028\n",
      "model: 6, epoch: 982, loss: 0.028\n",
      "model: 6, epoch: 983, loss: 0.028\n",
      "model: 6, epoch: 984, loss: 0.028\n",
      "model: 6, epoch: 985, loss: 0.028\n",
      "model: 6, epoch: 986, loss: 0.028\n",
      "model: 6, epoch: 987, loss: 0.028\n",
      "model: 6, epoch: 988, loss: 0.028\n",
      "model: 6, epoch: 989, loss: 0.028\n",
      "model: 6, epoch: 990, loss: 0.028\n",
      "model: 6, epoch: 991, loss: 0.028\n",
      "model: 6, epoch: 992, loss: 0.028\n",
      "model: 6, epoch: 993, loss: 0.028\n",
      "model: 6, epoch: 994, loss: 0.028\n",
      "model: 6, epoch: 995, loss: 0.028\n",
      "model: 6, epoch: 996, loss: 0.028\n",
      "model: 6, epoch: 997, loss: 0.028\n",
      "model: 6, epoch: 998, loss: 0.028\n",
      "model: 6, epoch: 999, loss: 0.028\n",
      "model: 7, epoch: 0, loss: 0.155\n",
      "model: 7, epoch: 1, loss: 0.144\n",
      "model: 7, epoch: 2, loss: 0.133\n",
      "model: 7, epoch: 3, loss: 0.123\n",
      "model: 7, epoch: 4, loss: 0.113\n",
      "model: 7, epoch: 5, loss: 0.105\n",
      "model: 7, epoch: 6, loss: 0.096\n",
      "model: 7, epoch: 7, loss: 0.089\n",
      "model: 7, epoch: 8, loss: 0.082\n",
      "model: 7, epoch: 9, loss: 0.075\n",
      "model: 7, epoch: 10, loss: 0.069\n",
      "model: 7, epoch: 11, loss: 0.064\n",
      "model: 7, epoch: 12, loss: 0.059\n",
      "model: 7, epoch: 13, loss: 0.055\n",
      "model: 7, epoch: 14, loss: 0.050\n",
      "model: 7, epoch: 15, loss: 0.047\n",
      "model: 7, epoch: 16, loss: 0.044\n",
      "model: 7, epoch: 17, loss: 0.041\n",
      "model: 7, epoch: 18, loss: 0.038\n",
      "model: 7, epoch: 19, loss: 0.036\n",
      "model: 7, epoch: 20, loss: 0.035\n",
      "model: 7, epoch: 21, loss: 0.033\n",
      "model: 7, epoch: 22, loss: 0.032\n",
      "model: 7, epoch: 23, loss: 0.031\n",
      "model: 7, epoch: 24, loss: 0.030\n",
      "model: 7, epoch: 25, loss: 0.030\n",
      "model: 7, epoch: 26, loss: 0.030\n",
      "model: 7, epoch: 27, loss: 0.029\n",
      "model: 7, epoch: 28, loss: 0.029\n",
      "model: 7, epoch: 29, loss: 0.029\n",
      "model: 7, epoch: 30, loss: 0.029\n",
      "model: 7, epoch: 31, loss: 0.029\n",
      "model: 7, epoch: 32, loss: 0.029\n",
      "model: 7, epoch: 33, loss: 0.029\n",
      "model: 7, epoch: 34, loss: 0.029\n",
      "model: 7, epoch: 35, loss: 0.030\n",
      "model: 7, epoch: 36, loss: 0.030\n",
      "model: 7, epoch: 37, loss: 0.030\n",
      "model: 7, epoch: 38, loss: 0.029\n",
      "model: 7, epoch: 39, loss: 0.029\n",
      "model: 7, epoch: 40, loss: 0.029\n",
      "model: 7, epoch: 41, loss: 0.029\n",
      "model: 7, epoch: 42, loss: 0.029\n",
      "model: 7, epoch: 43, loss: 0.029\n",
      "model: 7, epoch: 44, loss: 0.029\n",
      "model: 7, epoch: 45, loss: 0.028\n",
      "model: 7, epoch: 46, loss: 0.028\n",
      "model: 7, epoch: 47, loss: 0.028\n",
      "model: 7, epoch: 48, loss: 0.028\n",
      "model: 7, epoch: 49, loss: 0.027\n",
      "model: 7, epoch: 50, loss: 0.027\n",
      "model: 7, epoch: 51, loss: 0.027\n",
      "model: 7, epoch: 52, loss: 0.027\n",
      "model: 7, epoch: 53, loss: 0.026\n",
      "model: 7, epoch: 54, loss: 0.026\n",
      "model: 7, epoch: 55, loss: 0.026\n",
      "model: 7, epoch: 56, loss: 0.026\n",
      "model: 7, epoch: 57, loss: 0.026\n",
      "model: 7, epoch: 58, loss: 0.026\n",
      "model: 7, epoch: 59, loss: 0.025\n",
      "model: 7, epoch: 60, loss: 0.025\n",
      "model: 7, epoch: 61, loss: 0.025\n",
      "model: 7, epoch: 62, loss: 0.025\n",
      "model: 7, epoch: 63, loss: 0.025\n",
      "model: 7, epoch: 64, loss: 0.025\n",
      "model: 7, epoch: 65, loss: 0.025\n",
      "model: 7, epoch: 66, loss: 0.024\n",
      "model: 7, epoch: 67, loss: 0.024\n",
      "model: 7, epoch: 68, loss: 0.024\n",
      "model: 7, epoch: 69, loss: 0.024\n",
      "model: 7, epoch: 70, loss: 0.024\n",
      "model: 7, epoch: 71, loss: 0.024\n",
      "model: 7, epoch: 72, loss: 0.024\n",
      "model: 7, epoch: 73, loss: 0.023\n",
      "model: 7, epoch: 74, loss: 0.023\n",
      "model: 7, epoch: 75, loss: 0.023\n",
      "model: 7, epoch: 76, loss: 0.023\n",
      "model: 7, epoch: 77, loss: 0.023\n",
      "model: 7, epoch: 78, loss: 0.023\n",
      "model: 7, epoch: 79, loss: 0.022\n",
      "model: 7, epoch: 80, loss: 0.022\n",
      "model: 7, epoch: 81, loss: 0.022\n",
      "model: 7, epoch: 82, loss: 0.022\n",
      "model: 7, epoch: 83, loss: 0.022\n",
      "model: 7, epoch: 84, loss: 0.022\n",
      "model: 7, epoch: 85, loss: 0.022\n",
      "model: 7, epoch: 86, loss: 0.021\n",
      "model: 7, epoch: 87, loss: 0.021\n",
      "model: 7, epoch: 88, loss: 0.021\n",
      "model: 7, epoch: 89, loss: 0.021\n",
      "model: 7, epoch: 90, loss: 0.021\n",
      "model: 7, epoch: 91, loss: 0.021\n",
      "model: 7, epoch: 92, loss: 0.020\n",
      "model: 7, epoch: 93, loss: 0.020\n",
      "model: 7, epoch: 94, loss: 0.020\n",
      "model: 7, epoch: 95, loss: 0.020\n",
      "model: 7, epoch: 96, loss: 0.020\n",
      "model: 7, epoch: 97, loss: 0.020\n",
      "model: 7, epoch: 98, loss: 0.020\n",
      "model: 7, epoch: 99, loss: 0.019\n",
      "model: 7, epoch: 100, loss: 0.019\n",
      "model: 7, epoch: 101, loss: 0.019\n",
      "model: 7, epoch: 102, loss: 0.019\n",
      "model: 7, epoch: 103, loss: 0.019\n",
      "model: 7, epoch: 104, loss: 0.019\n",
      "model: 7, epoch: 105, loss: 0.018\n",
      "model: 7, epoch: 106, loss: 0.018\n",
      "model: 7, epoch: 107, loss: 0.018\n",
      "model: 7, epoch: 108, loss: 0.018\n",
      "model: 7, epoch: 109, loss: 0.018\n",
      "model: 7, epoch: 110, loss: 0.018\n",
      "model: 7, epoch: 111, loss: 0.018\n",
      "model: 7, epoch: 112, loss: 0.017\n",
      "model: 7, epoch: 113, loss: 0.017\n",
      "model: 7, epoch: 114, loss: 0.017\n",
      "model: 7, epoch: 115, loss: 0.017\n",
      "model: 7, epoch: 116, loss: 0.017\n",
      "model: 7, epoch: 117, loss: 0.017\n",
      "model: 7, epoch: 118, loss: 0.017\n",
      "model: 7, epoch: 119, loss: 0.017\n",
      "model: 7, epoch: 120, loss: 0.016\n",
      "model: 7, epoch: 121, loss: 0.016\n",
      "model: 7, epoch: 122, loss: 0.016\n",
      "model: 7, epoch: 123, loss: 0.016\n",
      "model: 7, epoch: 124, loss: 0.016\n",
      "model: 7, epoch: 125, loss: 0.016\n",
      "model: 7, epoch: 126, loss: 0.016\n",
      "model: 7, epoch: 127, loss: 0.015\n",
      "model: 7, epoch: 128, loss: 0.015\n",
      "model: 7, epoch: 129, loss: 0.015\n",
      "model: 7, epoch: 130, loss: 0.015\n",
      "model: 7, epoch: 131, loss: 0.015\n",
      "model: 7, epoch: 132, loss: 0.015\n",
      "model: 7, epoch: 133, loss: 0.015\n",
      "model: 7, epoch: 134, loss: 0.015\n",
      "model: 7, epoch: 135, loss: 0.015\n",
      "model: 7, epoch: 136, loss: 0.014\n",
      "model: 7, epoch: 137, loss: 0.014\n",
      "model: 7, epoch: 138, loss: 0.014\n",
      "model: 7, epoch: 139, loss: 0.014\n",
      "model: 7, epoch: 140, loss: 0.014\n",
      "model: 7, epoch: 141, loss: 0.014\n",
      "model: 7, epoch: 142, loss: 0.014\n",
      "model: 7, epoch: 143, loss: 0.014\n",
      "model: 7, epoch: 144, loss: 0.014\n",
      "model: 7, epoch: 145, loss: 0.014\n",
      "model: 7, epoch: 146, loss: 0.013\n",
      "model: 7, epoch: 147, loss: 0.013\n",
      "model: 7, epoch: 148, loss: 0.013\n",
      "model: 7, epoch: 149, loss: 0.013\n",
      "model: 7, epoch: 150, loss: 0.013\n",
      "model: 7, epoch: 151, loss: 0.013\n",
      "model: 7, epoch: 152, loss: 0.013\n",
      "model: 7, epoch: 153, loss: 0.013\n",
      "model: 7, epoch: 154, loss: 0.013\n",
      "model: 7, epoch: 155, loss: 0.013\n",
      "model: 7, epoch: 156, loss: 0.012\n",
      "model: 7, epoch: 157, loss: 0.012\n",
      "model: 7, epoch: 158, loss: 0.012\n",
      "model: 7, epoch: 159, loss: 0.012\n",
      "model: 7, epoch: 160, loss: 0.012\n",
      "model: 7, epoch: 161, loss: 0.012\n",
      "model: 7, epoch: 162, loss: 0.012\n",
      "model: 7, epoch: 163, loss: 0.012\n",
      "model: 7, epoch: 164, loss: 0.012\n",
      "model: 7, epoch: 165, loss: 0.012\n",
      "model: 7, epoch: 166, loss: 0.012\n",
      "model: 7, epoch: 167, loss: 0.012\n",
      "model: 7, epoch: 168, loss: 0.012\n",
      "model: 7, epoch: 169, loss: 0.011\n",
      "model: 7, epoch: 170, loss: 0.011\n",
      "model: 7, epoch: 171, loss: 0.011\n",
      "model: 7, epoch: 172, loss: 0.011\n",
      "model: 7, epoch: 173, loss: 0.011\n",
      "model: 7, epoch: 174, loss: 0.011\n",
      "model: 7, epoch: 175, loss: 0.011\n",
      "model: 7, epoch: 176, loss: 0.011\n",
      "model: 7, epoch: 177, loss: 0.011\n",
      "model: 7, epoch: 178, loss: 0.011\n",
      "model: 7, epoch: 179, loss: 0.011\n",
      "model: 7, epoch: 180, loss: 0.011\n",
      "model: 7, epoch: 181, loss: 0.011\n",
      "model: 7, epoch: 182, loss: 0.011\n",
      "model: 7, epoch: 183, loss: 0.011\n",
      "model: 7, epoch: 184, loss: 0.011\n",
      "model: 7, epoch: 185, loss: 0.010\n",
      "model: 7, epoch: 186, loss: 0.010\n",
      "model: 7, epoch: 187, loss: 0.010\n",
      "model: 7, epoch: 188, loss: 0.010\n",
      "model: 7, epoch: 189, loss: 0.010\n",
      "model: 7, epoch: 190, loss: 0.010\n",
      "model: 7, epoch: 191, loss: 0.010\n",
      "model: 7, epoch: 192, loss: 0.010\n",
      "model: 7, epoch: 193, loss: 0.010\n",
      "model: 7, epoch: 194, loss: 0.010\n",
      "model: 7, epoch: 195, loss: 0.010\n",
      "model: 7, epoch: 196, loss: 0.010\n",
      "model: 7, epoch: 197, loss: 0.010\n",
      "model: 7, epoch: 198, loss: 0.010\n",
      "model: 7, epoch: 199, loss: 0.010\n",
      "model: 7, epoch: 200, loss: 0.010\n",
      "model: 7, epoch: 201, loss: 0.010\n",
      "model: 7, epoch: 202, loss: 0.010\n",
      "model: 7, epoch: 203, loss: 0.010\n",
      "model: 7, epoch: 204, loss: 0.010\n",
      "model: 7, epoch: 205, loss: 0.010\n",
      "model: 7, epoch: 206, loss: 0.010\n",
      "model: 7, epoch: 207, loss: 0.009\n",
      "model: 7, epoch: 208, loss: 0.009\n",
      "model: 7, epoch: 209, loss: 0.009\n",
      "model: 7, epoch: 210, loss: 0.009\n",
      "model: 7, epoch: 211, loss: 0.009\n",
      "model: 7, epoch: 212, loss: 0.009\n",
      "model: 7, epoch: 213, loss: 0.009\n",
      "model: 7, epoch: 214, loss: 0.009\n",
      "model: 7, epoch: 215, loss: 0.009\n",
      "model: 7, epoch: 216, loss: 0.009\n",
      "model: 7, epoch: 217, loss: 0.009\n",
      "model: 7, epoch: 218, loss: 0.009\n",
      "model: 7, epoch: 219, loss: 0.009\n",
      "model: 7, epoch: 220, loss: 0.009\n",
      "model: 7, epoch: 221, loss: 0.009\n",
      "model: 7, epoch: 222, loss: 0.009\n",
      "model: 7, epoch: 223, loss: 0.009\n",
      "model: 7, epoch: 224, loss: 0.009\n",
      "model: 7, epoch: 225, loss: 0.009\n",
      "model: 7, epoch: 226, loss: 0.009\n",
      "model: 7, epoch: 227, loss: 0.009\n",
      "model: 7, epoch: 228, loss: 0.009\n",
      "model: 7, epoch: 229, loss: 0.009\n",
      "model: 7, epoch: 230, loss: 0.009\n",
      "model: 7, epoch: 231, loss: 0.009\n",
      "model: 7, epoch: 232, loss: 0.009\n",
      "model: 7, epoch: 233, loss: 0.009\n",
      "model: 7, epoch: 234, loss: 0.009\n",
      "model: 7, epoch: 235, loss: 0.009\n",
      "model: 7, epoch: 236, loss: 0.009\n",
      "model: 7, epoch: 237, loss: 0.009\n",
      "model: 7, epoch: 238, loss: 0.009\n",
      "model: 7, epoch: 239, loss: 0.009\n",
      "model: 7, epoch: 240, loss: 0.009\n",
      "model: 7, epoch: 241, loss: 0.009\n",
      "model: 7, epoch: 242, loss: 0.008\n",
      "model: 7, epoch: 243, loss: 0.008\n",
      "model: 7, epoch: 244, loss: 0.008\n",
      "model: 7, epoch: 245, loss: 0.008\n",
      "model: 7, epoch: 246, loss: 0.008\n",
      "model: 7, epoch: 247, loss: 0.008\n",
      "model: 7, epoch: 248, loss: 0.008\n",
      "model: 7, epoch: 249, loss: 0.008\n",
      "model: 7, epoch: 250, loss: 0.008\n",
      "model: 7, epoch: 251, loss: 0.008\n",
      "model: 7, epoch: 252, loss: 0.008\n",
      "model: 7, epoch: 253, loss: 0.008\n",
      "model: 7, epoch: 254, loss: 0.008\n",
      "model: 7, epoch: 255, loss: 0.008\n",
      "model: 7, epoch: 256, loss: 0.008\n",
      "model: 7, epoch: 257, loss: 0.008\n",
      "model: 7, epoch: 258, loss: 0.008\n",
      "model: 7, epoch: 259, loss: 0.008\n",
      "model: 7, epoch: 260, loss: 0.008\n",
      "model: 7, epoch: 261, loss: 0.008\n",
      "model: 7, epoch: 262, loss: 0.008\n",
      "model: 7, epoch: 263, loss: 0.008\n",
      "model: 7, epoch: 264, loss: 0.008\n",
      "model: 7, epoch: 265, loss: 0.008\n",
      "model: 7, epoch: 266, loss: 0.008\n",
      "model: 7, epoch: 267, loss: 0.008\n",
      "model: 7, epoch: 268, loss: 0.008\n",
      "model: 7, epoch: 269, loss: 0.008\n",
      "model: 7, epoch: 270, loss: 0.008\n",
      "model: 7, epoch: 271, loss: 0.008\n",
      "model: 7, epoch: 272, loss: 0.008\n",
      "model: 7, epoch: 273, loss: 0.008\n",
      "model: 7, epoch: 274, loss: 0.008\n",
      "model: 7, epoch: 275, loss: 0.008\n",
      "model: 7, epoch: 276, loss: 0.008\n",
      "model: 7, epoch: 277, loss: 0.008\n",
      "model: 7, epoch: 278, loss: 0.008\n",
      "model: 7, epoch: 279, loss: 0.008\n",
      "model: 7, epoch: 280, loss: 0.008\n",
      "model: 7, epoch: 281, loss: 0.008\n",
      "model: 7, epoch: 282, loss: 0.008\n",
      "model: 7, epoch: 283, loss: 0.008\n",
      "model: 7, epoch: 284, loss: 0.008\n",
      "model: 7, epoch: 285, loss: 0.008\n",
      "model: 7, epoch: 286, loss: 0.008\n",
      "model: 7, epoch: 287, loss: 0.008\n",
      "model: 7, epoch: 288, loss: 0.008\n",
      "model: 7, epoch: 289, loss: 0.008\n",
      "model: 7, epoch: 290, loss: 0.008\n",
      "model: 7, epoch: 291, loss: 0.008\n",
      "model: 7, epoch: 292, loss: 0.008\n",
      "model: 7, epoch: 293, loss: 0.008\n",
      "model: 7, epoch: 294, loss: 0.008\n",
      "model: 7, epoch: 295, loss: 0.008\n",
      "model: 7, epoch: 296, loss: 0.008\n",
      "model: 7, epoch: 297, loss: 0.008\n",
      "model: 7, epoch: 298, loss: 0.008\n",
      "model: 7, epoch: 299, loss: 0.008\n",
      "model: 7, epoch: 300, loss: 0.008\n",
      "model: 7, epoch: 301, loss: 0.008\n",
      "model: 7, epoch: 302, loss: 0.008\n",
      "model: 7, epoch: 303, loss: 0.008\n",
      "model: 7, epoch: 304, loss: 0.008\n",
      "model: 7, epoch: 305, loss: 0.008\n",
      "model: 7, epoch: 306, loss: 0.008\n",
      "model: 7, epoch: 307, loss: 0.008\n",
      "model: 7, epoch: 308, loss: 0.008\n",
      "model: 7, epoch: 309, loss: 0.008\n",
      "model: 7, epoch: 310, loss: 0.008\n",
      "model: 7, epoch: 311, loss: 0.008\n",
      "model: 7, epoch: 312, loss: 0.008\n",
      "model: 7, epoch: 313, loss: 0.008\n",
      "model: 7, epoch: 314, loss: 0.008\n",
      "model: 7, epoch: 315, loss: 0.008\n",
      "model: 7, epoch: 316, loss: 0.008\n",
      "model: 7, epoch: 317, loss: 0.008\n",
      "model: 7, epoch: 318, loss: 0.008\n",
      "model: 7, epoch: 319, loss: 0.008\n",
      "model: 7, epoch: 320, loss: 0.008\n",
      "model: 7, epoch: 321, loss: 0.008\n",
      "model: 7, epoch: 322, loss: 0.008\n",
      "model: 7, epoch: 323, loss: 0.008\n",
      "model: 7, epoch: 324, loss: 0.008\n",
      "model: 7, epoch: 325, loss: 0.007\n",
      "model: 7, epoch: 326, loss: 0.007\n",
      "model: 7, epoch: 327, loss: 0.007\n",
      "model: 7, epoch: 328, loss: 0.007\n",
      "model: 7, epoch: 329, loss: 0.007\n",
      "model: 7, epoch: 330, loss: 0.007\n",
      "model: 7, epoch: 331, loss: 0.007\n",
      "model: 7, epoch: 332, loss: 0.007\n",
      "model: 7, epoch: 333, loss: 0.007\n",
      "model: 7, epoch: 334, loss: 0.007\n",
      "model: 7, epoch: 335, loss: 0.007\n",
      "model: 7, epoch: 336, loss: 0.007\n",
      "model: 7, epoch: 337, loss: 0.007\n",
      "model: 7, epoch: 338, loss: 0.007\n",
      "model: 7, epoch: 339, loss: 0.007\n",
      "model: 7, epoch: 340, loss: 0.007\n",
      "model: 7, epoch: 341, loss: 0.007\n",
      "model: 7, epoch: 342, loss: 0.007\n",
      "model: 7, epoch: 343, loss: 0.007\n",
      "model: 7, epoch: 344, loss: 0.007\n",
      "model: 7, epoch: 345, loss: 0.007\n",
      "model: 7, epoch: 346, loss: 0.007\n",
      "model: 7, epoch: 347, loss: 0.007\n",
      "model: 7, epoch: 348, loss: 0.007\n",
      "model: 7, epoch: 349, loss: 0.007\n",
      "model: 7, epoch: 350, loss: 0.007\n",
      "model: 7, epoch: 351, loss: 0.007\n",
      "model: 7, epoch: 352, loss: 0.007\n",
      "model: 7, epoch: 353, loss: 0.007\n",
      "model: 7, epoch: 354, loss: 0.007\n",
      "model: 7, epoch: 355, loss: 0.007\n",
      "model: 7, epoch: 356, loss: 0.007\n",
      "model: 7, epoch: 357, loss: 0.007\n",
      "model: 7, epoch: 358, loss: 0.007\n",
      "model: 7, epoch: 359, loss: 0.007\n",
      "model: 7, epoch: 360, loss: 0.007\n",
      "model: 7, epoch: 361, loss: 0.007\n",
      "model: 7, epoch: 362, loss: 0.007\n",
      "model: 7, epoch: 363, loss: 0.007\n",
      "model: 7, epoch: 364, loss: 0.007\n",
      "model: 7, epoch: 365, loss: 0.007\n",
      "model: 7, epoch: 366, loss: 0.007\n",
      "model: 7, epoch: 367, loss: 0.007\n",
      "model: 7, epoch: 368, loss: 0.007\n",
      "model: 7, epoch: 369, loss: 0.007\n",
      "model: 7, epoch: 370, loss: 0.007\n",
      "model: 7, epoch: 371, loss: 0.007\n",
      "model: 7, epoch: 372, loss: 0.007\n",
      "model: 7, epoch: 373, loss: 0.007\n",
      "model: 7, epoch: 374, loss: 0.007\n",
      "model: 7, epoch: 375, loss: 0.007\n",
      "model: 7, epoch: 376, loss: 0.007\n",
      "model: 7, epoch: 377, loss: 0.007\n",
      "model: 7, epoch: 378, loss: 0.007\n",
      "model: 7, epoch: 379, loss: 0.007\n",
      "model: 7, epoch: 380, loss: 0.007\n",
      "model: 7, epoch: 381, loss: 0.007\n",
      "model: 7, epoch: 382, loss: 0.007\n",
      "model: 7, epoch: 383, loss: 0.007\n",
      "model: 7, epoch: 384, loss: 0.007\n",
      "model: 7, epoch: 385, loss: 0.007\n",
      "model: 7, epoch: 386, loss: 0.007\n",
      "model: 7, epoch: 387, loss: 0.007\n",
      "model: 7, epoch: 388, loss: 0.007\n",
      "model: 7, epoch: 389, loss: 0.007\n",
      "model: 7, epoch: 390, loss: 0.007\n",
      "model: 7, epoch: 391, loss: 0.007\n",
      "model: 7, epoch: 392, loss: 0.007\n",
      "model: 7, epoch: 393, loss: 0.007\n",
      "model: 7, epoch: 394, loss: 0.007\n",
      "model: 7, epoch: 395, loss: 0.007\n",
      "model: 7, epoch: 396, loss: 0.007\n",
      "model: 7, epoch: 397, loss: 0.007\n",
      "model: 7, epoch: 398, loss: 0.007\n",
      "model: 7, epoch: 399, loss: 0.007\n",
      "model: 7, epoch: 400, loss: 0.007\n",
      "model: 7, epoch: 401, loss: 0.007\n",
      "model: 7, epoch: 402, loss: 0.007\n",
      "model: 7, epoch: 403, loss: 0.007\n",
      "model: 7, epoch: 404, loss: 0.007\n",
      "model: 7, epoch: 405, loss: 0.007\n",
      "model: 7, epoch: 406, loss: 0.007\n",
      "model: 7, epoch: 407, loss: 0.007\n",
      "model: 7, epoch: 408, loss: 0.007\n",
      "model: 7, epoch: 409, loss: 0.007\n",
      "model: 7, epoch: 410, loss: 0.007\n",
      "model: 7, epoch: 411, loss: 0.007\n",
      "model: 7, epoch: 412, loss: 0.007\n",
      "model: 7, epoch: 413, loss: 0.007\n",
      "model: 7, epoch: 414, loss: 0.007\n",
      "model: 7, epoch: 415, loss: 0.007\n",
      "model: 7, epoch: 416, loss: 0.007\n",
      "model: 7, epoch: 417, loss: 0.007\n",
      "model: 7, epoch: 418, loss: 0.007\n",
      "model: 7, epoch: 419, loss: 0.007\n",
      "model: 7, epoch: 420, loss: 0.007\n",
      "model: 7, epoch: 421, loss: 0.007\n",
      "model: 7, epoch: 422, loss: 0.007\n",
      "model: 7, epoch: 423, loss: 0.007\n",
      "model: 7, epoch: 424, loss: 0.007\n",
      "model: 7, epoch: 425, loss: 0.007\n",
      "model: 7, epoch: 426, loss: 0.007\n",
      "model: 7, epoch: 427, loss: 0.007\n",
      "model: 7, epoch: 428, loss: 0.007\n",
      "model: 7, epoch: 429, loss: 0.007\n",
      "model: 7, epoch: 430, loss: 0.007\n",
      "model: 7, epoch: 431, loss: 0.007\n",
      "model: 7, epoch: 432, loss: 0.007\n",
      "model: 7, epoch: 433, loss: 0.007\n",
      "model: 7, epoch: 434, loss: 0.007\n",
      "model: 7, epoch: 435, loss: 0.007\n",
      "model: 7, epoch: 436, loss: 0.007\n",
      "model: 7, epoch: 437, loss: 0.007\n",
      "model: 7, epoch: 438, loss: 0.007\n",
      "model: 7, epoch: 439, loss: 0.007\n",
      "model: 7, epoch: 440, loss: 0.007\n",
      "model: 7, epoch: 441, loss: 0.007\n",
      "model: 7, epoch: 442, loss: 0.007\n",
      "model: 7, epoch: 443, loss: 0.007\n",
      "model: 7, epoch: 444, loss: 0.007\n",
      "model: 7, epoch: 445, loss: 0.007\n",
      "model: 7, epoch: 446, loss: 0.007\n",
      "model: 7, epoch: 447, loss: 0.007\n",
      "model: 7, epoch: 448, loss: 0.007\n",
      "model: 7, epoch: 449, loss: 0.007\n",
      "model: 7, epoch: 450, loss: 0.007\n",
      "model: 7, epoch: 451, loss: 0.007\n",
      "model: 7, epoch: 452, loss: 0.007\n",
      "model: 7, epoch: 453, loss: 0.007\n",
      "model: 7, epoch: 454, loss: 0.007\n",
      "model: 7, epoch: 455, loss: 0.007\n",
      "model: 7, epoch: 456, loss: 0.007\n",
      "model: 7, epoch: 457, loss: 0.007\n",
      "model: 7, epoch: 458, loss: 0.007\n",
      "model: 7, epoch: 459, loss: 0.007\n",
      "model: 7, epoch: 460, loss: 0.007\n",
      "model: 7, epoch: 461, loss: 0.007\n",
      "model: 7, epoch: 462, loss: 0.007\n",
      "model: 7, epoch: 463, loss: 0.007\n",
      "model: 7, epoch: 464, loss: 0.007\n",
      "model: 7, epoch: 465, loss: 0.007\n",
      "model: 7, epoch: 466, loss: 0.007\n",
      "model: 7, epoch: 467, loss: 0.007\n",
      "model: 7, epoch: 468, loss: 0.007\n",
      "model: 7, epoch: 469, loss: 0.007\n",
      "model: 7, epoch: 470, loss: 0.007\n",
      "model: 7, epoch: 471, loss: 0.007\n",
      "model: 7, epoch: 472, loss: 0.007\n",
      "model: 7, epoch: 473, loss: 0.007\n",
      "model: 7, epoch: 474, loss: 0.007\n",
      "model: 7, epoch: 475, loss: 0.007\n",
      "model: 7, epoch: 476, loss: 0.007\n",
      "model: 7, epoch: 477, loss: 0.007\n",
      "model: 7, epoch: 478, loss: 0.007\n",
      "model: 7, epoch: 479, loss: 0.007\n",
      "model: 7, epoch: 480, loss: 0.007\n",
      "model: 7, epoch: 481, loss: 0.007\n",
      "model: 7, epoch: 482, loss: 0.007\n",
      "model: 7, epoch: 483, loss: 0.007\n",
      "model: 7, epoch: 484, loss: 0.007\n",
      "model: 7, epoch: 485, loss: 0.007\n",
      "model: 7, epoch: 486, loss: 0.007\n",
      "model: 7, epoch: 487, loss: 0.007\n",
      "model: 7, epoch: 488, loss: 0.007\n",
      "model: 7, epoch: 489, loss: 0.007\n",
      "model: 7, epoch: 490, loss: 0.007\n",
      "model: 7, epoch: 491, loss: 0.007\n",
      "model: 7, epoch: 492, loss: 0.007\n",
      "model: 7, epoch: 493, loss: 0.007\n",
      "model: 7, epoch: 494, loss: 0.007\n",
      "model: 7, epoch: 495, loss: 0.007\n",
      "model: 7, epoch: 496, loss: 0.007\n",
      "model: 7, epoch: 497, loss: 0.007\n",
      "model: 7, epoch: 498, loss: 0.007\n",
      "model: 7, epoch: 499, loss: 0.007\n",
      "model: 7, epoch: 500, loss: 0.007\n",
      "model: 7, epoch: 501, loss: 0.007\n",
      "model: 7, epoch: 502, loss: 0.007\n",
      "model: 7, epoch: 503, loss: 0.007\n",
      "model: 7, epoch: 504, loss: 0.007\n",
      "model: 7, epoch: 505, loss: 0.007\n",
      "model: 7, epoch: 506, loss: 0.007\n",
      "model: 7, epoch: 507, loss: 0.007\n",
      "model: 7, epoch: 508, loss: 0.007\n",
      "model: 7, epoch: 509, loss: 0.007\n",
      "model: 7, epoch: 510, loss: 0.007\n",
      "model: 7, epoch: 511, loss: 0.007\n",
      "model: 7, epoch: 512, loss: 0.007\n",
      "model: 7, epoch: 513, loss: 0.007\n",
      "model: 7, epoch: 514, loss: 0.007\n",
      "model: 7, epoch: 515, loss: 0.007\n",
      "model: 7, epoch: 516, loss: 0.007\n",
      "model: 7, epoch: 517, loss: 0.007\n",
      "model: 7, epoch: 518, loss: 0.007\n",
      "model: 7, epoch: 519, loss: 0.007\n",
      "model: 7, epoch: 520, loss: 0.007\n",
      "model: 7, epoch: 521, loss: 0.007\n",
      "model: 7, epoch: 522, loss: 0.007\n",
      "model: 7, epoch: 523, loss: 0.007\n",
      "model: 7, epoch: 524, loss: 0.007\n",
      "model: 7, epoch: 525, loss: 0.007\n",
      "model: 7, epoch: 526, loss: 0.007\n",
      "model: 7, epoch: 527, loss: 0.007\n",
      "model: 7, epoch: 528, loss: 0.007\n",
      "model: 7, epoch: 529, loss: 0.007\n",
      "model: 7, epoch: 530, loss: 0.007\n",
      "model: 7, epoch: 531, loss: 0.007\n",
      "model: 7, epoch: 532, loss: 0.007\n",
      "model: 7, epoch: 533, loss: 0.007\n",
      "model: 7, epoch: 534, loss: 0.007\n",
      "model: 7, epoch: 535, loss: 0.007\n",
      "model: 7, epoch: 536, loss: 0.007\n",
      "model: 7, epoch: 537, loss: 0.007\n",
      "model: 7, epoch: 538, loss: 0.007\n",
      "model: 7, epoch: 539, loss: 0.007\n",
      "model: 7, epoch: 540, loss: 0.007\n",
      "model: 7, epoch: 541, loss: 0.007\n",
      "model: 7, epoch: 542, loss: 0.007\n",
      "model: 7, epoch: 543, loss: 0.007\n",
      "model: 7, epoch: 544, loss: 0.006\n",
      "model: 7, epoch: 545, loss: 0.006\n",
      "model: 7, epoch: 546, loss: 0.006\n",
      "model: 7, epoch: 547, loss: 0.006\n",
      "model: 7, epoch: 548, loss: 0.006\n",
      "model: 7, epoch: 549, loss: 0.006\n",
      "model: 7, epoch: 550, loss: 0.006\n",
      "model: 7, epoch: 551, loss: 0.006\n",
      "model: 7, epoch: 552, loss: 0.006\n",
      "model: 7, epoch: 553, loss: 0.006\n",
      "model: 7, epoch: 554, loss: 0.006\n",
      "model: 7, epoch: 555, loss: 0.006\n",
      "model: 7, epoch: 556, loss: 0.006\n",
      "model: 7, epoch: 557, loss: 0.006\n",
      "model: 7, epoch: 558, loss: 0.006\n",
      "model: 7, epoch: 559, loss: 0.006\n",
      "model: 7, epoch: 560, loss: 0.006\n",
      "model: 7, epoch: 561, loss: 0.006\n",
      "model: 7, epoch: 562, loss: 0.006\n",
      "model: 7, epoch: 563, loss: 0.006\n",
      "model: 7, epoch: 564, loss: 0.006\n",
      "model: 7, epoch: 565, loss: 0.006\n",
      "model: 7, epoch: 566, loss: 0.006\n",
      "model: 7, epoch: 567, loss: 0.006\n",
      "model: 7, epoch: 568, loss: 0.006\n",
      "model: 7, epoch: 569, loss: 0.006\n",
      "model: 7, epoch: 570, loss: 0.006\n",
      "model: 7, epoch: 571, loss: 0.006\n",
      "model: 7, epoch: 572, loss: 0.006\n",
      "model: 7, epoch: 573, loss: 0.006\n",
      "model: 7, epoch: 574, loss: 0.006\n",
      "model: 7, epoch: 575, loss: 0.006\n",
      "model: 7, epoch: 576, loss: 0.006\n",
      "model: 7, epoch: 577, loss: 0.006\n",
      "model: 7, epoch: 578, loss: 0.006\n",
      "model: 7, epoch: 579, loss: 0.006\n",
      "model: 7, epoch: 580, loss: 0.006\n",
      "model: 7, epoch: 581, loss: 0.006\n",
      "model: 7, epoch: 582, loss: 0.006\n",
      "model: 7, epoch: 583, loss: 0.006\n",
      "model: 7, epoch: 584, loss: 0.006\n",
      "model: 7, epoch: 585, loss: 0.006\n",
      "model: 7, epoch: 586, loss: 0.006\n",
      "model: 7, epoch: 587, loss: 0.006\n",
      "model: 7, epoch: 588, loss: 0.006\n",
      "model: 7, epoch: 589, loss: 0.006\n",
      "model: 7, epoch: 590, loss: 0.006\n",
      "model: 7, epoch: 591, loss: 0.006\n",
      "model: 7, epoch: 592, loss: 0.006\n",
      "model: 7, epoch: 593, loss: 0.006\n",
      "model: 7, epoch: 594, loss: 0.006\n",
      "model: 7, epoch: 595, loss: 0.006\n",
      "model: 7, epoch: 596, loss: 0.006\n",
      "model: 7, epoch: 597, loss: 0.006\n",
      "model: 7, epoch: 598, loss: 0.006\n",
      "model: 7, epoch: 599, loss: 0.006\n",
      "model: 7, epoch: 600, loss: 0.006\n",
      "model: 7, epoch: 601, loss: 0.006\n",
      "model: 7, epoch: 602, loss: 0.006\n",
      "model: 7, epoch: 603, loss: 0.006\n",
      "model: 7, epoch: 604, loss: 0.006\n",
      "model: 7, epoch: 605, loss: 0.006\n",
      "model: 7, epoch: 606, loss: 0.006\n",
      "model: 7, epoch: 607, loss: 0.006\n",
      "model: 7, epoch: 608, loss: 0.006\n",
      "model: 7, epoch: 609, loss: 0.006\n",
      "model: 7, epoch: 610, loss: 0.006\n",
      "model: 7, epoch: 611, loss: 0.006\n",
      "model: 7, epoch: 612, loss: 0.006\n",
      "model: 7, epoch: 613, loss: 0.006\n",
      "model: 7, epoch: 614, loss: 0.006\n",
      "model: 7, epoch: 615, loss: 0.006\n",
      "model: 7, epoch: 616, loss: 0.006\n",
      "model: 7, epoch: 617, loss: 0.006\n",
      "model: 7, epoch: 618, loss: 0.006\n",
      "model: 7, epoch: 619, loss: 0.006\n",
      "model: 7, epoch: 620, loss: 0.006\n",
      "model: 7, epoch: 621, loss: 0.006\n",
      "model: 7, epoch: 622, loss: 0.006\n",
      "model: 7, epoch: 623, loss: 0.006\n",
      "model: 7, epoch: 624, loss: 0.006\n",
      "model: 7, epoch: 625, loss: 0.006\n",
      "model: 7, epoch: 626, loss: 0.006\n",
      "model: 7, epoch: 627, loss: 0.006\n",
      "model: 7, epoch: 628, loss: 0.006\n",
      "model: 7, epoch: 629, loss: 0.006\n",
      "model: 7, epoch: 630, loss: 0.006\n",
      "model: 7, epoch: 631, loss: 0.006\n",
      "model: 7, epoch: 632, loss: 0.006\n",
      "model: 7, epoch: 633, loss: 0.006\n",
      "model: 7, epoch: 634, loss: 0.006\n",
      "model: 7, epoch: 635, loss: 0.006\n",
      "model: 7, epoch: 636, loss: 0.006\n",
      "model: 7, epoch: 637, loss: 0.006\n",
      "model: 7, epoch: 638, loss: 0.006\n",
      "model: 7, epoch: 639, loss: 0.006\n",
      "model: 7, epoch: 640, loss: 0.006\n",
      "model: 7, epoch: 641, loss: 0.006\n",
      "model: 7, epoch: 642, loss: 0.006\n",
      "model: 7, epoch: 643, loss: 0.006\n",
      "model: 7, epoch: 644, loss: 0.006\n",
      "model: 7, epoch: 645, loss: 0.006\n",
      "model: 7, epoch: 646, loss: 0.006\n",
      "model: 7, epoch: 647, loss: 0.006\n",
      "model: 7, epoch: 648, loss: 0.006\n",
      "model: 7, epoch: 649, loss: 0.006\n",
      "model: 7, epoch: 650, loss: 0.006\n",
      "model: 7, epoch: 651, loss: 0.006\n",
      "model: 7, epoch: 652, loss: 0.006\n",
      "model: 7, epoch: 653, loss: 0.006\n",
      "model: 7, epoch: 654, loss: 0.006\n",
      "model: 7, epoch: 655, loss: 0.006\n",
      "model: 7, epoch: 656, loss: 0.006\n",
      "model: 7, epoch: 657, loss: 0.006\n",
      "model: 7, epoch: 658, loss: 0.006\n",
      "model: 7, epoch: 659, loss: 0.006\n",
      "model: 7, epoch: 660, loss: 0.006\n",
      "model: 7, epoch: 661, loss: 0.006\n",
      "model: 7, epoch: 662, loss: 0.006\n",
      "model: 7, epoch: 663, loss: 0.006\n",
      "model: 7, epoch: 664, loss: 0.006\n",
      "model: 7, epoch: 665, loss: 0.006\n",
      "model: 7, epoch: 666, loss: 0.006\n",
      "model: 7, epoch: 667, loss: 0.006\n",
      "model: 7, epoch: 668, loss: 0.006\n",
      "model: 7, epoch: 669, loss: 0.006\n",
      "model: 7, epoch: 670, loss: 0.006\n",
      "model: 7, epoch: 671, loss: 0.006\n",
      "model: 7, epoch: 672, loss: 0.006\n",
      "model: 7, epoch: 673, loss: 0.006\n",
      "model: 7, epoch: 674, loss: 0.006\n",
      "model: 7, epoch: 675, loss: 0.006\n",
      "model: 7, epoch: 676, loss: 0.006\n",
      "model: 7, epoch: 677, loss: 0.006\n",
      "model: 7, epoch: 678, loss: 0.006\n",
      "model: 7, epoch: 679, loss: 0.006\n",
      "model: 7, epoch: 680, loss: 0.006\n",
      "model: 7, epoch: 681, loss: 0.006\n",
      "model: 7, epoch: 682, loss: 0.006\n",
      "model: 7, epoch: 683, loss: 0.006\n",
      "model: 7, epoch: 684, loss: 0.006\n",
      "model: 7, epoch: 685, loss: 0.006\n",
      "model: 7, epoch: 686, loss: 0.006\n",
      "model: 7, epoch: 687, loss: 0.006\n",
      "model: 7, epoch: 688, loss: 0.006\n",
      "model: 7, epoch: 689, loss: 0.006\n",
      "model: 7, epoch: 690, loss: 0.006\n",
      "model: 7, epoch: 691, loss: 0.006\n",
      "model: 7, epoch: 692, loss: 0.006\n",
      "model: 7, epoch: 693, loss: 0.006\n",
      "model: 7, epoch: 694, loss: 0.006\n",
      "model: 7, epoch: 695, loss: 0.006\n",
      "model: 7, epoch: 696, loss: 0.006\n",
      "model: 7, epoch: 697, loss: 0.006\n",
      "model: 7, epoch: 698, loss: 0.006\n",
      "model: 7, epoch: 699, loss: 0.006\n",
      "model: 7, epoch: 700, loss: 0.006\n",
      "model: 7, epoch: 701, loss: 0.006\n",
      "model: 7, epoch: 702, loss: 0.006\n",
      "model: 7, epoch: 703, loss: 0.006\n",
      "model: 7, epoch: 704, loss: 0.006\n",
      "model: 7, epoch: 705, loss: 0.006\n",
      "model: 7, epoch: 706, loss: 0.006\n",
      "model: 7, epoch: 707, loss: 0.006\n",
      "model: 7, epoch: 708, loss: 0.006\n",
      "model: 7, epoch: 709, loss: 0.006\n",
      "model: 7, epoch: 710, loss: 0.006\n",
      "model: 7, epoch: 711, loss: 0.006\n",
      "model: 7, epoch: 712, loss: 0.006\n",
      "model: 7, epoch: 713, loss: 0.006\n",
      "model: 7, epoch: 714, loss: 0.006\n",
      "model: 7, epoch: 715, loss: 0.006\n",
      "model: 7, epoch: 716, loss: 0.006\n",
      "model: 7, epoch: 717, loss: 0.006\n",
      "model: 7, epoch: 718, loss: 0.006\n",
      "model: 7, epoch: 719, loss: 0.006\n",
      "model: 7, epoch: 720, loss: 0.006\n",
      "model: 7, epoch: 721, loss: 0.006\n",
      "model: 7, epoch: 722, loss: 0.006\n",
      "model: 7, epoch: 723, loss: 0.006\n",
      "model: 7, epoch: 724, loss: 0.006\n",
      "model: 7, epoch: 725, loss: 0.006\n",
      "model: 7, epoch: 726, loss: 0.006\n",
      "model: 7, epoch: 727, loss: 0.006\n",
      "model: 7, epoch: 728, loss: 0.006\n",
      "model: 7, epoch: 729, loss: 0.006\n",
      "model: 7, epoch: 730, loss: 0.006\n",
      "model: 7, epoch: 731, loss: 0.006\n",
      "model: 7, epoch: 732, loss: 0.006\n",
      "model: 7, epoch: 733, loss: 0.006\n",
      "model: 7, epoch: 734, loss: 0.006\n",
      "model: 7, epoch: 735, loss: 0.006\n",
      "model: 7, epoch: 736, loss: 0.006\n",
      "model: 7, epoch: 737, loss: 0.006\n",
      "model: 7, epoch: 738, loss: 0.006\n",
      "model: 7, epoch: 739, loss: 0.006\n",
      "model: 7, epoch: 740, loss: 0.006\n",
      "model: 7, epoch: 741, loss: 0.006\n",
      "model: 7, epoch: 742, loss: 0.006\n",
      "model: 7, epoch: 743, loss: 0.006\n",
      "model: 7, epoch: 744, loss: 0.006\n",
      "model: 7, epoch: 745, loss: 0.006\n",
      "model: 7, epoch: 746, loss: 0.006\n",
      "model: 7, epoch: 747, loss: 0.006\n",
      "model: 7, epoch: 748, loss: 0.006\n",
      "model: 7, epoch: 749, loss: 0.006\n",
      "model: 7, epoch: 750, loss: 0.006\n",
      "model: 7, epoch: 751, loss: 0.006\n",
      "model: 7, epoch: 752, loss: 0.006\n",
      "model: 7, epoch: 753, loss: 0.006\n",
      "model: 7, epoch: 754, loss: 0.006\n",
      "model: 7, epoch: 755, loss: 0.006\n",
      "model: 7, epoch: 756, loss: 0.006\n",
      "model: 7, epoch: 757, loss: 0.006\n",
      "model: 7, epoch: 758, loss: 0.006\n",
      "model: 7, epoch: 759, loss: 0.006\n",
      "model: 7, epoch: 760, loss: 0.006\n",
      "model: 7, epoch: 761, loss: 0.006\n",
      "model: 7, epoch: 762, loss: 0.006\n",
      "model: 7, epoch: 763, loss: 0.006\n",
      "model: 7, epoch: 764, loss: 0.006\n",
      "model: 7, epoch: 765, loss: 0.006\n",
      "model: 7, epoch: 766, loss: 0.006\n",
      "model: 7, epoch: 767, loss: 0.006\n",
      "model: 7, epoch: 768, loss: 0.006\n",
      "model: 7, epoch: 769, loss: 0.006\n",
      "model: 7, epoch: 770, loss: 0.006\n",
      "model: 7, epoch: 771, loss: 0.006\n",
      "model: 7, epoch: 772, loss: 0.006\n",
      "model: 7, epoch: 773, loss: 0.006\n",
      "model: 7, epoch: 774, loss: 0.006\n",
      "model: 7, epoch: 775, loss: 0.006\n",
      "model: 7, epoch: 776, loss: 0.006\n",
      "model: 7, epoch: 777, loss: 0.006\n",
      "model: 7, epoch: 778, loss: 0.006\n",
      "model: 7, epoch: 779, loss: 0.006\n",
      "model: 7, epoch: 780, loss: 0.006\n",
      "model: 7, epoch: 781, loss: 0.006\n",
      "model: 7, epoch: 782, loss: 0.006\n",
      "model: 7, epoch: 783, loss: 0.006\n",
      "model: 7, epoch: 784, loss: 0.006\n",
      "model: 7, epoch: 785, loss: 0.006\n",
      "model: 7, epoch: 786, loss: 0.006\n",
      "model: 7, epoch: 787, loss: 0.006\n",
      "model: 7, epoch: 788, loss: 0.006\n",
      "model: 7, epoch: 789, loss: 0.006\n",
      "model: 7, epoch: 790, loss: 0.006\n",
      "model: 7, epoch: 791, loss: 0.006\n",
      "model: 7, epoch: 792, loss: 0.006\n",
      "model: 7, epoch: 793, loss: 0.006\n",
      "model: 7, epoch: 794, loss: 0.006\n",
      "model: 7, epoch: 795, loss: 0.006\n",
      "model: 7, epoch: 796, loss: 0.006\n",
      "model: 7, epoch: 797, loss: 0.006\n",
      "model: 7, epoch: 798, loss: 0.006\n",
      "model: 7, epoch: 799, loss: 0.006\n",
      "model: 7, epoch: 800, loss: 0.006\n",
      "model: 7, epoch: 801, loss: 0.006\n",
      "model: 7, epoch: 802, loss: 0.006\n",
      "model: 7, epoch: 803, loss: 0.006\n",
      "model: 7, epoch: 804, loss: 0.006\n",
      "model: 7, epoch: 805, loss: 0.006\n",
      "model: 7, epoch: 806, loss: 0.006\n",
      "model: 7, epoch: 807, loss: 0.006\n",
      "model: 7, epoch: 808, loss: 0.006\n",
      "model: 7, epoch: 809, loss: 0.006\n",
      "model: 7, epoch: 810, loss: 0.006\n",
      "model: 7, epoch: 811, loss: 0.006\n",
      "model: 7, epoch: 812, loss: 0.006\n",
      "model: 7, epoch: 813, loss: 0.006\n",
      "model: 7, epoch: 814, loss: 0.006\n",
      "model: 7, epoch: 815, loss: 0.006\n",
      "model: 7, epoch: 816, loss: 0.006\n",
      "model: 7, epoch: 817, loss: 0.006\n",
      "model: 7, epoch: 818, loss: 0.006\n",
      "model: 7, epoch: 819, loss: 0.006\n",
      "model: 7, epoch: 820, loss: 0.006\n",
      "model: 7, epoch: 821, loss: 0.006\n",
      "model: 7, epoch: 822, loss: 0.006\n",
      "model: 7, epoch: 823, loss: 0.006\n",
      "model: 7, epoch: 824, loss: 0.006\n",
      "model: 7, epoch: 825, loss: 0.006\n",
      "model: 7, epoch: 826, loss: 0.006\n",
      "model: 7, epoch: 827, loss: 0.006\n",
      "model: 7, epoch: 828, loss: 0.006\n",
      "model: 7, epoch: 829, loss: 0.006\n",
      "model: 7, epoch: 830, loss: 0.006\n",
      "model: 7, epoch: 831, loss: 0.006\n",
      "model: 7, epoch: 832, loss: 0.006\n",
      "model: 7, epoch: 833, loss: 0.006\n",
      "model: 7, epoch: 834, loss: 0.006\n",
      "model: 7, epoch: 835, loss: 0.006\n",
      "model: 7, epoch: 836, loss: 0.006\n",
      "model: 7, epoch: 837, loss: 0.006\n",
      "model: 7, epoch: 838, loss: 0.006\n",
      "model: 7, epoch: 839, loss: 0.006\n",
      "model: 7, epoch: 840, loss: 0.006\n",
      "model: 7, epoch: 841, loss: 0.006\n",
      "model: 7, epoch: 842, loss: 0.006\n",
      "model: 7, epoch: 843, loss: 0.006\n",
      "model: 7, epoch: 844, loss: 0.006\n",
      "model: 7, epoch: 845, loss: 0.006\n",
      "model: 7, epoch: 846, loss: 0.006\n",
      "model: 7, epoch: 847, loss: 0.006\n",
      "model: 7, epoch: 848, loss: 0.006\n",
      "model: 7, epoch: 849, loss: 0.006\n",
      "model: 7, epoch: 850, loss: 0.006\n",
      "model: 7, epoch: 851, loss: 0.006\n",
      "model: 7, epoch: 852, loss: 0.006\n",
      "model: 7, epoch: 853, loss: 0.006\n",
      "model: 7, epoch: 854, loss: 0.006\n",
      "model: 7, epoch: 855, loss: 0.006\n",
      "model: 7, epoch: 856, loss: 0.006\n",
      "model: 7, epoch: 857, loss: 0.006\n",
      "model: 7, epoch: 858, loss: 0.006\n",
      "model: 7, epoch: 859, loss: 0.006\n",
      "model: 7, epoch: 860, loss: 0.006\n",
      "model: 7, epoch: 861, loss: 0.006\n",
      "model: 7, epoch: 862, loss: 0.006\n",
      "model: 7, epoch: 863, loss: 0.006\n",
      "model: 7, epoch: 864, loss: 0.006\n",
      "model: 7, epoch: 865, loss: 0.006\n",
      "model: 7, epoch: 866, loss: 0.006\n",
      "model: 7, epoch: 867, loss: 0.006\n",
      "model: 7, epoch: 868, loss: 0.006\n",
      "model: 7, epoch: 869, loss: 0.006\n",
      "model: 7, epoch: 870, loss: 0.006\n",
      "model: 7, epoch: 871, loss: 0.006\n",
      "model: 7, epoch: 872, loss: 0.006\n",
      "model: 7, epoch: 873, loss: 0.006\n",
      "model: 7, epoch: 874, loss: 0.006\n",
      "model: 7, epoch: 875, loss: 0.006\n",
      "model: 7, epoch: 876, loss: 0.006\n",
      "model: 7, epoch: 877, loss: 0.006\n",
      "model: 7, epoch: 878, loss: 0.006\n",
      "model: 7, epoch: 879, loss: 0.006\n",
      "model: 7, epoch: 880, loss: 0.006\n",
      "model: 7, epoch: 881, loss: 0.006\n",
      "model: 7, epoch: 882, loss: 0.006\n",
      "model: 7, epoch: 883, loss: 0.006\n",
      "model: 7, epoch: 884, loss: 0.006\n",
      "model: 7, epoch: 885, loss: 0.006\n",
      "model: 7, epoch: 886, loss: 0.006\n",
      "model: 7, epoch: 887, loss: 0.006\n",
      "model: 7, epoch: 888, loss: 0.006\n",
      "model: 7, epoch: 889, loss: 0.006\n",
      "model: 7, epoch: 890, loss: 0.006\n",
      "model: 7, epoch: 891, loss: 0.006\n",
      "model: 7, epoch: 892, loss: 0.006\n",
      "model: 7, epoch: 893, loss: 0.006\n",
      "model: 7, epoch: 894, loss: 0.006\n",
      "model: 7, epoch: 895, loss: 0.006\n",
      "model: 7, epoch: 896, loss: 0.006\n",
      "model: 7, epoch: 897, loss: 0.006\n",
      "model: 7, epoch: 898, loss: 0.006\n",
      "model: 7, epoch: 899, loss: 0.006\n",
      "model: 7, epoch: 900, loss: 0.006\n",
      "model: 7, epoch: 901, loss: 0.006\n",
      "model: 7, epoch: 902, loss: 0.006\n",
      "model: 7, epoch: 903, loss: 0.006\n",
      "model: 7, epoch: 904, loss: 0.006\n",
      "model: 7, epoch: 905, loss: 0.006\n",
      "model: 7, epoch: 906, loss: 0.006\n",
      "model: 7, epoch: 907, loss: 0.006\n",
      "model: 7, epoch: 908, loss: 0.006\n",
      "model: 7, epoch: 909, loss: 0.006\n",
      "model: 7, epoch: 910, loss: 0.006\n",
      "model: 7, epoch: 911, loss: 0.006\n",
      "model: 7, epoch: 912, loss: 0.006\n",
      "model: 7, epoch: 913, loss: 0.006\n",
      "model: 7, epoch: 914, loss: 0.006\n",
      "model: 7, epoch: 915, loss: 0.006\n",
      "model: 7, epoch: 916, loss: 0.006\n",
      "model: 7, epoch: 917, loss: 0.006\n",
      "model: 7, epoch: 918, loss: 0.006\n",
      "model: 7, epoch: 919, loss: 0.006\n",
      "model: 7, epoch: 920, loss: 0.006\n",
      "model: 7, epoch: 921, loss: 0.006\n",
      "model: 7, epoch: 922, loss: 0.006\n",
      "model: 7, epoch: 923, loss: 0.006\n",
      "model: 7, epoch: 924, loss: 0.006\n",
      "model: 7, epoch: 925, loss: 0.006\n",
      "model: 7, epoch: 926, loss: 0.006\n",
      "model: 7, epoch: 927, loss: 0.006\n",
      "model: 7, epoch: 928, loss: 0.006\n",
      "model: 7, epoch: 929, loss: 0.006\n",
      "model: 7, epoch: 930, loss: 0.006\n",
      "model: 7, epoch: 931, loss: 0.006\n",
      "model: 7, epoch: 932, loss: 0.006\n",
      "model: 7, epoch: 933, loss: 0.006\n",
      "model: 7, epoch: 934, loss: 0.006\n",
      "model: 7, epoch: 935, loss: 0.006\n",
      "model: 7, epoch: 936, loss: 0.006\n",
      "model: 7, epoch: 937, loss: 0.006\n",
      "model: 7, epoch: 938, loss: 0.006\n",
      "model: 7, epoch: 939, loss: 0.006\n",
      "model: 7, epoch: 940, loss: 0.006\n",
      "model: 7, epoch: 941, loss: 0.006\n",
      "model: 7, epoch: 942, loss: 0.006\n",
      "model: 7, epoch: 943, loss: 0.006\n",
      "model: 7, epoch: 944, loss: 0.006\n",
      "model: 7, epoch: 945, loss: 0.006\n",
      "model: 7, epoch: 946, loss: 0.006\n",
      "model: 7, epoch: 947, loss: 0.006\n",
      "model: 7, epoch: 948, loss: 0.006\n",
      "model: 7, epoch: 949, loss: 0.006\n",
      "model: 7, epoch: 950, loss: 0.006\n",
      "model: 7, epoch: 951, loss: 0.006\n",
      "model: 7, epoch: 952, loss: 0.006\n",
      "model: 7, epoch: 953, loss: 0.006\n",
      "model: 7, epoch: 954, loss: 0.006\n",
      "model: 7, epoch: 955, loss: 0.006\n",
      "model: 7, epoch: 956, loss: 0.006\n",
      "model: 7, epoch: 957, loss: 0.006\n",
      "model: 7, epoch: 958, loss: 0.006\n",
      "model: 7, epoch: 959, loss: 0.006\n",
      "model: 7, epoch: 960, loss: 0.006\n",
      "model: 7, epoch: 961, loss: 0.006\n",
      "model: 7, epoch: 962, loss: 0.006\n",
      "model: 7, epoch: 963, loss: 0.006\n",
      "model: 7, epoch: 964, loss: 0.006\n",
      "model: 7, epoch: 965, loss: 0.006\n",
      "model: 7, epoch: 966, loss: 0.006\n",
      "model: 7, epoch: 967, loss: 0.006\n",
      "model: 7, epoch: 968, loss: 0.006\n",
      "model: 7, epoch: 969, loss: 0.006\n",
      "model: 7, epoch: 970, loss: 0.006\n",
      "model: 7, epoch: 971, loss: 0.006\n",
      "model: 7, epoch: 972, loss: 0.006\n",
      "model: 7, epoch: 973, loss: 0.006\n",
      "model: 7, epoch: 974, loss: 0.006\n",
      "model: 7, epoch: 975, loss: 0.006\n",
      "model: 7, epoch: 976, loss: 0.006\n",
      "model: 7, epoch: 977, loss: 0.006\n",
      "model: 7, epoch: 978, loss: 0.006\n",
      "model: 7, epoch: 979, loss: 0.006\n",
      "model: 7, epoch: 980, loss: 0.006\n",
      "model: 7, epoch: 981, loss: 0.006\n",
      "model: 7, epoch: 982, loss: 0.006\n",
      "model: 7, epoch: 983, loss: 0.006\n",
      "model: 7, epoch: 984, loss: 0.006\n",
      "model: 7, epoch: 985, loss: 0.006\n",
      "model: 7, epoch: 986, loss: 0.006\n",
      "model: 7, epoch: 987, loss: 0.006\n",
      "model: 7, epoch: 988, loss: 0.006\n",
      "model: 7, epoch: 989, loss: 0.006\n",
      "model: 7, epoch: 990, loss: 0.006\n",
      "model: 7, epoch: 991, loss: 0.006\n",
      "model: 7, epoch: 992, loss: 0.006\n",
      "model: 7, epoch: 993, loss: 0.006\n",
      "model: 7, epoch: 994, loss: 0.006\n",
      "model: 7, epoch: 995, loss: 0.006\n",
      "model: 7, epoch: 996, loss: 0.006\n",
      "model: 7, epoch: 997, loss: 0.006\n",
      "model: 7, epoch: 998, loss: 0.006\n",
      "model: 7, epoch: 999, loss: 0.006\n",
      "model: 8, epoch: 0, loss: 0.253\n",
      "model: 8, epoch: 1, loss: 0.225\n",
      "model: 8, epoch: 2, loss: 0.200\n",
      "model: 8, epoch: 3, loss: 0.179\n",
      "model: 8, epoch: 4, loss: 0.161\n",
      "model: 8, epoch: 5, loss: 0.145\n",
      "model: 8, epoch: 6, loss: 0.132\n",
      "model: 8, epoch: 7, loss: 0.120\n",
      "model: 8, epoch: 8, loss: 0.109\n",
      "model: 8, epoch: 9, loss: 0.099\n",
      "model: 8, epoch: 10, loss: 0.090\n",
      "model: 8, epoch: 11, loss: 0.082\n",
      "model: 8, epoch: 12, loss: 0.074\n",
      "model: 8, epoch: 13, loss: 0.067\n",
      "model: 8, epoch: 14, loss: 0.060\n",
      "model: 8, epoch: 15, loss: 0.054\n",
      "model: 8, epoch: 16, loss: 0.049\n",
      "model: 8, epoch: 17, loss: 0.044\n",
      "model: 8, epoch: 18, loss: 0.040\n",
      "model: 8, epoch: 19, loss: 0.037\n",
      "model: 8, epoch: 20, loss: 0.034\n",
      "model: 8, epoch: 21, loss: 0.032\n",
      "model: 8, epoch: 22, loss: 0.031\n",
      "model: 8, epoch: 23, loss: 0.030\n",
      "model: 8, epoch: 24, loss: 0.029\n",
      "model: 8, epoch: 25, loss: 0.029\n",
      "model: 8, epoch: 26, loss: 0.029\n",
      "model: 8, epoch: 27, loss: 0.029\n",
      "model: 8, epoch: 28, loss: 0.029\n",
      "model: 8, epoch: 29, loss: 0.029\n",
      "model: 8, epoch: 30, loss: 0.030\n",
      "model: 8, epoch: 31, loss: 0.030\n",
      "model: 8, epoch: 32, loss: 0.030\n",
      "model: 8, epoch: 33, loss: 0.030\n",
      "model: 8, epoch: 34, loss: 0.030\n",
      "model: 8, epoch: 35, loss: 0.031\n",
      "model: 8, epoch: 36, loss: 0.031\n",
      "model: 8, epoch: 37, loss: 0.031\n",
      "model: 8, epoch: 38, loss: 0.031\n",
      "model: 8, epoch: 39, loss: 0.031\n",
      "model: 8, epoch: 40, loss: 0.031\n",
      "model: 8, epoch: 41, loss: 0.031\n",
      "model: 8, epoch: 42, loss: 0.031\n",
      "model: 8, epoch: 43, loss: 0.031\n",
      "model: 8, epoch: 44, loss: 0.030\n",
      "model: 8, epoch: 45, loss: 0.030\n",
      "model: 8, epoch: 46, loss: 0.030\n",
      "model: 8, epoch: 47, loss: 0.030\n",
      "model: 8, epoch: 48, loss: 0.029\n",
      "model: 8, epoch: 49, loss: 0.029\n",
      "model: 8, epoch: 50, loss: 0.029\n",
      "model: 8, epoch: 51, loss: 0.029\n",
      "model: 8, epoch: 52, loss: 0.029\n",
      "model: 8, epoch: 53, loss: 0.029\n",
      "model: 8, epoch: 54, loss: 0.029\n",
      "model: 8, epoch: 55, loss: 0.029\n",
      "model: 8, epoch: 56, loss: 0.028\n",
      "model: 8, epoch: 57, loss: 0.028\n",
      "model: 8, epoch: 58, loss: 0.028\n",
      "model: 8, epoch: 59, loss: 0.028\n",
      "model: 8, epoch: 60, loss: 0.028\n",
      "model: 8, epoch: 61, loss: 0.028\n",
      "model: 8, epoch: 62, loss: 0.028\n",
      "model: 8, epoch: 63, loss: 0.028\n",
      "model: 8, epoch: 64, loss: 0.028\n",
      "model: 8, epoch: 65, loss: 0.028\n",
      "model: 8, epoch: 66, loss: 0.028\n",
      "model: 8, epoch: 67, loss: 0.028\n",
      "model: 8, epoch: 68, loss: 0.028\n",
      "model: 8, epoch: 69, loss: 0.028\n",
      "model: 8, epoch: 70, loss: 0.028\n",
      "model: 8, epoch: 71, loss: 0.028\n",
      "model: 8, epoch: 72, loss: 0.028\n",
      "model: 8, epoch: 73, loss: 0.028\n",
      "model: 8, epoch: 74, loss: 0.028\n",
      "model: 8, epoch: 75, loss: 0.028\n",
      "model: 8, epoch: 76, loss: 0.028\n",
      "model: 8, epoch: 77, loss: 0.028\n",
      "model: 8, epoch: 78, loss: 0.028\n",
      "model: 8, epoch: 79, loss: 0.028\n",
      "model: 8, epoch: 80, loss: 0.028\n",
      "model: 8, epoch: 81, loss: 0.028\n",
      "model: 8, epoch: 82, loss: 0.028\n",
      "model: 8, epoch: 83, loss: 0.028\n",
      "model: 8, epoch: 84, loss: 0.028\n",
      "model: 8, epoch: 85, loss: 0.028\n",
      "model: 8, epoch: 86, loss: 0.028\n",
      "model: 8, epoch: 87, loss: 0.028\n",
      "model: 8, epoch: 88, loss: 0.028\n",
      "model: 8, epoch: 89, loss: 0.028\n",
      "model: 8, epoch: 90, loss: 0.028\n",
      "model: 8, epoch: 91, loss: 0.028\n",
      "model: 8, epoch: 92, loss: 0.028\n",
      "model: 8, epoch: 93, loss: 0.028\n",
      "model: 8, epoch: 94, loss: 0.028\n",
      "model: 8, epoch: 95, loss: 0.028\n",
      "model: 8, epoch: 96, loss: 0.028\n",
      "model: 8, epoch: 97, loss: 0.028\n",
      "model: 8, epoch: 98, loss: 0.028\n",
      "model: 8, epoch: 99, loss: 0.028\n",
      "model: 8, epoch: 100, loss: 0.028\n",
      "model: 8, epoch: 101, loss: 0.028\n",
      "model: 8, epoch: 102, loss: 0.028\n",
      "model: 8, epoch: 103, loss: 0.028\n",
      "model: 8, epoch: 104, loss: 0.028\n",
      "model: 8, epoch: 105, loss: 0.028\n",
      "model: 8, epoch: 106, loss: 0.028\n",
      "model: 8, epoch: 107, loss: 0.028\n",
      "model: 8, epoch: 108, loss: 0.028\n",
      "model: 8, epoch: 109, loss: 0.028\n",
      "model: 8, epoch: 110, loss: 0.028\n",
      "model: 8, epoch: 111, loss: 0.028\n",
      "model: 8, epoch: 112, loss: 0.028\n",
      "model: 8, epoch: 113, loss: 0.028\n",
      "model: 8, epoch: 114, loss: 0.028\n",
      "model: 8, epoch: 115, loss: 0.028\n",
      "model: 8, epoch: 116, loss: 0.028\n",
      "model: 8, epoch: 117, loss: 0.028\n",
      "model: 8, epoch: 118, loss: 0.028\n",
      "model: 8, epoch: 119, loss: 0.028\n",
      "model: 8, epoch: 120, loss: 0.028\n",
      "model: 8, epoch: 121, loss: 0.028\n",
      "model: 8, epoch: 122, loss: 0.028\n",
      "model: 8, epoch: 123, loss: 0.028\n",
      "model: 8, epoch: 124, loss: 0.028\n",
      "model: 8, epoch: 125, loss: 0.028\n",
      "model: 8, epoch: 126, loss: 0.028\n",
      "model: 8, epoch: 127, loss: 0.028\n",
      "model: 8, epoch: 128, loss: 0.028\n",
      "model: 8, epoch: 129, loss: 0.028\n",
      "model: 8, epoch: 130, loss: 0.028\n",
      "model: 8, epoch: 131, loss: 0.028\n",
      "model: 8, epoch: 132, loss: 0.028\n",
      "model: 8, epoch: 133, loss: 0.028\n",
      "model: 8, epoch: 134, loss: 0.028\n",
      "model: 8, epoch: 135, loss: 0.028\n",
      "model: 8, epoch: 136, loss: 0.028\n",
      "model: 8, epoch: 137, loss: 0.028\n",
      "model: 8, epoch: 138, loss: 0.028\n",
      "model: 8, epoch: 139, loss: 0.028\n",
      "model: 8, epoch: 140, loss: 0.028\n",
      "model: 8, epoch: 141, loss: 0.028\n",
      "model: 8, epoch: 142, loss: 0.028\n",
      "model: 8, epoch: 143, loss: 0.028\n",
      "model: 8, epoch: 144, loss: 0.028\n",
      "model: 8, epoch: 145, loss: 0.028\n",
      "model: 8, epoch: 146, loss: 0.028\n",
      "model: 8, epoch: 147, loss: 0.028\n",
      "model: 8, epoch: 148, loss: 0.028\n",
      "model: 8, epoch: 149, loss: 0.028\n",
      "model: 8, epoch: 150, loss: 0.028\n",
      "model: 8, epoch: 151, loss: 0.028\n",
      "model: 8, epoch: 152, loss: 0.028\n",
      "model: 8, epoch: 153, loss: 0.027\n",
      "model: 8, epoch: 154, loss: 0.027\n",
      "model: 8, epoch: 155, loss: 0.027\n",
      "model: 8, epoch: 156, loss: 0.027\n",
      "model: 8, epoch: 157, loss: 0.027\n",
      "model: 8, epoch: 158, loss: 0.027\n",
      "model: 8, epoch: 159, loss: 0.027\n",
      "model: 8, epoch: 160, loss: 0.027\n",
      "model: 8, epoch: 161, loss: 0.027\n",
      "model: 8, epoch: 162, loss: 0.027\n",
      "model: 8, epoch: 163, loss: 0.027\n",
      "model: 8, epoch: 164, loss: 0.027\n",
      "model: 8, epoch: 165, loss: 0.027\n",
      "model: 8, epoch: 166, loss: 0.027\n",
      "model: 8, epoch: 167, loss: 0.027\n",
      "model: 8, epoch: 168, loss: 0.027\n",
      "model: 8, epoch: 169, loss: 0.027\n",
      "model: 8, epoch: 170, loss: 0.027\n",
      "model: 8, epoch: 171, loss: 0.027\n",
      "model: 8, epoch: 172, loss: 0.027\n",
      "model: 8, epoch: 173, loss: 0.027\n",
      "model: 8, epoch: 174, loss: 0.027\n",
      "model: 8, epoch: 175, loss: 0.027\n",
      "model: 8, epoch: 176, loss: 0.027\n",
      "model: 8, epoch: 177, loss: 0.027\n",
      "model: 8, epoch: 178, loss: 0.027\n",
      "model: 8, epoch: 179, loss: 0.027\n",
      "model: 8, epoch: 180, loss: 0.027\n",
      "model: 8, epoch: 181, loss: 0.027\n",
      "model: 8, epoch: 182, loss: 0.027\n",
      "model: 8, epoch: 183, loss: 0.027\n",
      "model: 8, epoch: 184, loss: 0.027\n",
      "model: 8, epoch: 185, loss: 0.027\n",
      "model: 8, epoch: 186, loss: 0.027\n",
      "model: 8, epoch: 187, loss: 0.027\n",
      "model: 8, epoch: 188, loss: 0.027\n",
      "model: 8, epoch: 189, loss: 0.027\n",
      "model: 8, epoch: 190, loss: 0.027\n",
      "model: 8, epoch: 191, loss: 0.027\n",
      "model: 8, epoch: 192, loss: 0.027\n",
      "model: 8, epoch: 193, loss: 0.027\n",
      "model: 8, epoch: 194, loss: 0.027\n",
      "model: 8, epoch: 195, loss: 0.027\n",
      "model: 8, epoch: 196, loss: 0.027\n",
      "model: 8, epoch: 197, loss: 0.027\n",
      "model: 8, epoch: 198, loss: 0.027\n",
      "model: 8, epoch: 199, loss: 0.027\n",
      "model: 8, epoch: 200, loss: 0.027\n",
      "model: 8, epoch: 201, loss: 0.027\n",
      "model: 8, epoch: 202, loss: 0.027\n",
      "model: 8, epoch: 203, loss: 0.027\n",
      "model: 8, epoch: 204, loss: 0.027\n",
      "model: 8, epoch: 205, loss: 0.027\n",
      "model: 8, epoch: 206, loss: 0.027\n",
      "model: 8, epoch: 207, loss: 0.027\n",
      "model: 8, epoch: 208, loss: 0.027\n",
      "model: 8, epoch: 209, loss: 0.027\n",
      "model: 8, epoch: 210, loss: 0.027\n",
      "model: 8, epoch: 211, loss: 0.027\n",
      "model: 8, epoch: 212, loss: 0.027\n",
      "model: 8, epoch: 213, loss: 0.027\n",
      "model: 8, epoch: 214, loss: 0.027\n",
      "model: 8, epoch: 215, loss: 0.027\n",
      "model: 8, epoch: 216, loss: 0.027\n",
      "model: 8, epoch: 217, loss: 0.027\n",
      "model: 8, epoch: 218, loss: 0.027\n",
      "model: 8, epoch: 219, loss: 0.027\n",
      "model: 8, epoch: 220, loss: 0.027\n",
      "model: 8, epoch: 221, loss: 0.027\n",
      "model: 8, epoch: 222, loss: 0.027\n",
      "model: 8, epoch: 223, loss: 0.027\n",
      "model: 8, epoch: 224, loss: 0.027\n",
      "model: 8, epoch: 225, loss: 0.027\n",
      "model: 8, epoch: 226, loss: 0.027\n",
      "model: 8, epoch: 227, loss: 0.027\n",
      "model: 8, epoch: 228, loss: 0.027\n",
      "model: 8, epoch: 229, loss: 0.027\n",
      "model: 8, epoch: 230, loss: 0.027\n",
      "model: 8, epoch: 231, loss: 0.027\n",
      "model: 8, epoch: 232, loss: 0.027\n",
      "model: 8, epoch: 233, loss: 0.027\n",
      "model: 8, epoch: 234, loss: 0.027\n",
      "model: 8, epoch: 235, loss: 0.027\n",
      "model: 8, epoch: 236, loss: 0.027\n",
      "model: 8, epoch: 237, loss: 0.026\n",
      "model: 8, epoch: 238, loss: 0.026\n",
      "model: 8, epoch: 239, loss: 0.026\n",
      "model: 8, epoch: 240, loss: 0.026\n",
      "model: 8, epoch: 241, loss: 0.026\n",
      "model: 8, epoch: 242, loss: 0.026\n",
      "model: 8, epoch: 243, loss: 0.026\n",
      "model: 8, epoch: 244, loss: 0.026\n",
      "model: 8, epoch: 245, loss: 0.026\n",
      "model: 8, epoch: 246, loss: 0.026\n",
      "model: 8, epoch: 247, loss: 0.026\n",
      "model: 8, epoch: 248, loss: 0.026\n",
      "model: 8, epoch: 249, loss: 0.026\n",
      "model: 8, epoch: 250, loss: 0.026\n",
      "model: 8, epoch: 251, loss: 0.026\n",
      "model: 8, epoch: 252, loss: 0.026\n",
      "model: 8, epoch: 253, loss: 0.026\n",
      "model: 8, epoch: 254, loss: 0.026\n",
      "model: 8, epoch: 255, loss: 0.026\n",
      "model: 8, epoch: 256, loss: 0.026\n",
      "model: 8, epoch: 257, loss: 0.026\n",
      "model: 8, epoch: 258, loss: 0.026\n",
      "model: 8, epoch: 259, loss: 0.026\n",
      "model: 8, epoch: 260, loss: 0.026\n",
      "model: 8, epoch: 261, loss: 0.026\n",
      "model: 8, epoch: 262, loss: 0.026\n",
      "model: 8, epoch: 263, loss: 0.026\n",
      "model: 8, epoch: 264, loss: 0.026\n",
      "model: 8, epoch: 265, loss: 0.026\n",
      "model: 8, epoch: 266, loss: 0.026\n",
      "model: 8, epoch: 267, loss: 0.026\n",
      "model: 8, epoch: 268, loss: 0.026\n",
      "model: 8, epoch: 269, loss: 0.026\n",
      "model: 8, epoch: 270, loss: 0.026\n",
      "model: 8, epoch: 271, loss: 0.026\n",
      "model: 8, epoch: 272, loss: 0.026\n",
      "model: 8, epoch: 273, loss: 0.026\n",
      "model: 8, epoch: 274, loss: 0.026\n",
      "model: 8, epoch: 275, loss: 0.026\n",
      "model: 8, epoch: 276, loss: 0.026\n",
      "model: 8, epoch: 277, loss: 0.026\n",
      "model: 8, epoch: 278, loss: 0.026\n",
      "model: 8, epoch: 279, loss: 0.026\n",
      "model: 8, epoch: 280, loss: 0.026\n",
      "model: 8, epoch: 281, loss: 0.026\n",
      "model: 8, epoch: 282, loss: 0.026\n",
      "model: 8, epoch: 283, loss: 0.026\n",
      "model: 8, epoch: 284, loss: 0.026\n",
      "model: 8, epoch: 285, loss: 0.026\n",
      "model: 8, epoch: 286, loss: 0.026\n",
      "model: 8, epoch: 287, loss: 0.026\n",
      "model: 8, epoch: 288, loss: 0.026\n",
      "model: 8, epoch: 289, loss: 0.026\n",
      "model: 8, epoch: 290, loss: 0.026\n",
      "model: 8, epoch: 291, loss: 0.026\n",
      "model: 8, epoch: 292, loss: 0.026\n",
      "model: 8, epoch: 293, loss: 0.026\n",
      "model: 8, epoch: 294, loss: 0.026\n",
      "model: 8, epoch: 295, loss: 0.026\n",
      "model: 8, epoch: 296, loss: 0.026\n",
      "model: 8, epoch: 297, loss: 0.026\n",
      "model: 8, epoch: 298, loss: 0.026\n",
      "model: 8, epoch: 299, loss: 0.026\n",
      "model: 8, epoch: 300, loss: 0.026\n",
      "model: 8, epoch: 301, loss: 0.026\n",
      "model: 8, epoch: 302, loss: 0.026\n",
      "model: 8, epoch: 303, loss: 0.026\n",
      "model: 8, epoch: 304, loss: 0.026\n",
      "model: 8, epoch: 305, loss: 0.026\n",
      "model: 8, epoch: 306, loss: 0.026\n",
      "model: 8, epoch: 307, loss: 0.026\n",
      "model: 8, epoch: 308, loss: 0.025\n",
      "model: 8, epoch: 309, loss: 0.025\n",
      "model: 8, epoch: 310, loss: 0.025\n",
      "model: 8, epoch: 311, loss: 0.025\n",
      "model: 8, epoch: 312, loss: 0.025\n",
      "model: 8, epoch: 313, loss: 0.025\n",
      "model: 8, epoch: 314, loss: 0.025\n",
      "model: 8, epoch: 315, loss: 0.025\n",
      "model: 8, epoch: 316, loss: 0.025\n",
      "model: 8, epoch: 317, loss: 0.025\n",
      "model: 8, epoch: 318, loss: 0.025\n",
      "model: 8, epoch: 319, loss: 0.025\n",
      "model: 8, epoch: 320, loss: 0.025\n",
      "model: 8, epoch: 321, loss: 0.025\n",
      "model: 8, epoch: 322, loss: 0.025\n",
      "model: 8, epoch: 323, loss: 0.025\n",
      "model: 8, epoch: 324, loss: 0.025\n",
      "model: 8, epoch: 325, loss: 0.025\n",
      "model: 8, epoch: 326, loss: 0.025\n",
      "model: 8, epoch: 327, loss: 0.025\n",
      "model: 8, epoch: 328, loss: 0.025\n",
      "model: 8, epoch: 329, loss: 0.025\n",
      "model: 8, epoch: 330, loss: 0.025\n",
      "model: 8, epoch: 331, loss: 0.025\n",
      "model: 8, epoch: 332, loss: 0.025\n",
      "model: 8, epoch: 333, loss: 0.025\n",
      "model: 8, epoch: 334, loss: 0.025\n",
      "model: 8, epoch: 335, loss: 0.025\n",
      "model: 8, epoch: 336, loss: 0.025\n",
      "model: 8, epoch: 337, loss: 0.025\n",
      "model: 8, epoch: 338, loss: 0.025\n",
      "model: 8, epoch: 339, loss: 0.025\n",
      "model: 8, epoch: 340, loss: 0.025\n",
      "model: 8, epoch: 341, loss: 0.025\n",
      "model: 8, epoch: 342, loss: 0.025\n",
      "model: 8, epoch: 343, loss: 0.025\n",
      "model: 8, epoch: 344, loss: 0.025\n",
      "model: 8, epoch: 345, loss: 0.025\n",
      "model: 8, epoch: 346, loss: 0.025\n",
      "model: 8, epoch: 347, loss: 0.025\n",
      "model: 8, epoch: 348, loss: 0.025\n",
      "model: 8, epoch: 349, loss: 0.025\n",
      "model: 8, epoch: 350, loss: 0.025\n",
      "model: 8, epoch: 351, loss: 0.025\n",
      "model: 8, epoch: 352, loss: 0.025\n",
      "model: 8, epoch: 353, loss: 0.025\n",
      "model: 8, epoch: 354, loss: 0.025\n",
      "model: 8, epoch: 355, loss: 0.025\n",
      "model: 8, epoch: 356, loss: 0.025\n",
      "model: 8, epoch: 357, loss: 0.025\n",
      "model: 8, epoch: 358, loss: 0.025\n",
      "model: 8, epoch: 359, loss: 0.025\n",
      "model: 8, epoch: 360, loss: 0.025\n",
      "model: 8, epoch: 361, loss: 0.025\n",
      "model: 8, epoch: 362, loss: 0.025\n",
      "model: 8, epoch: 363, loss: 0.025\n",
      "model: 8, epoch: 364, loss: 0.025\n",
      "model: 8, epoch: 365, loss: 0.025\n",
      "model: 8, epoch: 366, loss: 0.025\n",
      "model: 8, epoch: 367, loss: 0.025\n",
      "model: 8, epoch: 368, loss: 0.025\n",
      "model: 8, epoch: 369, loss: 0.025\n",
      "model: 8, epoch: 370, loss: 0.024\n",
      "model: 8, epoch: 371, loss: 0.024\n",
      "model: 8, epoch: 372, loss: 0.024\n",
      "model: 8, epoch: 373, loss: 0.024\n",
      "model: 8, epoch: 374, loss: 0.024\n",
      "model: 8, epoch: 375, loss: 0.024\n",
      "model: 8, epoch: 376, loss: 0.024\n",
      "model: 8, epoch: 377, loss: 0.024\n",
      "model: 8, epoch: 378, loss: 0.024\n",
      "model: 8, epoch: 379, loss: 0.024\n",
      "model: 8, epoch: 380, loss: 0.024\n",
      "model: 8, epoch: 381, loss: 0.024\n",
      "model: 8, epoch: 382, loss: 0.024\n",
      "model: 8, epoch: 383, loss: 0.024\n",
      "model: 8, epoch: 384, loss: 0.024\n",
      "model: 8, epoch: 385, loss: 0.024\n",
      "model: 8, epoch: 386, loss: 0.024\n",
      "model: 8, epoch: 387, loss: 0.024\n",
      "model: 8, epoch: 388, loss: 0.024\n",
      "model: 8, epoch: 389, loss: 0.024\n",
      "model: 8, epoch: 390, loss: 0.024\n",
      "model: 8, epoch: 391, loss: 0.024\n",
      "model: 8, epoch: 392, loss: 0.024\n",
      "model: 8, epoch: 393, loss: 0.024\n",
      "model: 8, epoch: 394, loss: 0.024\n",
      "model: 8, epoch: 395, loss: 0.024\n",
      "model: 8, epoch: 396, loss: 0.024\n",
      "model: 8, epoch: 397, loss: 0.024\n",
      "model: 8, epoch: 398, loss: 0.024\n",
      "model: 8, epoch: 399, loss: 0.024\n",
      "model: 8, epoch: 400, loss: 0.024\n",
      "model: 8, epoch: 401, loss: 0.024\n",
      "model: 8, epoch: 402, loss: 0.024\n",
      "model: 8, epoch: 403, loss: 0.024\n",
      "model: 8, epoch: 404, loss: 0.024\n",
      "model: 8, epoch: 405, loss: 0.024\n",
      "model: 8, epoch: 406, loss: 0.024\n",
      "model: 8, epoch: 407, loss: 0.024\n",
      "model: 8, epoch: 408, loss: 0.024\n",
      "model: 8, epoch: 409, loss: 0.024\n",
      "model: 8, epoch: 410, loss: 0.024\n",
      "model: 8, epoch: 411, loss: 0.024\n",
      "model: 8, epoch: 412, loss: 0.024\n",
      "model: 8, epoch: 413, loss: 0.024\n",
      "model: 8, epoch: 414, loss: 0.024\n",
      "model: 8, epoch: 415, loss: 0.024\n",
      "model: 8, epoch: 416, loss: 0.024\n",
      "model: 8, epoch: 417, loss: 0.024\n",
      "model: 8, epoch: 418, loss: 0.024\n",
      "model: 8, epoch: 419, loss: 0.024\n",
      "model: 8, epoch: 420, loss: 0.024\n",
      "model: 8, epoch: 421, loss: 0.024\n",
      "model: 8, epoch: 422, loss: 0.024\n",
      "model: 8, epoch: 423, loss: 0.024\n",
      "model: 8, epoch: 424, loss: 0.023\n",
      "model: 8, epoch: 425, loss: 0.023\n",
      "model: 8, epoch: 426, loss: 0.023\n",
      "model: 8, epoch: 427, loss: 0.023\n",
      "model: 8, epoch: 428, loss: 0.023\n",
      "model: 8, epoch: 429, loss: 0.023\n",
      "model: 8, epoch: 430, loss: 0.023\n",
      "model: 8, epoch: 431, loss: 0.023\n",
      "model: 8, epoch: 432, loss: 0.023\n",
      "model: 8, epoch: 433, loss: 0.023\n",
      "model: 8, epoch: 434, loss: 0.023\n",
      "model: 8, epoch: 435, loss: 0.023\n",
      "model: 8, epoch: 436, loss: 0.023\n",
      "model: 8, epoch: 437, loss: 0.023\n",
      "model: 8, epoch: 438, loss: 0.023\n",
      "model: 8, epoch: 439, loss: 0.023\n",
      "model: 8, epoch: 440, loss: 0.023\n",
      "model: 8, epoch: 441, loss: 0.023\n",
      "model: 8, epoch: 442, loss: 0.023\n",
      "model: 8, epoch: 443, loss: 0.023\n",
      "model: 8, epoch: 444, loss: 0.023\n",
      "model: 8, epoch: 445, loss: 0.023\n",
      "model: 8, epoch: 446, loss: 0.023\n",
      "model: 8, epoch: 447, loss: 0.023\n",
      "model: 8, epoch: 448, loss: 0.023\n",
      "model: 8, epoch: 449, loss: 0.023\n",
      "model: 8, epoch: 450, loss: 0.023\n",
      "model: 8, epoch: 451, loss: 0.023\n",
      "model: 8, epoch: 452, loss: 0.023\n",
      "model: 8, epoch: 453, loss: 0.023\n",
      "model: 8, epoch: 454, loss: 0.023\n",
      "model: 8, epoch: 455, loss: 0.023\n",
      "model: 8, epoch: 456, loss: 0.023\n",
      "model: 8, epoch: 457, loss: 0.023\n",
      "model: 8, epoch: 458, loss: 0.023\n",
      "model: 8, epoch: 459, loss: 0.023\n",
      "model: 8, epoch: 460, loss: 0.023\n",
      "model: 8, epoch: 461, loss: 0.023\n",
      "model: 8, epoch: 462, loss: 0.023\n",
      "model: 8, epoch: 463, loss: 0.023\n",
      "model: 8, epoch: 464, loss: 0.023\n",
      "model: 8, epoch: 465, loss: 0.023\n",
      "model: 8, epoch: 466, loss: 0.023\n",
      "model: 8, epoch: 467, loss: 0.023\n",
      "model: 8, epoch: 468, loss: 0.023\n",
      "model: 8, epoch: 469, loss: 0.023\n",
      "model: 8, epoch: 470, loss: 0.023\n",
      "model: 8, epoch: 471, loss: 0.022\n",
      "model: 8, epoch: 472, loss: 0.022\n",
      "model: 8, epoch: 473, loss: 0.022\n",
      "model: 8, epoch: 474, loss: 0.022\n",
      "model: 8, epoch: 475, loss: 0.022\n",
      "model: 8, epoch: 476, loss: 0.022\n",
      "model: 8, epoch: 477, loss: 0.022\n",
      "model: 8, epoch: 478, loss: 0.022\n",
      "model: 8, epoch: 479, loss: 0.022\n",
      "model: 8, epoch: 480, loss: 0.022\n",
      "model: 8, epoch: 481, loss: 0.022\n",
      "model: 8, epoch: 482, loss: 0.022\n",
      "model: 8, epoch: 483, loss: 0.022\n",
      "model: 8, epoch: 484, loss: 0.022\n",
      "model: 8, epoch: 485, loss: 0.022\n",
      "model: 8, epoch: 486, loss: 0.022\n",
      "model: 8, epoch: 487, loss: 0.022\n",
      "model: 8, epoch: 488, loss: 0.022\n",
      "model: 8, epoch: 489, loss: 0.022\n",
      "model: 8, epoch: 490, loss: 0.022\n",
      "model: 8, epoch: 491, loss: 0.022\n",
      "model: 8, epoch: 492, loss: 0.022\n",
      "model: 8, epoch: 493, loss: 0.022\n",
      "model: 8, epoch: 494, loss: 0.022\n",
      "model: 8, epoch: 495, loss: 0.022\n",
      "model: 8, epoch: 496, loss: 0.022\n",
      "model: 8, epoch: 497, loss: 0.022\n",
      "model: 8, epoch: 498, loss: 0.022\n",
      "model: 8, epoch: 499, loss: 0.022\n",
      "model: 8, epoch: 500, loss: 0.022\n",
      "model: 8, epoch: 501, loss: 0.022\n",
      "model: 8, epoch: 502, loss: 0.022\n",
      "model: 8, epoch: 503, loss: 0.022\n",
      "model: 8, epoch: 504, loss: 0.022\n",
      "model: 8, epoch: 505, loss: 0.022\n",
      "model: 8, epoch: 506, loss: 0.022\n",
      "model: 8, epoch: 507, loss: 0.022\n",
      "model: 8, epoch: 508, loss: 0.022\n",
      "model: 8, epoch: 509, loss: 0.022\n",
      "model: 8, epoch: 510, loss: 0.022\n",
      "model: 8, epoch: 511, loss: 0.022\n",
      "model: 8, epoch: 512, loss: 0.022\n",
      "model: 8, epoch: 513, loss: 0.022\n",
      "model: 8, epoch: 514, loss: 0.021\n",
      "model: 8, epoch: 515, loss: 0.021\n",
      "model: 8, epoch: 516, loss: 0.021\n",
      "model: 8, epoch: 517, loss: 0.021\n",
      "model: 8, epoch: 518, loss: 0.021\n",
      "model: 8, epoch: 519, loss: 0.021\n",
      "model: 8, epoch: 520, loss: 0.021\n",
      "model: 8, epoch: 521, loss: 0.021\n",
      "model: 8, epoch: 522, loss: 0.021\n",
      "model: 8, epoch: 523, loss: 0.021\n",
      "model: 8, epoch: 524, loss: 0.021\n",
      "model: 8, epoch: 525, loss: 0.021\n",
      "model: 8, epoch: 526, loss: 0.021\n",
      "model: 8, epoch: 527, loss: 0.021\n",
      "model: 8, epoch: 528, loss: 0.021\n",
      "model: 8, epoch: 529, loss: 0.021\n",
      "model: 8, epoch: 530, loss: 0.021\n",
      "model: 8, epoch: 531, loss: 0.021\n",
      "model: 8, epoch: 532, loss: 0.021\n",
      "model: 8, epoch: 533, loss: 0.021\n",
      "model: 8, epoch: 534, loss: 0.021\n",
      "model: 8, epoch: 535, loss: 0.021\n",
      "model: 8, epoch: 536, loss: 0.021\n",
      "model: 8, epoch: 537, loss: 0.021\n",
      "model: 8, epoch: 538, loss: 0.021\n",
      "model: 8, epoch: 539, loss: 0.021\n",
      "model: 8, epoch: 540, loss: 0.021\n",
      "model: 8, epoch: 541, loss: 0.021\n",
      "model: 8, epoch: 542, loss: 0.021\n",
      "model: 8, epoch: 543, loss: 0.021\n",
      "model: 8, epoch: 544, loss: 0.021\n",
      "model: 8, epoch: 545, loss: 0.021\n",
      "model: 8, epoch: 546, loss: 0.021\n",
      "model: 8, epoch: 547, loss: 0.021\n",
      "model: 8, epoch: 548, loss: 0.021\n",
      "model: 8, epoch: 549, loss: 0.021\n",
      "model: 8, epoch: 550, loss: 0.021\n",
      "model: 8, epoch: 551, loss: 0.021\n",
      "model: 8, epoch: 552, loss: 0.020\n",
      "model: 8, epoch: 553, loss: 0.020\n",
      "model: 8, epoch: 554, loss: 0.020\n",
      "model: 8, epoch: 555, loss: 0.020\n",
      "model: 8, epoch: 556, loss: 0.020\n",
      "model: 8, epoch: 557, loss: 0.020\n",
      "model: 8, epoch: 558, loss: 0.020\n",
      "model: 8, epoch: 559, loss: 0.020\n",
      "model: 8, epoch: 560, loss: 0.020\n",
      "model: 8, epoch: 561, loss: 0.020\n",
      "model: 8, epoch: 562, loss: 0.020\n",
      "model: 8, epoch: 563, loss: 0.020\n",
      "model: 8, epoch: 564, loss: 0.020\n",
      "model: 8, epoch: 565, loss: 0.020\n",
      "model: 8, epoch: 566, loss: 0.020\n",
      "model: 8, epoch: 567, loss: 0.020\n",
      "model: 8, epoch: 568, loss: 0.020\n",
      "model: 8, epoch: 569, loss: 0.020\n",
      "model: 8, epoch: 570, loss: 0.020\n",
      "model: 8, epoch: 571, loss: 0.020\n",
      "model: 8, epoch: 572, loss: 0.020\n",
      "model: 8, epoch: 573, loss: 0.020\n",
      "model: 8, epoch: 574, loss: 0.020\n",
      "model: 8, epoch: 575, loss: 0.020\n",
      "model: 8, epoch: 576, loss: 0.020\n",
      "model: 8, epoch: 577, loss: 0.020\n",
      "model: 8, epoch: 578, loss: 0.020\n",
      "model: 8, epoch: 579, loss: 0.020\n",
      "model: 8, epoch: 580, loss: 0.020\n",
      "model: 8, epoch: 581, loss: 0.020\n",
      "model: 8, epoch: 582, loss: 0.020\n",
      "model: 8, epoch: 583, loss: 0.020\n",
      "model: 8, epoch: 584, loss: 0.020\n",
      "model: 8, epoch: 585, loss: 0.020\n",
      "model: 8, epoch: 586, loss: 0.020\n",
      "model: 8, epoch: 587, loss: 0.019\n",
      "model: 8, epoch: 588, loss: 0.019\n",
      "model: 8, epoch: 589, loss: 0.019\n",
      "model: 8, epoch: 590, loss: 0.019\n",
      "model: 8, epoch: 591, loss: 0.019\n",
      "model: 8, epoch: 592, loss: 0.019\n",
      "model: 8, epoch: 593, loss: 0.019\n",
      "model: 8, epoch: 594, loss: 0.019\n",
      "model: 8, epoch: 595, loss: 0.019\n",
      "model: 8, epoch: 596, loss: 0.019\n",
      "model: 8, epoch: 597, loss: 0.019\n",
      "model: 8, epoch: 598, loss: 0.019\n",
      "model: 8, epoch: 599, loss: 0.019\n",
      "model: 8, epoch: 600, loss: 0.019\n",
      "model: 8, epoch: 601, loss: 0.019\n",
      "model: 8, epoch: 602, loss: 0.019\n",
      "model: 8, epoch: 603, loss: 0.019\n",
      "model: 8, epoch: 604, loss: 0.019\n",
      "model: 8, epoch: 605, loss: 0.019\n",
      "model: 8, epoch: 606, loss: 0.019\n",
      "model: 8, epoch: 607, loss: 0.019\n",
      "model: 8, epoch: 608, loss: 0.019\n",
      "model: 8, epoch: 609, loss: 0.019\n",
      "model: 8, epoch: 610, loss: 0.019\n",
      "model: 8, epoch: 611, loss: 0.019\n",
      "model: 8, epoch: 612, loss: 0.019\n",
      "model: 8, epoch: 613, loss: 0.019\n",
      "model: 8, epoch: 614, loss: 0.019\n",
      "model: 8, epoch: 615, loss: 0.019\n",
      "model: 8, epoch: 616, loss: 0.019\n",
      "model: 8, epoch: 617, loss: 0.019\n",
      "model: 8, epoch: 618, loss: 0.019\n",
      "model: 8, epoch: 619, loss: 0.019\n",
      "model: 8, epoch: 620, loss: 0.018\n",
      "model: 8, epoch: 621, loss: 0.018\n",
      "model: 8, epoch: 622, loss: 0.018\n",
      "model: 8, epoch: 623, loss: 0.018\n",
      "model: 8, epoch: 624, loss: 0.018\n",
      "model: 8, epoch: 625, loss: 0.018\n",
      "model: 8, epoch: 626, loss: 0.018\n",
      "model: 8, epoch: 627, loss: 0.018\n",
      "model: 8, epoch: 628, loss: 0.018\n",
      "model: 8, epoch: 629, loss: 0.018\n",
      "model: 8, epoch: 630, loss: 0.018\n",
      "model: 8, epoch: 631, loss: 0.018\n",
      "model: 8, epoch: 632, loss: 0.018\n",
      "model: 8, epoch: 633, loss: 0.018\n",
      "model: 8, epoch: 634, loss: 0.018\n",
      "model: 8, epoch: 635, loss: 0.018\n",
      "model: 8, epoch: 636, loss: 0.018\n",
      "model: 8, epoch: 637, loss: 0.018\n",
      "model: 8, epoch: 638, loss: 0.018\n",
      "model: 8, epoch: 639, loss: 0.018\n",
      "model: 8, epoch: 640, loss: 0.018\n",
      "model: 8, epoch: 641, loss: 0.018\n",
      "model: 8, epoch: 642, loss: 0.018\n",
      "model: 8, epoch: 643, loss: 0.018\n",
      "model: 8, epoch: 644, loss: 0.018\n",
      "model: 8, epoch: 645, loss: 0.018\n",
      "model: 8, epoch: 646, loss: 0.018\n",
      "model: 8, epoch: 647, loss: 0.018\n",
      "model: 8, epoch: 648, loss: 0.018\n",
      "model: 8, epoch: 649, loss: 0.018\n",
      "model: 8, epoch: 650, loss: 0.017\n",
      "model: 8, epoch: 651, loss: 0.017\n",
      "model: 8, epoch: 652, loss: 0.017\n",
      "model: 8, epoch: 653, loss: 0.017\n",
      "model: 8, epoch: 654, loss: 0.017\n",
      "model: 8, epoch: 655, loss: 0.017\n",
      "model: 8, epoch: 656, loss: 0.017\n",
      "model: 8, epoch: 657, loss: 0.017\n",
      "model: 8, epoch: 658, loss: 0.017\n",
      "model: 8, epoch: 659, loss: 0.017\n",
      "model: 8, epoch: 660, loss: 0.017\n",
      "model: 8, epoch: 661, loss: 0.017\n",
      "model: 8, epoch: 662, loss: 0.017\n",
      "model: 8, epoch: 663, loss: 0.017\n",
      "model: 8, epoch: 664, loss: 0.017\n",
      "model: 8, epoch: 665, loss: 0.017\n",
      "model: 8, epoch: 666, loss: 0.017\n",
      "model: 8, epoch: 667, loss: 0.017\n",
      "model: 8, epoch: 668, loss: 0.017\n",
      "model: 8, epoch: 669, loss: 0.017\n",
      "model: 8, epoch: 670, loss: 0.017\n",
      "model: 8, epoch: 671, loss: 0.017\n",
      "model: 8, epoch: 672, loss: 0.017\n",
      "model: 8, epoch: 673, loss: 0.017\n",
      "model: 8, epoch: 674, loss: 0.017\n",
      "model: 8, epoch: 675, loss: 0.017\n",
      "model: 8, epoch: 676, loss: 0.017\n",
      "model: 8, epoch: 677, loss: 0.017\n",
      "model: 8, epoch: 678, loss: 0.017\n",
      "model: 8, epoch: 679, loss: 0.017\n",
      "model: 8, epoch: 680, loss: 0.016\n",
      "model: 8, epoch: 681, loss: 0.016\n",
      "model: 8, epoch: 682, loss: 0.016\n",
      "model: 8, epoch: 683, loss: 0.016\n",
      "model: 8, epoch: 684, loss: 0.016\n",
      "model: 8, epoch: 685, loss: 0.016\n",
      "model: 8, epoch: 686, loss: 0.016\n",
      "model: 8, epoch: 687, loss: 0.016\n",
      "model: 8, epoch: 688, loss: 0.016\n",
      "model: 8, epoch: 689, loss: 0.016\n",
      "model: 8, epoch: 690, loss: 0.016\n",
      "model: 8, epoch: 691, loss: 0.016\n",
      "model: 8, epoch: 692, loss: 0.016\n",
      "model: 8, epoch: 693, loss: 0.016\n",
      "model: 8, epoch: 694, loss: 0.016\n",
      "model: 8, epoch: 695, loss: 0.016\n",
      "model: 8, epoch: 696, loss: 0.016\n",
      "model: 8, epoch: 697, loss: 0.016\n",
      "model: 8, epoch: 698, loss: 0.016\n",
      "model: 8, epoch: 699, loss: 0.016\n",
      "model: 8, epoch: 700, loss: 0.016\n",
      "model: 8, epoch: 701, loss: 0.016\n",
      "model: 8, epoch: 702, loss: 0.016\n",
      "model: 8, epoch: 703, loss: 0.016\n",
      "model: 8, epoch: 704, loss: 0.016\n",
      "model: 8, epoch: 705, loss: 0.016\n",
      "model: 8, epoch: 706, loss: 0.016\n",
      "model: 8, epoch: 707, loss: 0.016\n",
      "model: 8, epoch: 708, loss: 0.015\n",
      "model: 8, epoch: 709, loss: 0.015\n",
      "model: 8, epoch: 710, loss: 0.015\n",
      "model: 8, epoch: 711, loss: 0.015\n",
      "model: 8, epoch: 712, loss: 0.015\n",
      "model: 8, epoch: 713, loss: 0.015\n",
      "model: 8, epoch: 714, loss: 0.015\n",
      "model: 8, epoch: 715, loss: 0.015\n",
      "model: 8, epoch: 716, loss: 0.015\n",
      "model: 8, epoch: 717, loss: 0.015\n",
      "model: 8, epoch: 718, loss: 0.015\n",
      "model: 8, epoch: 719, loss: 0.015\n",
      "model: 8, epoch: 720, loss: 0.015\n",
      "model: 8, epoch: 721, loss: 0.015\n",
      "model: 8, epoch: 722, loss: 0.015\n",
      "model: 8, epoch: 723, loss: 0.015\n",
      "model: 8, epoch: 724, loss: 0.015\n",
      "model: 8, epoch: 725, loss: 0.015\n",
      "model: 8, epoch: 726, loss: 0.015\n",
      "model: 8, epoch: 727, loss: 0.015\n",
      "model: 8, epoch: 728, loss: 0.015\n",
      "model: 8, epoch: 729, loss: 0.015\n",
      "model: 8, epoch: 730, loss: 0.015\n",
      "model: 8, epoch: 731, loss: 0.015\n",
      "model: 8, epoch: 732, loss: 0.015\n",
      "model: 8, epoch: 733, loss: 0.015\n",
      "model: 8, epoch: 734, loss: 0.015\n",
      "model: 8, epoch: 735, loss: 0.015\n",
      "model: 8, epoch: 736, loss: 0.014\n",
      "model: 8, epoch: 737, loss: 0.014\n",
      "model: 8, epoch: 738, loss: 0.014\n",
      "model: 8, epoch: 739, loss: 0.014\n",
      "model: 8, epoch: 740, loss: 0.014\n",
      "model: 8, epoch: 741, loss: 0.014\n",
      "model: 8, epoch: 742, loss: 0.014\n",
      "model: 8, epoch: 743, loss: 0.014\n",
      "model: 8, epoch: 744, loss: 0.014\n",
      "model: 8, epoch: 745, loss: 0.014\n",
      "model: 8, epoch: 746, loss: 0.014\n",
      "model: 8, epoch: 747, loss: 0.014\n",
      "model: 8, epoch: 748, loss: 0.014\n",
      "model: 8, epoch: 749, loss: 0.014\n",
      "model: 8, epoch: 750, loss: 0.014\n",
      "model: 8, epoch: 751, loss: 0.014\n",
      "model: 8, epoch: 752, loss: 0.014\n",
      "model: 8, epoch: 753, loss: 0.014\n",
      "model: 8, epoch: 754, loss: 0.014\n",
      "model: 8, epoch: 755, loss: 0.014\n",
      "model: 8, epoch: 756, loss: 0.014\n",
      "model: 8, epoch: 757, loss: 0.014\n",
      "model: 8, epoch: 758, loss: 0.014\n",
      "model: 8, epoch: 759, loss: 0.014\n",
      "model: 8, epoch: 760, loss: 0.014\n",
      "model: 8, epoch: 761, loss: 0.014\n",
      "model: 8, epoch: 762, loss: 0.014\n",
      "model: 8, epoch: 763, loss: 0.014\n",
      "model: 8, epoch: 764, loss: 0.014\n",
      "model: 8, epoch: 765, loss: 0.013\n",
      "model: 8, epoch: 766, loss: 0.013\n",
      "model: 8, epoch: 767, loss: 0.013\n",
      "model: 8, epoch: 768, loss: 0.013\n",
      "model: 8, epoch: 769, loss: 0.013\n",
      "model: 8, epoch: 770, loss: 0.013\n",
      "model: 8, epoch: 771, loss: 0.013\n",
      "model: 8, epoch: 772, loss: 0.013\n",
      "model: 8, epoch: 773, loss: 0.013\n",
      "model: 8, epoch: 774, loss: 0.013\n",
      "model: 8, epoch: 775, loss: 0.013\n",
      "model: 8, epoch: 776, loss: 0.013\n",
      "model: 8, epoch: 777, loss: 0.013\n",
      "model: 8, epoch: 778, loss: 0.013\n",
      "model: 8, epoch: 779, loss: 0.013\n",
      "model: 8, epoch: 780, loss: 0.013\n",
      "model: 8, epoch: 781, loss: 0.013\n",
      "model: 8, epoch: 782, loss: 0.013\n",
      "model: 8, epoch: 783, loss: 0.013\n",
      "model: 8, epoch: 784, loss: 0.013\n",
      "model: 8, epoch: 785, loss: 0.013\n",
      "model: 8, epoch: 786, loss: 0.013\n",
      "model: 8, epoch: 787, loss: 0.013\n",
      "model: 8, epoch: 788, loss: 0.013\n",
      "model: 8, epoch: 789, loss: 0.013\n",
      "model: 8, epoch: 790, loss: 0.013\n",
      "model: 8, epoch: 791, loss: 0.013\n",
      "model: 8, epoch: 792, loss: 0.013\n",
      "model: 8, epoch: 793, loss: 0.013\n",
      "model: 8, epoch: 794, loss: 0.012\n",
      "model: 8, epoch: 795, loss: 0.012\n",
      "model: 8, epoch: 796, loss: 0.012\n",
      "model: 8, epoch: 797, loss: 0.012\n",
      "model: 8, epoch: 798, loss: 0.012\n",
      "model: 8, epoch: 799, loss: 0.012\n",
      "model: 8, epoch: 800, loss: 0.012\n",
      "model: 8, epoch: 801, loss: 0.012\n",
      "model: 8, epoch: 802, loss: 0.012\n",
      "model: 8, epoch: 803, loss: 0.012\n",
      "model: 8, epoch: 804, loss: 0.012\n",
      "model: 8, epoch: 805, loss: 0.012\n",
      "model: 8, epoch: 806, loss: 0.012\n",
      "model: 8, epoch: 807, loss: 0.012\n",
      "model: 8, epoch: 808, loss: 0.012\n",
      "model: 8, epoch: 809, loss: 0.012\n",
      "model: 8, epoch: 810, loss: 0.012\n",
      "model: 8, epoch: 811, loss: 0.012\n",
      "model: 8, epoch: 812, loss: 0.012\n",
      "model: 8, epoch: 813, loss: 0.012\n",
      "model: 8, epoch: 814, loss: 0.012\n",
      "model: 8, epoch: 815, loss: 0.012\n",
      "model: 8, epoch: 816, loss: 0.012\n",
      "model: 8, epoch: 817, loss: 0.012\n",
      "model: 8, epoch: 818, loss: 0.012\n",
      "model: 8, epoch: 819, loss: 0.012\n",
      "model: 8, epoch: 820, loss: 0.012\n",
      "model: 8, epoch: 821, loss: 0.012\n",
      "model: 8, epoch: 822, loss: 0.012\n",
      "model: 8, epoch: 823, loss: 0.012\n",
      "model: 8, epoch: 824, loss: 0.012\n",
      "model: 8, epoch: 825, loss: 0.011\n",
      "model: 8, epoch: 826, loss: 0.011\n",
      "model: 8, epoch: 827, loss: 0.011\n",
      "model: 8, epoch: 828, loss: 0.011\n",
      "model: 8, epoch: 829, loss: 0.011\n",
      "model: 8, epoch: 830, loss: 0.011\n",
      "model: 8, epoch: 831, loss: 0.011\n",
      "model: 8, epoch: 832, loss: 0.011\n",
      "model: 8, epoch: 833, loss: 0.011\n",
      "model: 8, epoch: 834, loss: 0.011\n",
      "model: 8, epoch: 835, loss: 0.011\n",
      "model: 8, epoch: 836, loss: 0.011\n",
      "model: 8, epoch: 837, loss: 0.011\n",
      "model: 8, epoch: 838, loss: 0.011\n",
      "model: 8, epoch: 839, loss: 0.011\n",
      "model: 8, epoch: 840, loss: 0.011\n",
      "model: 8, epoch: 841, loss: 0.011\n",
      "model: 8, epoch: 842, loss: 0.011\n",
      "model: 8, epoch: 843, loss: 0.011\n",
      "model: 8, epoch: 844, loss: 0.011\n",
      "model: 8, epoch: 845, loss: 0.011\n",
      "model: 8, epoch: 846, loss: 0.011\n",
      "model: 8, epoch: 847, loss: 0.011\n",
      "model: 8, epoch: 848, loss: 0.011\n",
      "model: 8, epoch: 849, loss: 0.011\n",
      "model: 8, epoch: 850, loss: 0.011\n",
      "model: 8, epoch: 851, loss: 0.011\n",
      "model: 8, epoch: 852, loss: 0.011\n",
      "model: 8, epoch: 853, loss: 0.011\n",
      "model: 8, epoch: 854, loss: 0.011\n",
      "model: 8, epoch: 855, loss: 0.011\n",
      "model: 8, epoch: 856, loss: 0.011\n",
      "model: 8, epoch: 857, loss: 0.011\n",
      "model: 8, epoch: 858, loss: 0.011\n",
      "model: 8, epoch: 859, loss: 0.010\n",
      "model: 8, epoch: 860, loss: 0.010\n",
      "model: 8, epoch: 861, loss: 0.010\n",
      "model: 8, epoch: 862, loss: 0.010\n",
      "model: 8, epoch: 863, loss: 0.010\n",
      "model: 8, epoch: 864, loss: 0.010\n",
      "model: 8, epoch: 865, loss: 0.010\n",
      "model: 8, epoch: 866, loss: 0.010\n",
      "model: 8, epoch: 867, loss: 0.010\n",
      "model: 8, epoch: 868, loss: 0.010\n",
      "model: 8, epoch: 869, loss: 0.010\n",
      "model: 8, epoch: 870, loss: 0.010\n",
      "model: 8, epoch: 871, loss: 0.010\n",
      "model: 8, epoch: 872, loss: 0.010\n",
      "model: 8, epoch: 873, loss: 0.010\n",
      "model: 8, epoch: 874, loss: 0.010\n",
      "model: 8, epoch: 875, loss: 0.010\n",
      "model: 8, epoch: 876, loss: 0.010\n",
      "model: 8, epoch: 877, loss: 0.010\n",
      "model: 8, epoch: 878, loss: 0.010\n",
      "model: 8, epoch: 879, loss: 0.010\n",
      "model: 8, epoch: 880, loss: 0.010\n",
      "model: 8, epoch: 881, loss: 0.010\n",
      "model: 8, epoch: 882, loss: 0.010\n",
      "model: 8, epoch: 883, loss: 0.010\n",
      "model: 8, epoch: 884, loss: 0.010\n",
      "model: 8, epoch: 885, loss: 0.010\n",
      "model: 8, epoch: 886, loss: 0.010\n",
      "model: 8, epoch: 887, loss: 0.010\n",
      "model: 8, epoch: 888, loss: 0.010\n",
      "model: 8, epoch: 889, loss: 0.010\n",
      "model: 8, epoch: 890, loss: 0.010\n",
      "model: 8, epoch: 891, loss: 0.010\n",
      "model: 8, epoch: 892, loss: 0.010\n",
      "model: 8, epoch: 893, loss: 0.010\n",
      "model: 8, epoch: 894, loss: 0.010\n",
      "model: 8, epoch: 895, loss: 0.010\n",
      "model: 8, epoch: 896, loss: 0.010\n",
      "model: 8, epoch: 897, loss: 0.010\n",
      "model: 8, epoch: 898, loss: 0.009\n",
      "model: 8, epoch: 899, loss: 0.009\n",
      "model: 8, epoch: 900, loss: 0.009\n",
      "model: 8, epoch: 901, loss: 0.009\n",
      "model: 8, epoch: 902, loss: 0.009\n",
      "model: 8, epoch: 903, loss: 0.009\n",
      "model: 8, epoch: 904, loss: 0.009\n",
      "model: 8, epoch: 905, loss: 0.009\n",
      "model: 8, epoch: 906, loss: 0.009\n",
      "model: 8, epoch: 907, loss: 0.009\n",
      "model: 8, epoch: 908, loss: 0.009\n",
      "model: 8, epoch: 909, loss: 0.009\n",
      "model: 8, epoch: 910, loss: 0.009\n",
      "model: 8, epoch: 911, loss: 0.009\n",
      "model: 8, epoch: 912, loss: 0.009\n",
      "model: 8, epoch: 913, loss: 0.009\n",
      "model: 8, epoch: 914, loss: 0.009\n",
      "model: 8, epoch: 915, loss: 0.009\n",
      "model: 8, epoch: 916, loss: 0.009\n",
      "model: 8, epoch: 917, loss: 0.009\n",
      "model: 8, epoch: 918, loss: 0.009\n",
      "model: 8, epoch: 919, loss: 0.009\n",
      "model: 8, epoch: 920, loss: 0.009\n",
      "model: 8, epoch: 921, loss: 0.009\n",
      "model: 8, epoch: 922, loss: 0.009\n",
      "model: 8, epoch: 923, loss: 0.009\n",
      "model: 8, epoch: 924, loss: 0.009\n",
      "model: 8, epoch: 925, loss: 0.009\n",
      "model: 8, epoch: 926, loss: 0.009\n",
      "model: 8, epoch: 927, loss: 0.009\n",
      "model: 8, epoch: 928, loss: 0.009\n",
      "model: 8, epoch: 929, loss: 0.009\n",
      "model: 8, epoch: 930, loss: 0.009\n",
      "model: 8, epoch: 931, loss: 0.009\n",
      "model: 8, epoch: 932, loss: 0.009\n",
      "model: 8, epoch: 933, loss: 0.009\n",
      "model: 8, epoch: 934, loss: 0.009\n",
      "model: 8, epoch: 935, loss: 0.009\n",
      "model: 8, epoch: 936, loss: 0.009\n",
      "model: 8, epoch: 937, loss: 0.009\n",
      "model: 8, epoch: 938, loss: 0.009\n",
      "model: 8, epoch: 939, loss: 0.009\n",
      "model: 8, epoch: 940, loss: 0.009\n",
      "model: 8, epoch: 941, loss: 0.009\n",
      "model: 8, epoch: 942, loss: 0.009\n",
      "model: 8, epoch: 943, loss: 0.009\n",
      "model: 8, epoch: 944, loss: 0.009\n",
      "model: 8, epoch: 945, loss: 0.009\n",
      "model: 8, epoch: 946, loss: 0.009\n",
      "model: 8, epoch: 947, loss: 0.009\n",
      "model: 8, epoch: 948, loss: 0.008\n",
      "model: 8, epoch: 949, loss: 0.008\n",
      "model: 8, epoch: 950, loss: 0.008\n",
      "model: 8, epoch: 951, loss: 0.008\n",
      "model: 8, epoch: 952, loss: 0.008\n",
      "model: 8, epoch: 953, loss: 0.008\n",
      "model: 8, epoch: 954, loss: 0.008\n",
      "model: 8, epoch: 955, loss: 0.008\n",
      "model: 8, epoch: 956, loss: 0.008\n",
      "model: 8, epoch: 957, loss: 0.008\n",
      "model: 8, epoch: 958, loss: 0.008\n",
      "model: 8, epoch: 959, loss: 0.008\n",
      "model: 8, epoch: 960, loss: 0.008\n",
      "model: 8, epoch: 961, loss: 0.008\n",
      "model: 8, epoch: 962, loss: 0.008\n",
      "model: 8, epoch: 963, loss: 0.008\n",
      "model: 8, epoch: 964, loss: 0.008\n",
      "model: 8, epoch: 965, loss: 0.008\n",
      "model: 8, epoch: 966, loss: 0.008\n",
      "model: 8, epoch: 967, loss: 0.008\n",
      "model: 8, epoch: 968, loss: 0.008\n",
      "model: 8, epoch: 969, loss: 0.008\n",
      "model: 8, epoch: 970, loss: 0.008\n",
      "model: 8, epoch: 971, loss: 0.008\n",
      "model: 8, epoch: 972, loss: 0.008\n",
      "model: 8, epoch: 973, loss: 0.008\n",
      "model: 8, epoch: 974, loss: 0.008\n",
      "model: 8, epoch: 975, loss: 0.008\n",
      "model: 8, epoch: 976, loss: 0.008\n",
      "model: 8, epoch: 977, loss: 0.008\n",
      "model: 8, epoch: 978, loss: 0.008\n",
      "model: 8, epoch: 979, loss: 0.008\n",
      "model: 8, epoch: 980, loss: 0.008\n",
      "model: 8, epoch: 981, loss: 0.008\n",
      "model: 8, epoch: 982, loss: 0.008\n",
      "model: 8, epoch: 983, loss: 0.008\n",
      "model: 8, epoch: 984, loss: 0.008\n",
      "model: 8, epoch: 985, loss: 0.008\n",
      "model: 8, epoch: 986, loss: 0.008\n",
      "model: 8, epoch: 987, loss: 0.008\n",
      "model: 8, epoch: 988, loss: 0.008\n",
      "model: 8, epoch: 989, loss: 0.008\n",
      "model: 8, epoch: 990, loss: 0.008\n",
      "model: 8, epoch: 991, loss: 0.008\n",
      "model: 8, epoch: 992, loss: 0.008\n",
      "model: 8, epoch: 993, loss: 0.008\n",
      "model: 8, epoch: 994, loss: 0.008\n",
      "model: 8, epoch: 995, loss: 0.008\n",
      "model: 8, epoch: 996, loss: 0.008\n",
      "model: 8, epoch: 997, loss: 0.008\n",
      "model: 8, epoch: 998, loss: 0.008\n",
      "model: 8, epoch: 999, loss: 0.008\n",
      "model: 9, epoch: 0, loss: 0.390\n",
      "model: 9, epoch: 1, loss: 0.347\n",
      "model: 9, epoch: 2, loss: 0.309\n",
      "model: 9, epoch: 3, loss: 0.276\n",
      "model: 9, epoch: 4, loss: 0.248\n",
      "model: 9, epoch: 5, loss: 0.225\n",
      "model: 9, epoch: 6, loss: 0.208\n",
      "model: 9, epoch: 7, loss: 0.195\n",
      "model: 9, epoch: 8, loss: 0.186\n",
      "model: 9, epoch: 9, loss: 0.181\n",
      "model: 9, epoch: 10, loss: 0.179\n",
      "model: 9, epoch: 11, loss: 0.178\n",
      "model: 9, epoch: 12, loss: 0.179\n",
      "model: 9, epoch: 13, loss: 0.181\n",
      "model: 9, epoch: 14, loss: 0.183\n",
      "model: 9, epoch: 15, loss: 0.184\n",
      "model: 9, epoch: 16, loss: 0.184\n",
      "model: 9, epoch: 17, loss: 0.183\n",
      "model: 9, epoch: 18, loss: 0.181\n",
      "model: 9, epoch: 19, loss: 0.178\n",
      "model: 9, epoch: 20, loss: 0.175\n",
      "model: 9, epoch: 21, loss: 0.171\n",
      "model: 9, epoch: 22, loss: 0.167\n",
      "model: 9, epoch: 23, loss: 0.163\n",
      "model: 9, epoch: 24, loss: 0.159\n",
      "model: 9, epoch: 25, loss: 0.155\n",
      "model: 9, epoch: 26, loss: 0.152\n",
      "model: 9, epoch: 27, loss: 0.149\n",
      "model: 9, epoch: 28, loss: 0.147\n",
      "model: 9, epoch: 29, loss: 0.145\n",
      "model: 9, epoch: 30, loss: 0.143\n",
      "model: 9, epoch: 31, loss: 0.142\n",
      "model: 9, epoch: 32, loss: 0.140\n",
      "model: 9, epoch: 33, loss: 0.139\n",
      "model: 9, epoch: 34, loss: 0.138\n",
      "model: 9, epoch: 35, loss: 0.136\n",
      "model: 9, epoch: 36, loss: 0.134\n",
      "model: 9, epoch: 37, loss: 0.133\n",
      "model: 9, epoch: 38, loss: 0.131\n",
      "model: 9, epoch: 39, loss: 0.129\n",
      "model: 9, epoch: 40, loss: 0.127\n",
      "model: 9, epoch: 41, loss: 0.125\n",
      "model: 9, epoch: 42, loss: 0.123\n",
      "model: 9, epoch: 43, loss: 0.121\n",
      "model: 9, epoch: 44, loss: 0.119\n",
      "model: 9, epoch: 45, loss: 0.117\n",
      "model: 9, epoch: 46, loss: 0.115\n",
      "model: 9, epoch: 47, loss: 0.114\n",
      "model: 9, epoch: 48, loss: 0.112\n",
      "model: 9, epoch: 49, loss: 0.110\n",
      "model: 9, epoch: 50, loss: 0.109\n",
      "model: 9, epoch: 51, loss: 0.107\n",
      "model: 9, epoch: 52, loss: 0.106\n",
      "model: 9, epoch: 53, loss: 0.105\n",
      "model: 9, epoch: 54, loss: 0.103\n",
      "model: 9, epoch: 55, loss: 0.102\n",
      "model: 9, epoch: 56, loss: 0.100\n",
      "model: 9, epoch: 57, loss: 0.098\n",
      "model: 9, epoch: 58, loss: 0.097\n",
      "model: 9, epoch: 59, loss: 0.095\n",
      "model: 9, epoch: 60, loss: 0.094\n",
      "model: 9, epoch: 61, loss: 0.093\n",
      "model: 9, epoch: 62, loss: 0.091\n",
      "model: 9, epoch: 63, loss: 0.090\n",
      "model: 9, epoch: 64, loss: 0.089\n",
      "model: 9, epoch: 65, loss: 0.087\n",
      "model: 9, epoch: 66, loss: 0.086\n",
      "model: 9, epoch: 67, loss: 0.085\n",
      "model: 9, epoch: 68, loss: 0.083\n",
      "model: 9, epoch: 69, loss: 0.082\n",
      "model: 9, epoch: 70, loss: 0.081\n",
      "model: 9, epoch: 71, loss: 0.080\n",
      "model: 9, epoch: 72, loss: 0.079\n",
      "model: 9, epoch: 73, loss: 0.077\n",
      "model: 9, epoch: 74, loss: 0.076\n",
      "model: 9, epoch: 75, loss: 0.075\n",
      "model: 9, epoch: 76, loss: 0.074\n",
      "model: 9, epoch: 77, loss: 0.073\n",
      "model: 9, epoch: 78, loss: 0.072\n",
      "model: 9, epoch: 79, loss: 0.071\n",
      "model: 9, epoch: 80, loss: 0.070\n",
      "model: 9, epoch: 81, loss: 0.069\n",
      "model: 9, epoch: 82, loss: 0.068\n",
      "model: 9, epoch: 83, loss: 0.067\n",
      "model: 9, epoch: 84, loss: 0.066\n",
      "model: 9, epoch: 85, loss: 0.065\n",
      "model: 9, epoch: 86, loss: 0.064\n",
      "model: 9, epoch: 87, loss: 0.063\n",
      "model: 9, epoch: 88, loss: 0.062\n",
      "model: 9, epoch: 89, loss: 0.061\n",
      "model: 9, epoch: 90, loss: 0.060\n",
      "model: 9, epoch: 91, loss: 0.059\n",
      "model: 9, epoch: 92, loss: 0.059\n",
      "model: 9, epoch: 93, loss: 0.058\n",
      "model: 9, epoch: 94, loss: 0.057\n",
      "model: 9, epoch: 95, loss: 0.056\n",
      "model: 9, epoch: 96, loss: 0.055\n",
      "model: 9, epoch: 97, loss: 0.055\n",
      "model: 9, epoch: 98, loss: 0.054\n",
      "model: 9, epoch: 99, loss: 0.053\n",
      "model: 9, epoch: 100, loss: 0.053\n",
      "model: 9, epoch: 101, loss: 0.052\n",
      "model: 9, epoch: 102, loss: 0.051\n",
      "model: 9, epoch: 103, loss: 0.051\n",
      "model: 9, epoch: 104, loss: 0.050\n",
      "model: 9, epoch: 105, loss: 0.049\n",
      "model: 9, epoch: 106, loss: 0.049\n",
      "model: 9, epoch: 107, loss: 0.048\n",
      "model: 9, epoch: 108, loss: 0.047\n",
      "model: 9, epoch: 109, loss: 0.047\n",
      "model: 9, epoch: 110, loss: 0.046\n",
      "model: 9, epoch: 111, loss: 0.046\n",
      "model: 9, epoch: 112, loss: 0.045\n",
      "model: 9, epoch: 113, loss: 0.045\n",
      "model: 9, epoch: 114, loss: 0.044\n",
      "model: 9, epoch: 115, loss: 0.044\n",
      "model: 9, epoch: 116, loss: 0.043\n",
      "model: 9, epoch: 117, loss: 0.043\n",
      "model: 9, epoch: 118, loss: 0.042\n",
      "model: 9, epoch: 119, loss: 0.042\n",
      "model: 9, epoch: 120, loss: 0.041\n",
      "model: 9, epoch: 121, loss: 0.041\n",
      "model: 9, epoch: 122, loss: 0.041\n",
      "model: 9, epoch: 123, loss: 0.040\n",
      "model: 9, epoch: 124, loss: 0.040\n",
      "model: 9, epoch: 125, loss: 0.039\n",
      "model: 9, epoch: 126, loss: 0.039\n",
      "model: 9, epoch: 127, loss: 0.039\n",
      "model: 9, epoch: 128, loss: 0.038\n",
      "model: 9, epoch: 129, loss: 0.038\n",
      "model: 9, epoch: 130, loss: 0.038\n",
      "model: 9, epoch: 131, loss: 0.037\n",
      "model: 9, epoch: 132, loss: 0.037\n",
      "model: 9, epoch: 133, loss: 0.037\n",
      "model: 9, epoch: 134, loss: 0.036\n",
      "model: 9, epoch: 135, loss: 0.036\n",
      "model: 9, epoch: 136, loss: 0.036\n",
      "model: 9, epoch: 137, loss: 0.036\n",
      "model: 9, epoch: 138, loss: 0.035\n",
      "model: 9, epoch: 139, loss: 0.035\n",
      "model: 9, epoch: 140, loss: 0.035\n",
      "model: 9, epoch: 141, loss: 0.035\n",
      "model: 9, epoch: 142, loss: 0.034\n",
      "model: 9, epoch: 143, loss: 0.034\n",
      "model: 9, epoch: 144, loss: 0.034\n",
      "model: 9, epoch: 145, loss: 0.034\n",
      "model: 9, epoch: 146, loss: 0.033\n",
      "model: 9, epoch: 147, loss: 0.033\n",
      "model: 9, epoch: 148, loss: 0.033\n",
      "model: 9, epoch: 149, loss: 0.033\n",
      "model: 9, epoch: 150, loss: 0.033\n",
      "model: 9, epoch: 151, loss: 0.033\n",
      "model: 9, epoch: 152, loss: 0.032\n",
      "model: 9, epoch: 153, loss: 0.032\n",
      "model: 9, epoch: 154, loss: 0.032\n",
      "model: 9, epoch: 155, loss: 0.032\n",
      "model: 9, epoch: 156, loss: 0.032\n",
      "model: 9, epoch: 157, loss: 0.032\n",
      "model: 9, epoch: 158, loss: 0.031\n",
      "model: 9, epoch: 159, loss: 0.031\n",
      "model: 9, epoch: 160, loss: 0.031\n",
      "model: 9, epoch: 161, loss: 0.031\n",
      "model: 9, epoch: 162, loss: 0.031\n",
      "model: 9, epoch: 163, loss: 0.031\n",
      "model: 9, epoch: 164, loss: 0.031\n",
      "model: 9, epoch: 165, loss: 0.031\n",
      "model: 9, epoch: 166, loss: 0.031\n",
      "model: 9, epoch: 167, loss: 0.030\n",
      "model: 9, epoch: 168, loss: 0.030\n",
      "model: 9, epoch: 169, loss: 0.030\n",
      "model: 9, epoch: 170, loss: 0.030\n",
      "model: 9, epoch: 171, loss: 0.030\n",
      "model: 9, epoch: 172, loss: 0.030\n",
      "model: 9, epoch: 173, loss: 0.030\n",
      "model: 9, epoch: 174, loss: 0.030\n",
      "model: 9, epoch: 175, loss: 0.030\n",
      "model: 9, epoch: 176, loss: 0.030\n",
      "model: 9, epoch: 177, loss: 0.030\n",
      "model: 9, epoch: 178, loss: 0.030\n",
      "model: 9, epoch: 179, loss: 0.030\n",
      "model: 9, epoch: 180, loss: 0.030\n",
      "model: 9, epoch: 181, loss: 0.030\n",
      "model: 9, epoch: 182, loss: 0.029\n",
      "model: 9, epoch: 183, loss: 0.029\n",
      "model: 9, epoch: 184, loss: 0.029\n",
      "model: 9, epoch: 185, loss: 0.029\n",
      "model: 9, epoch: 186, loss: 0.029\n",
      "model: 9, epoch: 187, loss: 0.029\n",
      "model: 9, epoch: 188, loss: 0.029\n",
      "model: 9, epoch: 189, loss: 0.029\n",
      "model: 9, epoch: 190, loss: 0.029\n",
      "model: 9, epoch: 191, loss: 0.029\n",
      "model: 9, epoch: 192, loss: 0.029\n",
      "model: 9, epoch: 193, loss: 0.029\n",
      "model: 9, epoch: 194, loss: 0.029\n",
      "model: 9, epoch: 195, loss: 0.029\n",
      "model: 9, epoch: 196, loss: 0.029\n",
      "model: 9, epoch: 197, loss: 0.029\n",
      "model: 9, epoch: 198, loss: 0.029\n",
      "model: 9, epoch: 199, loss: 0.029\n",
      "model: 9, epoch: 200, loss: 0.029\n",
      "model: 9, epoch: 201, loss: 0.029\n",
      "model: 9, epoch: 202, loss: 0.029\n",
      "model: 9, epoch: 203, loss: 0.029\n",
      "model: 9, epoch: 204, loss: 0.029\n",
      "model: 9, epoch: 205, loss: 0.029\n",
      "model: 9, epoch: 206, loss: 0.029\n",
      "model: 9, epoch: 207, loss: 0.029\n",
      "model: 9, epoch: 208, loss: 0.029\n",
      "model: 9, epoch: 209, loss: 0.029\n",
      "model: 9, epoch: 210, loss: 0.029\n",
      "model: 9, epoch: 211, loss: 0.029\n",
      "model: 9, epoch: 212, loss: 0.029\n",
      "model: 9, epoch: 213, loss: 0.029\n",
      "model: 9, epoch: 214, loss: 0.029\n",
      "model: 9, epoch: 215, loss: 0.029\n",
      "model: 9, epoch: 216, loss: 0.029\n",
      "model: 9, epoch: 217, loss: 0.029\n",
      "model: 9, epoch: 218, loss: 0.029\n",
      "model: 9, epoch: 219, loss: 0.029\n",
      "model: 9, epoch: 220, loss: 0.029\n",
      "model: 9, epoch: 221, loss: 0.029\n",
      "model: 9, epoch: 222, loss: 0.029\n",
      "model: 9, epoch: 223, loss: 0.029\n",
      "model: 9, epoch: 224, loss: 0.029\n",
      "model: 9, epoch: 225, loss: 0.029\n",
      "model: 9, epoch: 226, loss: 0.029\n",
      "model: 9, epoch: 227, loss: 0.029\n",
      "model: 9, epoch: 228, loss: 0.029\n",
      "model: 9, epoch: 229, loss: 0.029\n",
      "model: 9, epoch: 230, loss: 0.029\n",
      "model: 9, epoch: 231, loss: 0.029\n",
      "model: 9, epoch: 232, loss: 0.029\n",
      "model: 9, epoch: 233, loss: 0.029\n",
      "model: 9, epoch: 234, loss: 0.029\n",
      "model: 9, epoch: 235, loss: 0.029\n",
      "model: 9, epoch: 236, loss: 0.029\n",
      "model: 9, epoch: 237, loss: 0.029\n",
      "model: 9, epoch: 238, loss: 0.029\n",
      "model: 9, epoch: 239, loss: 0.029\n",
      "model: 9, epoch: 240, loss: 0.029\n",
      "model: 9, epoch: 241, loss: 0.029\n",
      "model: 9, epoch: 242, loss: 0.029\n",
      "model: 9, epoch: 243, loss: 0.029\n",
      "model: 9, epoch: 244, loss: 0.029\n",
      "model: 9, epoch: 245, loss: 0.029\n",
      "model: 9, epoch: 246, loss: 0.029\n",
      "model: 9, epoch: 247, loss: 0.029\n",
      "model: 9, epoch: 248, loss: 0.029\n",
      "model: 9, epoch: 249, loss: 0.029\n",
      "model: 9, epoch: 250, loss: 0.029\n",
      "model: 9, epoch: 251, loss: 0.029\n",
      "model: 9, epoch: 252, loss: 0.029\n",
      "model: 9, epoch: 253, loss: 0.029\n",
      "model: 9, epoch: 254, loss: 0.029\n",
      "model: 9, epoch: 255, loss: 0.029\n",
      "model: 9, epoch: 256, loss: 0.029\n",
      "model: 9, epoch: 257, loss: 0.029\n",
      "model: 9, epoch: 258, loss: 0.029\n",
      "model: 9, epoch: 259, loss: 0.029\n",
      "model: 9, epoch: 260, loss: 0.029\n",
      "model: 9, epoch: 261, loss: 0.029\n",
      "model: 9, epoch: 262, loss: 0.029\n",
      "model: 9, epoch: 263, loss: 0.029\n",
      "model: 9, epoch: 264, loss: 0.029\n",
      "model: 9, epoch: 265, loss: 0.029\n",
      "model: 9, epoch: 266, loss: 0.029\n",
      "model: 9, epoch: 267, loss: 0.029\n",
      "model: 9, epoch: 268, loss: 0.029\n",
      "model: 9, epoch: 269, loss: 0.029\n",
      "model: 9, epoch: 270, loss: 0.029\n",
      "model: 9, epoch: 271, loss: 0.028\n",
      "model: 9, epoch: 272, loss: 0.028\n",
      "model: 9, epoch: 273, loss: 0.028\n",
      "model: 9, epoch: 274, loss: 0.028\n",
      "model: 9, epoch: 275, loss: 0.028\n",
      "model: 9, epoch: 276, loss: 0.028\n",
      "model: 9, epoch: 277, loss: 0.028\n",
      "model: 9, epoch: 278, loss: 0.028\n",
      "model: 9, epoch: 279, loss: 0.028\n",
      "model: 9, epoch: 280, loss: 0.028\n",
      "model: 9, epoch: 281, loss: 0.028\n",
      "model: 9, epoch: 282, loss: 0.028\n",
      "model: 9, epoch: 283, loss: 0.028\n",
      "model: 9, epoch: 284, loss: 0.028\n",
      "model: 9, epoch: 285, loss: 0.028\n",
      "model: 9, epoch: 286, loss: 0.028\n",
      "model: 9, epoch: 287, loss: 0.028\n",
      "model: 9, epoch: 288, loss: 0.028\n",
      "model: 9, epoch: 289, loss: 0.028\n",
      "model: 9, epoch: 290, loss: 0.028\n",
      "model: 9, epoch: 291, loss: 0.028\n",
      "model: 9, epoch: 292, loss: 0.028\n",
      "model: 9, epoch: 293, loss: 0.028\n",
      "model: 9, epoch: 294, loss: 0.028\n",
      "model: 9, epoch: 295, loss: 0.028\n",
      "model: 9, epoch: 296, loss: 0.028\n",
      "model: 9, epoch: 297, loss: 0.028\n",
      "model: 9, epoch: 298, loss: 0.028\n",
      "model: 9, epoch: 299, loss: 0.028\n",
      "model: 9, epoch: 300, loss: 0.028\n",
      "model: 9, epoch: 301, loss: 0.028\n",
      "model: 9, epoch: 302, loss: 0.028\n",
      "model: 9, epoch: 303, loss: 0.028\n",
      "model: 9, epoch: 304, loss: 0.028\n",
      "model: 9, epoch: 305, loss: 0.028\n",
      "model: 9, epoch: 306, loss: 0.028\n",
      "model: 9, epoch: 307, loss: 0.028\n",
      "model: 9, epoch: 308, loss: 0.028\n",
      "model: 9, epoch: 309, loss: 0.028\n",
      "model: 9, epoch: 310, loss: 0.028\n",
      "model: 9, epoch: 311, loss: 0.028\n",
      "model: 9, epoch: 312, loss: 0.028\n",
      "model: 9, epoch: 313, loss: 0.028\n",
      "model: 9, epoch: 314, loss: 0.028\n",
      "model: 9, epoch: 315, loss: 0.028\n",
      "model: 9, epoch: 316, loss: 0.028\n",
      "model: 9, epoch: 317, loss: 0.028\n",
      "model: 9, epoch: 318, loss: 0.028\n",
      "model: 9, epoch: 319, loss: 0.028\n",
      "model: 9, epoch: 320, loss: 0.028\n",
      "model: 9, epoch: 321, loss: 0.028\n",
      "model: 9, epoch: 322, loss: 0.028\n",
      "model: 9, epoch: 323, loss: 0.028\n",
      "model: 9, epoch: 324, loss: 0.028\n",
      "model: 9, epoch: 325, loss: 0.028\n",
      "model: 9, epoch: 326, loss: 0.028\n",
      "model: 9, epoch: 327, loss: 0.028\n",
      "model: 9, epoch: 328, loss: 0.028\n",
      "model: 9, epoch: 329, loss: 0.028\n",
      "model: 9, epoch: 330, loss: 0.028\n",
      "model: 9, epoch: 331, loss: 0.028\n",
      "model: 9, epoch: 332, loss: 0.028\n",
      "model: 9, epoch: 333, loss: 0.028\n",
      "model: 9, epoch: 334, loss: 0.028\n",
      "model: 9, epoch: 335, loss: 0.028\n",
      "model: 9, epoch: 336, loss: 0.028\n",
      "model: 9, epoch: 337, loss: 0.028\n",
      "model: 9, epoch: 338, loss: 0.028\n",
      "model: 9, epoch: 339, loss: 0.028\n",
      "model: 9, epoch: 340, loss: 0.028\n",
      "model: 9, epoch: 341, loss: 0.028\n",
      "model: 9, epoch: 342, loss: 0.028\n",
      "model: 9, epoch: 343, loss: 0.028\n",
      "model: 9, epoch: 344, loss: 0.028\n",
      "model: 9, epoch: 345, loss: 0.028\n",
      "model: 9, epoch: 346, loss: 0.028\n",
      "model: 9, epoch: 347, loss: 0.028\n",
      "model: 9, epoch: 348, loss: 0.028\n",
      "model: 9, epoch: 349, loss: 0.028\n",
      "model: 9, epoch: 350, loss: 0.028\n",
      "model: 9, epoch: 351, loss: 0.028\n",
      "model: 9, epoch: 352, loss: 0.028\n",
      "model: 9, epoch: 353, loss: 0.028\n",
      "model: 9, epoch: 354, loss: 0.028\n",
      "model: 9, epoch: 355, loss: 0.028\n",
      "model: 9, epoch: 356, loss: 0.028\n",
      "model: 9, epoch: 357, loss: 0.028\n",
      "model: 9, epoch: 358, loss: 0.028\n",
      "model: 9, epoch: 359, loss: 0.028\n",
      "model: 9, epoch: 360, loss: 0.028\n",
      "model: 9, epoch: 361, loss: 0.028\n",
      "model: 9, epoch: 362, loss: 0.028\n",
      "model: 9, epoch: 363, loss: 0.028\n",
      "model: 9, epoch: 364, loss: 0.028\n",
      "model: 9, epoch: 365, loss: 0.028\n",
      "model: 9, epoch: 366, loss: 0.028\n",
      "model: 9, epoch: 367, loss: 0.028\n",
      "model: 9, epoch: 368, loss: 0.028\n",
      "model: 9, epoch: 369, loss: 0.028\n",
      "model: 9, epoch: 370, loss: 0.028\n",
      "model: 9, epoch: 371, loss: 0.028\n",
      "model: 9, epoch: 372, loss: 0.028\n",
      "model: 9, epoch: 373, loss: 0.028\n",
      "model: 9, epoch: 374, loss: 0.028\n",
      "model: 9, epoch: 375, loss: 0.028\n",
      "model: 9, epoch: 376, loss: 0.028\n",
      "model: 9, epoch: 377, loss: 0.028\n",
      "model: 9, epoch: 378, loss: 0.028\n",
      "model: 9, epoch: 379, loss: 0.028\n",
      "model: 9, epoch: 380, loss: 0.028\n",
      "model: 9, epoch: 381, loss: 0.028\n",
      "model: 9, epoch: 382, loss: 0.028\n",
      "model: 9, epoch: 383, loss: 0.028\n",
      "model: 9, epoch: 384, loss: 0.028\n",
      "model: 9, epoch: 385, loss: 0.028\n",
      "model: 9, epoch: 386, loss: 0.028\n",
      "model: 9, epoch: 387, loss: 0.028\n",
      "model: 9, epoch: 388, loss: 0.028\n",
      "model: 9, epoch: 389, loss: 0.028\n",
      "model: 9, epoch: 390, loss: 0.028\n",
      "model: 9, epoch: 391, loss: 0.028\n",
      "model: 9, epoch: 392, loss: 0.028\n",
      "model: 9, epoch: 393, loss: 0.028\n",
      "model: 9, epoch: 394, loss: 0.028\n",
      "model: 9, epoch: 395, loss: 0.028\n",
      "model: 9, epoch: 396, loss: 0.028\n",
      "model: 9, epoch: 397, loss: 0.028\n",
      "model: 9, epoch: 398, loss: 0.028\n",
      "model: 9, epoch: 399, loss: 0.028\n",
      "model: 9, epoch: 400, loss: 0.028\n",
      "model: 9, epoch: 401, loss: 0.028\n",
      "model: 9, epoch: 402, loss: 0.028\n",
      "model: 9, epoch: 403, loss: 0.028\n",
      "model: 9, epoch: 404, loss: 0.028\n",
      "model: 9, epoch: 405, loss: 0.028\n",
      "model: 9, epoch: 406, loss: 0.028\n",
      "model: 9, epoch: 407, loss: 0.028\n",
      "model: 9, epoch: 408, loss: 0.028\n",
      "model: 9, epoch: 409, loss: 0.028\n",
      "model: 9, epoch: 410, loss: 0.028\n",
      "model: 9, epoch: 411, loss: 0.028\n",
      "model: 9, epoch: 412, loss: 0.028\n",
      "model: 9, epoch: 413, loss: 0.028\n",
      "model: 9, epoch: 414, loss: 0.028\n",
      "model: 9, epoch: 415, loss: 0.028\n",
      "model: 9, epoch: 416, loss: 0.028\n",
      "model: 9, epoch: 417, loss: 0.028\n",
      "model: 9, epoch: 418, loss: 0.028\n",
      "model: 9, epoch: 419, loss: 0.028\n",
      "model: 9, epoch: 420, loss: 0.028\n",
      "model: 9, epoch: 421, loss: 0.028\n",
      "model: 9, epoch: 422, loss: 0.028\n",
      "model: 9, epoch: 423, loss: 0.028\n",
      "model: 9, epoch: 424, loss: 0.028\n",
      "model: 9, epoch: 425, loss: 0.028\n",
      "model: 9, epoch: 426, loss: 0.028\n",
      "model: 9, epoch: 427, loss: 0.028\n",
      "model: 9, epoch: 428, loss: 0.028\n",
      "model: 9, epoch: 429, loss: 0.028\n",
      "model: 9, epoch: 430, loss: 0.028\n",
      "model: 9, epoch: 431, loss: 0.028\n",
      "model: 9, epoch: 432, loss: 0.028\n",
      "model: 9, epoch: 433, loss: 0.028\n",
      "model: 9, epoch: 434, loss: 0.028\n",
      "model: 9, epoch: 435, loss: 0.028\n",
      "model: 9, epoch: 436, loss: 0.028\n",
      "model: 9, epoch: 437, loss: 0.028\n",
      "model: 9, epoch: 438, loss: 0.028\n",
      "model: 9, epoch: 439, loss: 0.028\n",
      "model: 9, epoch: 440, loss: 0.028\n",
      "model: 9, epoch: 441, loss: 0.028\n",
      "model: 9, epoch: 442, loss: 0.028\n",
      "model: 9, epoch: 443, loss: 0.028\n",
      "model: 9, epoch: 444, loss: 0.028\n",
      "model: 9, epoch: 445, loss: 0.028\n",
      "model: 9, epoch: 446, loss: 0.028\n",
      "model: 9, epoch: 447, loss: 0.028\n",
      "model: 9, epoch: 448, loss: 0.028\n",
      "model: 9, epoch: 449, loss: 0.028\n",
      "model: 9, epoch: 450, loss: 0.028\n",
      "model: 9, epoch: 451, loss: 0.028\n",
      "model: 9, epoch: 452, loss: 0.028\n",
      "model: 9, epoch: 453, loss: 0.028\n",
      "model: 9, epoch: 454, loss: 0.028\n",
      "model: 9, epoch: 455, loss: 0.028\n",
      "model: 9, epoch: 456, loss: 0.028\n",
      "model: 9, epoch: 457, loss: 0.028\n",
      "model: 9, epoch: 458, loss: 0.028\n",
      "model: 9, epoch: 459, loss: 0.028\n",
      "model: 9, epoch: 460, loss: 0.028\n",
      "model: 9, epoch: 461, loss: 0.028\n",
      "model: 9, epoch: 462, loss: 0.028\n",
      "model: 9, epoch: 463, loss: 0.028\n",
      "model: 9, epoch: 464, loss: 0.028\n",
      "model: 9, epoch: 465, loss: 0.028\n",
      "model: 9, epoch: 466, loss: 0.028\n",
      "model: 9, epoch: 467, loss: 0.028\n",
      "model: 9, epoch: 468, loss: 0.028\n",
      "model: 9, epoch: 469, loss: 0.028\n",
      "model: 9, epoch: 470, loss: 0.028\n",
      "model: 9, epoch: 471, loss: 0.028\n",
      "model: 9, epoch: 472, loss: 0.028\n",
      "model: 9, epoch: 473, loss: 0.028\n",
      "model: 9, epoch: 474, loss: 0.028\n",
      "model: 9, epoch: 475, loss: 0.028\n",
      "model: 9, epoch: 476, loss: 0.028\n",
      "model: 9, epoch: 477, loss: 0.028\n",
      "model: 9, epoch: 478, loss: 0.028\n",
      "model: 9, epoch: 479, loss: 0.028\n",
      "model: 9, epoch: 480, loss: 0.028\n",
      "model: 9, epoch: 481, loss: 0.028\n",
      "model: 9, epoch: 482, loss: 0.028\n",
      "model: 9, epoch: 483, loss: 0.028\n",
      "model: 9, epoch: 484, loss: 0.028\n",
      "model: 9, epoch: 485, loss: 0.028\n",
      "model: 9, epoch: 486, loss: 0.028\n",
      "model: 9, epoch: 487, loss: 0.028\n",
      "model: 9, epoch: 488, loss: 0.028\n",
      "model: 9, epoch: 489, loss: 0.028\n",
      "model: 9, epoch: 490, loss: 0.028\n",
      "model: 9, epoch: 491, loss: 0.028\n",
      "model: 9, epoch: 492, loss: 0.028\n",
      "model: 9, epoch: 493, loss: 0.028\n",
      "model: 9, epoch: 494, loss: 0.028\n",
      "model: 9, epoch: 495, loss: 0.028\n",
      "model: 9, epoch: 496, loss: 0.028\n",
      "model: 9, epoch: 497, loss: 0.028\n",
      "model: 9, epoch: 498, loss: 0.028\n",
      "model: 9, epoch: 499, loss: 0.028\n",
      "model: 9, epoch: 500, loss: 0.028\n",
      "model: 9, epoch: 501, loss: 0.028\n",
      "model: 9, epoch: 502, loss: 0.028\n",
      "model: 9, epoch: 503, loss: 0.028\n",
      "model: 9, epoch: 504, loss: 0.028\n",
      "model: 9, epoch: 505, loss: 0.028\n",
      "model: 9, epoch: 506, loss: 0.028\n",
      "model: 9, epoch: 507, loss: 0.028\n",
      "model: 9, epoch: 508, loss: 0.028\n",
      "model: 9, epoch: 509, loss: 0.028\n",
      "model: 9, epoch: 510, loss: 0.028\n",
      "model: 9, epoch: 511, loss: 0.028\n",
      "model: 9, epoch: 512, loss: 0.028\n",
      "model: 9, epoch: 513, loss: 0.028\n",
      "model: 9, epoch: 514, loss: 0.028\n",
      "model: 9, epoch: 515, loss: 0.028\n",
      "model: 9, epoch: 516, loss: 0.028\n",
      "model: 9, epoch: 517, loss: 0.028\n",
      "model: 9, epoch: 518, loss: 0.028\n",
      "model: 9, epoch: 519, loss: 0.028\n",
      "model: 9, epoch: 520, loss: 0.028\n",
      "model: 9, epoch: 521, loss: 0.028\n",
      "model: 9, epoch: 522, loss: 0.028\n",
      "model: 9, epoch: 523, loss: 0.028\n",
      "model: 9, epoch: 524, loss: 0.028\n",
      "model: 9, epoch: 525, loss: 0.028\n",
      "model: 9, epoch: 526, loss: 0.028\n",
      "model: 9, epoch: 527, loss: 0.028\n",
      "model: 9, epoch: 528, loss: 0.028\n",
      "model: 9, epoch: 529, loss: 0.028\n",
      "model: 9, epoch: 530, loss: 0.028\n",
      "model: 9, epoch: 531, loss: 0.028\n",
      "model: 9, epoch: 532, loss: 0.028\n",
      "model: 9, epoch: 533, loss: 0.028\n",
      "model: 9, epoch: 534, loss: 0.028\n",
      "model: 9, epoch: 535, loss: 0.028\n",
      "model: 9, epoch: 536, loss: 0.028\n",
      "model: 9, epoch: 537, loss: 0.028\n",
      "model: 9, epoch: 538, loss: 0.028\n",
      "model: 9, epoch: 539, loss: 0.028\n",
      "model: 9, epoch: 540, loss: 0.028\n",
      "model: 9, epoch: 541, loss: 0.028\n",
      "model: 9, epoch: 542, loss: 0.028\n",
      "model: 9, epoch: 543, loss: 0.028\n",
      "model: 9, epoch: 544, loss: 0.028\n",
      "model: 9, epoch: 545, loss: 0.028\n",
      "model: 9, epoch: 546, loss: 0.028\n",
      "model: 9, epoch: 547, loss: 0.028\n",
      "model: 9, epoch: 548, loss: 0.028\n",
      "model: 9, epoch: 549, loss: 0.028\n",
      "model: 9, epoch: 550, loss: 0.028\n",
      "model: 9, epoch: 551, loss: 0.028\n",
      "model: 9, epoch: 552, loss: 0.028\n",
      "model: 9, epoch: 553, loss: 0.028\n",
      "model: 9, epoch: 554, loss: 0.028\n",
      "model: 9, epoch: 555, loss: 0.028\n",
      "model: 9, epoch: 556, loss: 0.028\n",
      "model: 9, epoch: 557, loss: 0.028\n",
      "model: 9, epoch: 558, loss: 0.028\n",
      "model: 9, epoch: 559, loss: 0.028\n",
      "model: 9, epoch: 560, loss: 0.028\n",
      "model: 9, epoch: 561, loss: 0.028\n",
      "model: 9, epoch: 562, loss: 0.028\n",
      "model: 9, epoch: 563, loss: 0.028\n",
      "model: 9, epoch: 564, loss: 0.028\n",
      "model: 9, epoch: 565, loss: 0.028\n",
      "model: 9, epoch: 566, loss: 0.028\n",
      "model: 9, epoch: 567, loss: 0.028\n",
      "model: 9, epoch: 568, loss: 0.028\n",
      "model: 9, epoch: 569, loss: 0.028\n",
      "model: 9, epoch: 570, loss: 0.028\n",
      "model: 9, epoch: 571, loss: 0.028\n",
      "model: 9, epoch: 572, loss: 0.028\n",
      "model: 9, epoch: 573, loss: 0.028\n",
      "model: 9, epoch: 574, loss: 0.028\n",
      "model: 9, epoch: 575, loss: 0.028\n",
      "model: 9, epoch: 576, loss: 0.028\n",
      "model: 9, epoch: 577, loss: 0.028\n",
      "model: 9, epoch: 578, loss: 0.028\n",
      "model: 9, epoch: 579, loss: 0.028\n",
      "model: 9, epoch: 580, loss: 0.028\n",
      "model: 9, epoch: 581, loss: 0.028\n",
      "model: 9, epoch: 582, loss: 0.028\n",
      "model: 9, epoch: 583, loss: 0.028\n",
      "model: 9, epoch: 584, loss: 0.028\n",
      "model: 9, epoch: 585, loss: 0.028\n",
      "model: 9, epoch: 586, loss: 0.028\n",
      "model: 9, epoch: 587, loss: 0.028\n",
      "model: 9, epoch: 588, loss: 0.028\n",
      "model: 9, epoch: 589, loss: 0.028\n",
      "model: 9, epoch: 590, loss: 0.028\n",
      "model: 9, epoch: 591, loss: 0.028\n",
      "model: 9, epoch: 592, loss: 0.028\n",
      "model: 9, epoch: 593, loss: 0.028\n",
      "model: 9, epoch: 594, loss: 0.028\n",
      "model: 9, epoch: 595, loss: 0.028\n",
      "model: 9, epoch: 596, loss: 0.028\n",
      "model: 9, epoch: 597, loss: 0.028\n",
      "model: 9, epoch: 598, loss: 0.028\n",
      "model: 9, epoch: 599, loss: 0.028\n",
      "model: 9, epoch: 600, loss: 0.028\n",
      "model: 9, epoch: 601, loss: 0.028\n",
      "model: 9, epoch: 602, loss: 0.028\n",
      "model: 9, epoch: 603, loss: 0.028\n",
      "model: 9, epoch: 604, loss: 0.028\n",
      "model: 9, epoch: 605, loss: 0.028\n",
      "model: 9, epoch: 606, loss: 0.028\n",
      "model: 9, epoch: 607, loss: 0.028\n",
      "model: 9, epoch: 608, loss: 0.028\n",
      "model: 9, epoch: 609, loss: 0.028\n",
      "model: 9, epoch: 610, loss: 0.028\n",
      "model: 9, epoch: 611, loss: 0.028\n",
      "model: 9, epoch: 612, loss: 0.028\n",
      "model: 9, epoch: 613, loss: 0.028\n",
      "model: 9, epoch: 614, loss: 0.028\n",
      "model: 9, epoch: 615, loss: 0.028\n",
      "model: 9, epoch: 616, loss: 0.028\n",
      "model: 9, epoch: 617, loss: 0.028\n",
      "model: 9, epoch: 618, loss: 0.028\n",
      "model: 9, epoch: 619, loss: 0.028\n",
      "model: 9, epoch: 620, loss: 0.028\n",
      "model: 9, epoch: 621, loss: 0.028\n",
      "model: 9, epoch: 622, loss: 0.028\n",
      "model: 9, epoch: 623, loss: 0.028\n",
      "model: 9, epoch: 624, loss: 0.028\n",
      "model: 9, epoch: 625, loss: 0.028\n",
      "model: 9, epoch: 626, loss: 0.028\n",
      "model: 9, epoch: 627, loss: 0.028\n",
      "model: 9, epoch: 628, loss: 0.028\n",
      "model: 9, epoch: 629, loss: 0.028\n",
      "model: 9, epoch: 630, loss: 0.028\n",
      "model: 9, epoch: 631, loss: 0.028\n",
      "model: 9, epoch: 632, loss: 0.028\n",
      "model: 9, epoch: 633, loss: 0.028\n",
      "model: 9, epoch: 634, loss: 0.028\n",
      "model: 9, epoch: 635, loss: 0.028\n",
      "model: 9, epoch: 636, loss: 0.028\n",
      "model: 9, epoch: 637, loss: 0.028\n",
      "model: 9, epoch: 638, loss: 0.028\n",
      "model: 9, epoch: 639, loss: 0.028\n",
      "model: 9, epoch: 640, loss: 0.028\n",
      "model: 9, epoch: 641, loss: 0.028\n",
      "model: 9, epoch: 642, loss: 0.028\n",
      "model: 9, epoch: 643, loss: 0.028\n",
      "model: 9, epoch: 644, loss: 0.028\n",
      "model: 9, epoch: 645, loss: 0.028\n",
      "model: 9, epoch: 646, loss: 0.028\n",
      "model: 9, epoch: 647, loss: 0.028\n",
      "model: 9, epoch: 648, loss: 0.028\n",
      "model: 9, epoch: 649, loss: 0.028\n",
      "model: 9, epoch: 650, loss: 0.028\n",
      "model: 9, epoch: 651, loss: 0.028\n",
      "model: 9, epoch: 652, loss: 0.028\n",
      "model: 9, epoch: 653, loss: 0.028\n",
      "model: 9, epoch: 654, loss: 0.028\n",
      "model: 9, epoch: 655, loss: 0.028\n",
      "model: 9, epoch: 656, loss: 0.028\n",
      "model: 9, epoch: 657, loss: 0.028\n",
      "model: 9, epoch: 658, loss: 0.028\n",
      "model: 9, epoch: 659, loss: 0.028\n",
      "model: 9, epoch: 660, loss: 0.028\n",
      "model: 9, epoch: 661, loss: 0.028\n",
      "model: 9, epoch: 662, loss: 0.028\n",
      "model: 9, epoch: 663, loss: 0.028\n",
      "model: 9, epoch: 664, loss: 0.028\n",
      "model: 9, epoch: 665, loss: 0.028\n",
      "model: 9, epoch: 666, loss: 0.028\n",
      "model: 9, epoch: 667, loss: 0.028\n",
      "model: 9, epoch: 668, loss: 0.028\n",
      "model: 9, epoch: 669, loss: 0.028\n",
      "model: 9, epoch: 670, loss: 0.028\n",
      "model: 9, epoch: 671, loss: 0.028\n",
      "model: 9, epoch: 672, loss: 0.028\n",
      "model: 9, epoch: 673, loss: 0.028\n",
      "model: 9, epoch: 674, loss: 0.028\n",
      "model: 9, epoch: 675, loss: 0.028\n",
      "model: 9, epoch: 676, loss: 0.028\n",
      "model: 9, epoch: 677, loss: 0.028\n",
      "model: 9, epoch: 678, loss: 0.028\n",
      "model: 9, epoch: 679, loss: 0.028\n",
      "model: 9, epoch: 680, loss: 0.028\n",
      "model: 9, epoch: 681, loss: 0.028\n",
      "model: 9, epoch: 682, loss: 0.028\n",
      "model: 9, epoch: 683, loss: 0.028\n",
      "model: 9, epoch: 684, loss: 0.028\n",
      "model: 9, epoch: 685, loss: 0.028\n",
      "model: 9, epoch: 686, loss: 0.028\n",
      "model: 9, epoch: 687, loss: 0.028\n",
      "model: 9, epoch: 688, loss: 0.028\n",
      "model: 9, epoch: 689, loss: 0.028\n",
      "model: 9, epoch: 690, loss: 0.028\n",
      "model: 9, epoch: 691, loss: 0.028\n",
      "model: 9, epoch: 692, loss: 0.028\n",
      "model: 9, epoch: 693, loss: 0.028\n",
      "model: 9, epoch: 694, loss: 0.028\n",
      "model: 9, epoch: 695, loss: 0.028\n",
      "model: 9, epoch: 696, loss: 0.028\n",
      "model: 9, epoch: 697, loss: 0.028\n",
      "model: 9, epoch: 698, loss: 0.028\n",
      "model: 9, epoch: 699, loss: 0.028\n",
      "model: 9, epoch: 700, loss: 0.028\n",
      "model: 9, epoch: 701, loss: 0.028\n",
      "model: 9, epoch: 702, loss: 0.028\n",
      "model: 9, epoch: 703, loss: 0.028\n",
      "model: 9, epoch: 704, loss: 0.028\n",
      "model: 9, epoch: 705, loss: 0.028\n",
      "model: 9, epoch: 706, loss: 0.028\n",
      "model: 9, epoch: 707, loss: 0.028\n",
      "model: 9, epoch: 708, loss: 0.028\n",
      "model: 9, epoch: 709, loss: 0.028\n",
      "model: 9, epoch: 710, loss: 0.028\n",
      "model: 9, epoch: 711, loss: 0.028\n",
      "model: 9, epoch: 712, loss: 0.028\n",
      "model: 9, epoch: 713, loss: 0.028\n",
      "model: 9, epoch: 714, loss: 0.028\n",
      "model: 9, epoch: 715, loss: 0.028\n",
      "model: 9, epoch: 716, loss: 0.028\n",
      "model: 9, epoch: 717, loss: 0.028\n",
      "model: 9, epoch: 718, loss: 0.028\n",
      "model: 9, epoch: 719, loss: 0.028\n",
      "model: 9, epoch: 720, loss: 0.028\n",
      "model: 9, epoch: 721, loss: 0.028\n",
      "model: 9, epoch: 722, loss: 0.028\n",
      "model: 9, epoch: 723, loss: 0.028\n",
      "model: 9, epoch: 724, loss: 0.028\n",
      "model: 9, epoch: 725, loss: 0.028\n",
      "model: 9, epoch: 726, loss: 0.028\n",
      "model: 9, epoch: 727, loss: 0.028\n",
      "model: 9, epoch: 728, loss: 0.028\n",
      "model: 9, epoch: 729, loss: 0.028\n",
      "model: 9, epoch: 730, loss: 0.028\n",
      "model: 9, epoch: 731, loss: 0.028\n",
      "model: 9, epoch: 732, loss: 0.028\n",
      "model: 9, epoch: 733, loss: 0.028\n",
      "model: 9, epoch: 734, loss: 0.028\n",
      "model: 9, epoch: 735, loss: 0.028\n",
      "model: 9, epoch: 736, loss: 0.028\n",
      "model: 9, epoch: 737, loss: 0.028\n",
      "model: 9, epoch: 738, loss: 0.028\n",
      "model: 9, epoch: 739, loss: 0.028\n",
      "model: 9, epoch: 740, loss: 0.028\n",
      "model: 9, epoch: 741, loss: 0.028\n",
      "model: 9, epoch: 742, loss: 0.028\n",
      "model: 9, epoch: 743, loss: 0.028\n",
      "model: 9, epoch: 744, loss: 0.028\n",
      "model: 9, epoch: 745, loss: 0.028\n",
      "model: 9, epoch: 746, loss: 0.028\n",
      "model: 9, epoch: 747, loss: 0.028\n",
      "model: 9, epoch: 748, loss: 0.028\n",
      "model: 9, epoch: 749, loss: 0.028\n",
      "model: 9, epoch: 750, loss: 0.028\n",
      "model: 9, epoch: 751, loss: 0.028\n",
      "model: 9, epoch: 752, loss: 0.028\n",
      "model: 9, epoch: 753, loss: 0.028\n",
      "model: 9, epoch: 754, loss: 0.028\n",
      "model: 9, epoch: 755, loss: 0.028\n",
      "model: 9, epoch: 756, loss: 0.028\n",
      "model: 9, epoch: 757, loss: 0.028\n",
      "model: 9, epoch: 758, loss: 0.028\n",
      "model: 9, epoch: 759, loss: 0.028\n",
      "model: 9, epoch: 760, loss: 0.028\n",
      "model: 9, epoch: 761, loss: 0.028\n",
      "model: 9, epoch: 762, loss: 0.028\n",
      "model: 9, epoch: 763, loss: 0.028\n",
      "model: 9, epoch: 764, loss: 0.028\n",
      "model: 9, epoch: 765, loss: 0.028\n",
      "model: 9, epoch: 766, loss: 0.028\n",
      "model: 9, epoch: 767, loss: 0.028\n",
      "model: 9, epoch: 768, loss: 0.028\n",
      "model: 9, epoch: 769, loss: 0.028\n",
      "model: 9, epoch: 770, loss: 0.028\n",
      "model: 9, epoch: 771, loss: 0.028\n",
      "model: 9, epoch: 772, loss: 0.028\n",
      "model: 9, epoch: 773, loss: 0.028\n",
      "model: 9, epoch: 774, loss: 0.028\n",
      "model: 9, epoch: 775, loss: 0.028\n",
      "model: 9, epoch: 776, loss: 0.028\n",
      "model: 9, epoch: 777, loss: 0.028\n",
      "model: 9, epoch: 778, loss: 0.028\n",
      "model: 9, epoch: 779, loss: 0.028\n",
      "model: 9, epoch: 780, loss: 0.028\n",
      "model: 9, epoch: 781, loss: 0.028\n",
      "model: 9, epoch: 782, loss: 0.028\n",
      "model: 9, epoch: 783, loss: 0.028\n",
      "model: 9, epoch: 784, loss: 0.028\n",
      "model: 9, epoch: 785, loss: 0.028\n",
      "model: 9, epoch: 786, loss: 0.028\n",
      "model: 9, epoch: 787, loss: 0.028\n",
      "model: 9, epoch: 788, loss: 0.028\n",
      "model: 9, epoch: 789, loss: 0.028\n",
      "model: 9, epoch: 790, loss: 0.028\n",
      "model: 9, epoch: 791, loss: 0.028\n",
      "model: 9, epoch: 792, loss: 0.028\n",
      "model: 9, epoch: 793, loss: 0.028\n",
      "model: 9, epoch: 794, loss: 0.028\n",
      "model: 9, epoch: 795, loss: 0.028\n",
      "model: 9, epoch: 796, loss: 0.028\n",
      "model: 9, epoch: 797, loss: 0.028\n",
      "model: 9, epoch: 798, loss: 0.028\n",
      "model: 9, epoch: 799, loss: 0.028\n",
      "model: 9, epoch: 800, loss: 0.028\n",
      "model: 9, epoch: 801, loss: 0.028\n",
      "model: 9, epoch: 802, loss: 0.028\n",
      "model: 9, epoch: 803, loss: 0.028\n",
      "model: 9, epoch: 804, loss: 0.028\n",
      "model: 9, epoch: 805, loss: 0.028\n",
      "model: 9, epoch: 806, loss: 0.028\n",
      "model: 9, epoch: 807, loss: 0.028\n",
      "model: 9, epoch: 808, loss: 0.028\n",
      "model: 9, epoch: 809, loss: 0.028\n",
      "model: 9, epoch: 810, loss: 0.028\n",
      "model: 9, epoch: 811, loss: 0.028\n",
      "model: 9, epoch: 812, loss: 0.028\n",
      "model: 9, epoch: 813, loss: 0.028\n",
      "model: 9, epoch: 814, loss: 0.028\n",
      "model: 9, epoch: 815, loss: 0.028\n",
      "model: 9, epoch: 816, loss: 0.028\n",
      "model: 9, epoch: 817, loss: 0.028\n",
      "model: 9, epoch: 818, loss: 0.028\n",
      "model: 9, epoch: 819, loss: 0.028\n",
      "model: 9, epoch: 820, loss: 0.028\n",
      "model: 9, epoch: 821, loss: 0.028\n",
      "model: 9, epoch: 822, loss: 0.028\n",
      "model: 9, epoch: 823, loss: 0.028\n",
      "model: 9, epoch: 824, loss: 0.028\n",
      "model: 9, epoch: 825, loss: 0.028\n",
      "model: 9, epoch: 826, loss: 0.028\n",
      "model: 9, epoch: 827, loss: 0.028\n",
      "model: 9, epoch: 828, loss: 0.028\n",
      "model: 9, epoch: 829, loss: 0.028\n",
      "model: 9, epoch: 830, loss: 0.028\n",
      "model: 9, epoch: 831, loss: 0.028\n",
      "model: 9, epoch: 832, loss: 0.028\n",
      "model: 9, epoch: 833, loss: 0.028\n",
      "model: 9, epoch: 834, loss: 0.028\n",
      "model: 9, epoch: 835, loss: 0.028\n",
      "model: 9, epoch: 836, loss: 0.028\n",
      "model: 9, epoch: 837, loss: 0.028\n",
      "model: 9, epoch: 838, loss: 0.028\n",
      "model: 9, epoch: 839, loss: 0.028\n",
      "model: 9, epoch: 840, loss: 0.028\n",
      "model: 9, epoch: 841, loss: 0.028\n",
      "model: 9, epoch: 842, loss: 0.028\n",
      "model: 9, epoch: 843, loss: 0.028\n",
      "model: 9, epoch: 844, loss: 0.028\n",
      "model: 9, epoch: 845, loss: 0.028\n",
      "model: 9, epoch: 846, loss: 0.028\n",
      "model: 9, epoch: 847, loss: 0.028\n",
      "model: 9, epoch: 848, loss: 0.028\n",
      "model: 9, epoch: 849, loss: 0.028\n",
      "model: 9, epoch: 850, loss: 0.028\n",
      "model: 9, epoch: 851, loss: 0.028\n",
      "model: 9, epoch: 852, loss: 0.028\n",
      "model: 9, epoch: 853, loss: 0.028\n",
      "model: 9, epoch: 854, loss: 0.028\n",
      "model: 9, epoch: 855, loss: 0.028\n",
      "model: 9, epoch: 856, loss: 0.028\n",
      "model: 9, epoch: 857, loss: 0.028\n",
      "model: 9, epoch: 858, loss: 0.028\n",
      "model: 9, epoch: 859, loss: 0.028\n",
      "model: 9, epoch: 860, loss: 0.028\n",
      "model: 9, epoch: 861, loss: 0.028\n",
      "model: 9, epoch: 862, loss: 0.028\n",
      "model: 9, epoch: 863, loss: 0.028\n",
      "model: 9, epoch: 864, loss: 0.028\n",
      "model: 9, epoch: 865, loss: 0.028\n",
      "model: 9, epoch: 866, loss: 0.028\n",
      "model: 9, epoch: 867, loss: 0.028\n",
      "model: 9, epoch: 868, loss: 0.028\n",
      "model: 9, epoch: 869, loss: 0.028\n",
      "model: 9, epoch: 870, loss: 0.028\n",
      "model: 9, epoch: 871, loss: 0.028\n",
      "model: 9, epoch: 872, loss: 0.028\n",
      "model: 9, epoch: 873, loss: 0.028\n",
      "model: 9, epoch: 874, loss: 0.028\n",
      "model: 9, epoch: 875, loss: 0.028\n",
      "model: 9, epoch: 876, loss: 0.028\n",
      "model: 9, epoch: 877, loss: 0.028\n",
      "model: 9, epoch: 878, loss: 0.028\n",
      "model: 9, epoch: 879, loss: 0.028\n",
      "model: 9, epoch: 880, loss: 0.028\n",
      "model: 9, epoch: 881, loss: 0.028\n",
      "model: 9, epoch: 882, loss: 0.028\n",
      "model: 9, epoch: 883, loss: 0.028\n",
      "model: 9, epoch: 884, loss: 0.028\n",
      "model: 9, epoch: 885, loss: 0.028\n",
      "model: 9, epoch: 886, loss: 0.028\n",
      "model: 9, epoch: 887, loss: 0.028\n",
      "model: 9, epoch: 888, loss: 0.028\n",
      "model: 9, epoch: 889, loss: 0.028\n",
      "model: 9, epoch: 890, loss: 0.028\n",
      "model: 9, epoch: 891, loss: 0.028\n",
      "model: 9, epoch: 892, loss: 0.028\n",
      "model: 9, epoch: 893, loss: 0.028\n",
      "model: 9, epoch: 894, loss: 0.028\n",
      "model: 9, epoch: 895, loss: 0.028\n",
      "model: 9, epoch: 896, loss: 0.028\n",
      "model: 9, epoch: 897, loss: 0.028\n",
      "model: 9, epoch: 898, loss: 0.028\n",
      "model: 9, epoch: 899, loss: 0.028\n",
      "model: 9, epoch: 900, loss: 0.028\n",
      "model: 9, epoch: 901, loss: 0.028\n",
      "model: 9, epoch: 902, loss: 0.028\n",
      "model: 9, epoch: 903, loss: 0.028\n",
      "model: 9, epoch: 904, loss: 0.028\n",
      "model: 9, epoch: 905, loss: 0.028\n",
      "model: 9, epoch: 906, loss: 0.028\n",
      "model: 9, epoch: 907, loss: 0.028\n",
      "model: 9, epoch: 908, loss: 0.028\n",
      "model: 9, epoch: 909, loss: 0.028\n",
      "model: 9, epoch: 910, loss: 0.028\n",
      "model: 9, epoch: 911, loss: 0.028\n",
      "model: 9, epoch: 912, loss: 0.028\n",
      "model: 9, epoch: 913, loss: 0.028\n",
      "model: 9, epoch: 914, loss: 0.028\n",
      "model: 9, epoch: 915, loss: 0.028\n",
      "model: 9, epoch: 916, loss: 0.028\n",
      "model: 9, epoch: 917, loss: 0.028\n",
      "model: 9, epoch: 918, loss: 0.028\n",
      "model: 9, epoch: 919, loss: 0.028\n",
      "model: 9, epoch: 920, loss: 0.028\n",
      "model: 9, epoch: 921, loss: 0.028\n",
      "model: 9, epoch: 922, loss: 0.028\n",
      "model: 9, epoch: 923, loss: 0.028\n",
      "model: 9, epoch: 924, loss: 0.028\n",
      "model: 9, epoch: 925, loss: 0.028\n",
      "model: 9, epoch: 926, loss: 0.028\n",
      "model: 9, epoch: 927, loss: 0.028\n",
      "model: 9, epoch: 928, loss: 0.028\n",
      "model: 9, epoch: 929, loss: 0.028\n",
      "model: 9, epoch: 930, loss: 0.028\n",
      "model: 9, epoch: 931, loss: 0.028\n",
      "model: 9, epoch: 932, loss: 0.028\n",
      "model: 9, epoch: 933, loss: 0.028\n",
      "model: 9, epoch: 934, loss: 0.028\n",
      "model: 9, epoch: 935, loss: 0.028\n",
      "model: 9, epoch: 936, loss: 0.028\n",
      "model: 9, epoch: 937, loss: 0.028\n",
      "model: 9, epoch: 938, loss: 0.028\n",
      "model: 9, epoch: 939, loss: 0.028\n",
      "model: 9, epoch: 940, loss: 0.028\n",
      "model: 9, epoch: 941, loss: 0.028\n",
      "model: 9, epoch: 942, loss: 0.028\n",
      "model: 9, epoch: 943, loss: 0.028\n",
      "model: 9, epoch: 944, loss: 0.028\n",
      "model: 9, epoch: 945, loss: 0.028\n",
      "model: 9, epoch: 946, loss: 0.028\n",
      "model: 9, epoch: 947, loss: 0.028\n",
      "model: 9, epoch: 948, loss: 0.028\n",
      "model: 9, epoch: 949, loss: 0.028\n",
      "model: 9, epoch: 950, loss: 0.028\n",
      "model: 9, epoch: 951, loss: 0.028\n",
      "model: 9, epoch: 952, loss: 0.028\n",
      "model: 9, epoch: 953, loss: 0.028\n",
      "model: 9, epoch: 954, loss: 0.028\n",
      "model: 9, epoch: 955, loss: 0.028\n",
      "model: 9, epoch: 956, loss: 0.028\n",
      "model: 9, epoch: 957, loss: 0.028\n",
      "model: 9, epoch: 958, loss: 0.028\n",
      "model: 9, epoch: 959, loss: 0.028\n",
      "model: 9, epoch: 960, loss: 0.028\n",
      "model: 9, epoch: 961, loss: 0.028\n",
      "model: 9, epoch: 962, loss: 0.028\n",
      "model: 9, epoch: 963, loss: 0.028\n",
      "model: 9, epoch: 964, loss: 0.028\n",
      "model: 9, epoch: 965, loss: 0.028\n",
      "model: 9, epoch: 966, loss: 0.028\n",
      "model: 9, epoch: 967, loss: 0.028\n",
      "model: 9, epoch: 968, loss: 0.028\n",
      "model: 9, epoch: 969, loss: 0.028\n",
      "model: 9, epoch: 970, loss: 0.028\n",
      "model: 9, epoch: 971, loss: 0.028\n",
      "model: 9, epoch: 972, loss: 0.028\n",
      "model: 9, epoch: 973, loss: 0.028\n",
      "model: 9, epoch: 974, loss: 0.028\n",
      "model: 9, epoch: 975, loss: 0.028\n",
      "model: 9, epoch: 976, loss: 0.028\n",
      "model: 9, epoch: 977, loss: 0.028\n",
      "model: 9, epoch: 978, loss: 0.028\n",
      "model: 9, epoch: 979, loss: 0.028\n",
      "model: 9, epoch: 980, loss: 0.028\n",
      "model: 9, epoch: 981, loss: 0.028\n",
      "model: 9, epoch: 982, loss: 0.028\n",
      "model: 9, epoch: 983, loss: 0.028\n",
      "model: 9, epoch: 984, loss: 0.028\n",
      "model: 9, epoch: 985, loss: 0.028\n",
      "model: 9, epoch: 986, loss: 0.028\n",
      "model: 9, epoch: 987, loss: 0.028\n",
      "model: 9, epoch: 988, loss: 0.028\n",
      "model: 9, epoch: 989, loss: 0.028\n",
      "model: 9, epoch: 990, loss: 0.028\n",
      "model: 9, epoch: 991, loss: 0.028\n",
      "model: 9, epoch: 992, loss: 0.028\n",
      "model: 9, epoch: 993, loss: 0.028\n",
      "model: 9, epoch: 994, loss: 0.028\n",
      "model: 9, epoch: 995, loss: 0.028\n",
      "model: 9, epoch: 996, loss: 0.028\n",
      "model: 9, epoch: 997, loss: 0.028\n",
      "model: 9, epoch: 998, loss: 0.028\n",
      "model: 9, epoch: 999, loss: 0.028\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "n_networks = 9\n",
    "\n",
    "models = list()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "# 2번째 레이어에 비선형 활성호함수 도입 실험\n",
    "for i in range(n_networks):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "\n",
    "        nn.Linear(1, 100),\n",
    "\n",
    "        nn.ReLU() if i % 3 == 0 else nn.Tanh() if i % 3==1 else nn.Sigmoid(),\n",
    "\n",
    "        nn.Linear(100, 1)\n",
    "\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "\n",
    "    for t in range(1000):\n",
    "\n",
    "        y_pred = model(X_train)\n",
    "\n",
    "        loss = criterion(y_pred, y_train)\n",
    "    \n",
    "        print(f\"model: {i + 1}, epoch: {t}, loss: {loss.item():.3f}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0076)\n",
      "tensor(0.0148)\n",
      "tensor(0.0418)\n",
      "tensor(0.0077)\n",
      "tensor(0.0091)\n",
      "tensor(0.0424)\n",
      "tensor(0.0072)\n",
      "tensor(0.0116)\n",
      "tensor(0.0423)\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "acc=[]\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    X_test, y_test = Variable(X_test), Variable(y_test)\n",
    "\n",
    "    mse = ((torch.pow((model(X_test).data.cpu() - y_test.cpu()), 2)).sum()) / len(y_test.cpu())\n",
    "\n",
    "    acc.append(mse)\n",
    "\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "6\n",
      "tensor(0.0072)\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model=models[np.argmin(acc)]\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(np.argmin(acc))\n",
    "\n",
    "print(acc[np.argmin(acc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAt4klEQVR4nO3df1RU953/8RcDAZQKKKyMWFLc1gZUlCCKGE+N65zAht2UXesPaqJhOZieE9Q4e6ziQTCx6ZjkSNFCw9JqNz0rB9eziZslLnsINjFZJqigm5iN1u3WYNXhx3FlEnICCnz/yDeTThzUMdHRD8/HOfdEPvd9P/f9YeaE17lzZyZoaGhoSAAAAHc5S6AbAAAA+DoQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARggJdAO3y+DgoM6fP68xY8YoKCgo0O0AAIAbMDQ0pI8++kjx8fGyWK59LWbEhJrz588rISEh0G0AAICbcPbsWX3zm9+8Zs2ICTVjxoyR9NkvJTIyMsDdAACAG+F2u5WQkOD5O34tIybUfP6SU2RkJKEGAIC7zI3cOsKNwgAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGCAl0AwBwt0vc+FqgW7iuM9tyAt0CcMsRagAAuAsQnq+Pl58AAIARCDUAAMAIhBoAAGAEQg0AADACNwrDaNxYBwAjB1dqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARbirUVFVVKTExUeHh4crIyNDhw4evWb9v3z4lJSUpPDxcKSkpOnDgwLC1P/rRjxQUFKSKigqv8YsXL2r58uWKjIxUdHS0CgoK9PHHH99M+wAAwEB+h5q9e/fKbrerrKxMbW1tmjFjhrKystTZ2emzvrm5WXl5eSooKNCxY8eUm5ur3NxcnThx4qraV155Re+8847i4+Ov2rd8+XK9//77amxsVH19vQ4dOqRVq1b52z4AADCU36GmvLxchYWFys/P15QpU1RdXa3Ro0dr9+7dPut37Nih7OxsrV+/XsnJydq6davS0tJUWVnpVXfu3DmtXr1ae/bs0T333OO174MPPlBDQ4N+9atfKSMjQ/PmzdPPf/5z1dXV6fz58/4uAQAAGMivUNPf36/W1lbZbLYvJrBYZLPZ5HQ6fR7jdDq96iUpKyvLq35wcFCPPfaY1q9fr6lTp/qcIzo6Wunp6Z4xm80mi8WilpYWf5YAAAAM5de3dHd3d2tgYEBxcXFe43FxcTp58qTPY1wul896l8vl+fm5555TSEiI1qxZM+wc48eP9248JETjxo3zmudP9fX1qa+vz/Oz2+0efmEAAOCuF/B3P7W2tmrHjh36x3/8RwUFBX1t8zocDkVFRXm2hISEr21uAABw5/Er1MTGxio4OFgdHR1e4x0dHbJarT6PsVqt16x/66231NnZqXvvvVchISEKCQnRhx9+qL//+79XYmKiZ44v34h85coVXbx4cdjzFhcXq6enx7OdPXvWn6UCAIC7jF+hJjQ0VDNnzlRTU5NnbHBwUE1NTcrMzPR5TGZmple9JDU2NnrqH3vsMb377rs6fvy4Z4uPj9f69ev1H//xH545Ll26pNbWVs8cBw8e1ODgoDIyMnyeNywsTJGRkV4bAAAwl1/31EiS3W7XypUrlZ6ertmzZ6uiokK9vb3Kz8+XJK1YsUITJ06Uw+GQJK1du1bz58/X9u3blZOTo7q6Oh09elQ1NTWSpJiYGMXExHid45577pHVatV9990nSUpOTlZ2drYKCwtVXV2ty5cvq6ioSMuWLfP59m8AADDy+B1qli5dqq6uLpWWlsrlcik1NVUNDQ2em4Hb29tlsXxxAWju3Lmqra1VSUmJNm3apMmTJ2v//v2aNm2aX+fds2ePioqKtHDhQlksFi1atEg7d+70t30AAGCooKGhoaFAN3E7uN1uRUVFqaenh5eiRpDEja8FuoXrOrMtJ9At4CvieYbbYaQ+z/z5+x3wdz8BAAB8HQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARCDUAAMAIhBoAAGAEQg0AADACoQYAABiBUAMAAIxAqAEAAEYg1AAAACMQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGuKlQU1VVpcTERIWHhysjI0OHDx++Zv2+ffuUlJSk8PBwpaSk6MCBA177t2zZoqSkJEVERGjs2LGy2WxqaWnxqklMTFRQUJDXtm3btptpHwAAGMjvULN3717Z7XaVlZWpra1NM2bMUFZWljo7O33WNzc3Ky8vTwUFBTp27Jhyc3OVm5urEydOeGq++93vqrKyUu+9957efvttJSYm6qGHHlJXV5fXXM8884wuXLjg2VavXu1v+wAAwFB+h5ry8nIVFhYqPz9fU6ZMUXV1tUaPHq3du3f7rN+xY4eys7O1fv16JScna+vWrUpLS1NlZaWn5oc//KFsNpv+/M//XFOnTlV5ebncbrfeffddr7nGjBkjq9Xq2SIiIvxtHwAAGMqvUNPf36/W1lbZbLYvJrBYZLPZ5HQ6fR7jdDq96iUpKytr2Pr+/n7V1NQoKipKM2bM8Nq3bds2xcTE6P7779cLL7ygK1euDNtrX1+f3G631wYAAMwV4k9xd3e3BgYGFBcX5zUeFxenkydP+jzG5XL5rHe5XF5j9fX1WrZsmT755BNNmDBBjY2Nio2N9exfs2aN0tLSNG7cODU3N6u4uFgXLlxQeXm5z/M6HA49/fTT/iwPAADcxfwKNbfSggULdPz4cXV3d+uXv/yllixZopaWFo0fP16SZLfbPbXTp09XaGionnjiCTkcDoWFhV01X3FxsdcxbrdbCQkJt34hAAAgIPx6+Sk2NlbBwcHq6OjwGu/o6JDVavV5jNVqvaH6iIgIfec739GcOXO0a9cuhYSEaNeuXcP2kpGRoStXrujMmTM+94eFhSkyMtJrAwAA5vIr1ISGhmrmzJlqamryjA0ODqqpqUmZmZk+j8nMzPSql6TGxsZh6/903r6+vmH3Hz9+XBaLxXMlBwAAjGx+v/xkt9u1cuVKpaena/bs2aqoqFBvb6/y8/MlSStWrNDEiRPlcDgkSWvXrtX8+fO1fft25eTkqK6uTkePHlVNTY0kqbe3V88++6weeeQRTZgwQd3d3aqqqtK5c+e0ePFiSZ/dbNzS0qIFCxZozJgxcjqdWrdunR599FGNHTv26/pdAACAu5jfoWbp0qXq6upSaWmpXC6XUlNT1dDQ4LkZuL29XRbLFxeA5s6dq9raWpWUlGjTpk2aPHmy9u/fr2nTpkmSgoODdfLkSb300kvq7u5WTEyMZs2apbfeektTp06V9NlLSXV1ddqyZYv6+vo0adIkrVu3zuueGQAAMLIFDQ0NDQW6idvB7XYrKipKPT093F8zgiRufC3QLVzXmW05gW4BXxHPM9wOI/V55s/fb777CQAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARCDUAAMAIhBoAAGAEQg0AADACoQYAABiBUAMAAIxAqAEAAEYg1AAAACMQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACDcVaqqqqpSYmKjw8HBlZGTo8OHD16zft2+fkpKSFB4erpSUFB04cMBr/5YtW5SUlKSIiAiNHTtWNptNLS0tXjUXL17U8uXLFRkZqejoaBUUFOjjjz++mfYBAICB/A41e/fuld1uV1lZmdra2jRjxgxlZWWps7PTZ31zc7Py8vJUUFCgY8eOKTc3V7m5uTpx4oSn5rvf/a4qKyv13nvv6e2331ZiYqIeeughdXV1eWqWL1+u999/X42Njaqvr9ehQ4e0atWqm1gyAAAwUdDQ0NCQPwdkZGRo1qxZqqyslCQNDg4qISFBq1ev1saNG6+qX7p0qXp7e1VfX+8ZmzNnjlJTU1VdXe3zHG63W1FRUXr99de1cOFCffDBB5oyZYqOHDmi9PR0SVJDQ4Mefvhh/fGPf1R8fPx1+/58zp6eHkVGRvqzZNzFEje+FugWruvMtpxAt4CviOcZboeR+jzz5++3X1dq+vv71draKpvN9sUEFotsNpucTqfPY5xOp1e9JGVlZQ1b39/fr5qaGkVFRWnGjBmeOaKjoz2BRpJsNpssFstVL1N9rq+vT26322sDAADm8ivUdHd3a2BgQHFxcV7jcXFxcrlcPo9xuVw3VF9fX69vfOMbCg8P189+9jM1NjYqNjbWM8f48eO96kNCQjRu3Lhhz+twOBQVFeXZEhIS/FkqAAC4y9wx735asGCBjh8/rubmZmVnZ2vJkiXD3qdzI4qLi9XT0+PZzp49+zV2CwAA7jR+hZrY2FgFBwero6PDa7yjo0NWq9XnMVar9YbqIyIi9J3vfEdz5szRrl27FBISol27dnnm+HLAuXLlii5evDjsecPCwhQZGem1AQAAc/kVakJDQzVz5kw1NTV5xgYHB9XU1KTMzEyfx2RmZnrVS1JjY+Ow9X86b19fn2eOS5cuqbW11bP/4MGDGhwcVEZGhj9LAAAAhgrx9wC73a6VK1cqPT1ds2fPVkVFhXp7e5Wfny9JWrFihSZOnCiHwyFJWrt2rebPn6/t27crJydHdXV1Onr0qGpqaiRJvb29evbZZ/XII49owoQJ6u7uVlVVlc6dO6fFixdLkpKTk5Wdna3CwkJVV1fr8uXLKioq0rJly27onU8AAMB8foeapUuXqqurS6WlpXK5XEpNTVVDQ4PnZuD29nZZLF9cAJo7d65qa2tVUlKiTZs2afLkydq/f7+mTZsmSQoODtbJkyf10ksvqbu7WzExMZo1a5beeustTZ061TPPnj17VFRUpIULF8pisWjRokXauXPnV10/AAAwhN+fU3O34nNqRqaR+rkOuL14nuF2GKnPs1v2OTUAAAB3KkINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARCDUAAMAIhBoAAGAEQg0AADACoQYAABiBUAMAAIxAqAEAAEYg1AAAACMQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARbirUVFVVKTExUeHh4crIyNDhw4evWb9v3z4lJSUpPDxcKSkpOnDggGff5cuXtWHDBqWkpCgiIkLx8fFasWKFzp8/7zVHYmKigoKCvLZt27bdTPsAAMBAfoeavXv3ym63q6ysTG1tbZoxY4aysrLU2dnps765uVl5eXkqKCjQsWPHlJubq9zcXJ04cUKS9Mknn6itrU2bN29WW1ubXn75ZZ06dUqPPPLIVXM988wzunDhgmdbvXq1v+0DAABD+R1qysvLVVhYqPz8fE2ZMkXV1dUaPXq0du/e7bN+x44dys7O1vr165WcnKytW7cqLS1NlZWVkqSoqCg1NjZqyZIluu+++zRnzhxVVlaqtbVV7e3tXnONGTNGVqvVs0VERNzEkgEAgIn8CjX9/f1qbW2VzWb7YgKLRTabTU6n0+cxTqfTq16SsrKyhq2XpJ6eHgUFBSk6OtprfNu2bYqJidH999+vF154QVeuXBl2jr6+Prndbq8NAACYK8Sf4u7ubg0MDCguLs5rPC4uTidPnvR5jMvl8lnvcrl81n/66afasGGD8vLyFBkZ6Rlfs2aN0tLSNG7cODU3N6u4uFgXLlxQeXm5z3kcDoeefvppf5YHAADuYn6Fmlvt8uXLWrJkiYaGhvTiiy967bPb7Z5/T58+XaGhoXriiSfkcDgUFhZ21VzFxcVex7jdbiUkJNy65gEAQED5FWpiY2MVHBysjo4Or/GOjg5ZrVafx1it1huq/zzQfPjhhzp48KDXVRpfMjIydOXKFZ05c0b33XffVfvDwsJ8hh0AAGAmv+6pCQ0N1cyZM9XU1OQZGxwcVFNTkzIzM30ek5mZ6VUvSY2NjV71nwea06dP6/XXX1dMTMx1ezl+/LgsFovGjx/vzxIAAICh/H75yW63a+XKlUpPT9fs2bNVUVGh3t5e5efnS5JWrFihiRMnyuFwSJLWrl2r+fPna/v27crJyVFdXZ2OHj2qmpoaSZ8Fmh/84Adqa2tTfX29BgYGPPfbjBs3TqGhoXI6nWppadGCBQs0ZswYOZ1OrVu3To8++qjGjh37df0uAADAXczvULN06VJ1dXWptLRULpdLqampamho8NwM3N7eLovliwtAc+fOVW1trUpKSrRp0yZNnjxZ+/fv17Rp0yRJ586d06uvvipJSk1N9TrXb3/7Wz344IMKCwtTXV2dtmzZor6+Pk2aNEnr1q3zumcGAACMbEFDQ0NDgW7idnC73YqKilJPT89179eBORI3vhboFq7rzLacQLeAr4jnGW6Hkfo88+fvN9/9BAAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARCDUAAMAIhBoAAGAEQg0AADACoQYAABiBUAMAAIxAqAEAAEYg1AAAACMQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABghJsKNVVVVUpMTFR4eLgyMjJ0+PDha9bv27dPSUlJCg8PV0pKig4cOODZd/nyZW3YsEEpKSmKiIhQfHy8VqxYofPnz3vNcfHiRS1fvlyRkZGKjo5WQUGBPv7445tpHwAAGMjvULN3717Z7XaVlZWpra1NM2bMUFZWljo7O33WNzc3Ky8vTwUFBTp27Jhyc3OVm5urEydOSJI++eQTtbW1afPmzWpra9PLL7+sU6dO6ZFHHvGaZ/ny5Xr//ffV2Nio+vp6HTp0SKtWrbqJJQMAABMFDQ0NDflzQEZGhmbNmqXKykpJ0uDgoBISErR69Wpt3LjxqvqlS5eqt7dX9fX1nrE5c+YoNTVV1dXVPs9x5MgRzZ49Wx9++KHuvfdeffDBB5oyZYqOHDmi9PR0SVJDQ4Mefvhh/fGPf1R8fPx1+3a73YqKilJPT48iIyP9WTLuYokbXwt0C9d1ZltOoFvAV8TzDLfDSH2e+fP3268rNf39/WptbZXNZvtiAotFNptNTqfT5zFOp9OrXpKysrKGrZeknp4eBQUFKTo62jNHdHS0J9BIks1mk8ViUUtLi885+vr65Ha7vTYAAGAuv0JNd3e3BgYGFBcX5zUeFxcnl8vl8xiXy+VX/aeffqoNGzYoLy/Pk8hcLpfGjx/vVRcSEqJx48YNO4/D4VBUVJRnS0hIuKE1AgCAu9Md9e6ny5cva8mSJRoaGtKLL774leYqLi5WT0+PZzt79uzX1CUAALgThfhTHBsbq+DgYHV0dHiNd3R0yGq1+jzGarXeUP3ngebDDz/UwYMHvV43s1qtV92IfOXKFV28eHHY84aFhSksLOyG1wYAAO5ufl2pCQ0N1cyZM9XU1OQZGxwcVFNTkzIzM30ek5mZ6VUvSY2NjV71nwea06dP6/XXX1dMTMxVc1y6dEmtra2esYMHD2pwcFAZGRn+LAEAABjKrys1kmS327Vy5Uqlp6dr9uzZqqioUG9vr/Lz8yVJK1as0MSJE+VwOCRJa9eu1fz587V9+3bl5OSorq5OR48eVU1NjaTPAs0PfvADtbW1qb6+XgMDA577ZMaNG6fQ0FAlJycrOztbhYWFqq6u1uXLl1VUVKRly5bd0DufAACA+fwONUuXLlVXV5dKS0vlcrmUmpqqhoYGz83A7e3tsli+uAA0d+5c1dbWqqSkRJs2bdLkyZO1f/9+TZs2TZJ07tw5vfrqq5Kk1NRUr3P99re/1YMPPihJ2rNnj4qKirRw4UJZLBYtWrRIO3fuvJk1AwAAA/n9OTV3Kz6nZmQaqZ/rgNuL5xluh5H6PLtln1MDAABwpyLUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARCDUAAMAIhBoAAGAEQg0AADACoQYAABiBUAMAAIxAqAEAAEYg1AAAACMQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAY4aZCTVVVlRITExUeHq6MjAwdPnz4mvX79u1TUlKSwsPDlZKSogMHDnjtf/nll/XQQw8pJiZGQUFBOn78+FVzPPjggwoKCvLafvSjH91M+wAAwEB+h5q9e/fKbrerrKxMbW1tmjFjhrKystTZ2emzvrm5WXl5eSooKNCxY8eUm5ur3NxcnThxwlPT29urefPm6bnnnrvmuQsLC3XhwgXP9vzzz/vbPgAAMJTfoaa8vFyFhYXKz8/XlClTVF1drdGjR2v37t0+63fs2KHs7GytX79eycnJ2rp1q9LS0lRZWempeeyxx1RaWiqbzXbNc48ePVpWq9WzRUZG+ts+AAAwlF+hpr+/X62trV7hw2KxyGazyel0+jzG6XReFVaysrKGrb+WPXv2KDY2VtOmTVNxcbE++eSTYWv7+vrkdru9NgAAYK4Qf4q7u7s1MDCguLg4r/G4uDidPHnS5zEul8tnvcvl8qvRH/7wh/rWt76l+Ph4vfvuu9qwYYNOnTqll19+2We9w+HQ008/7dc5AADA3cuvUBNIq1at8vw7JSVFEyZM0MKFC/X73/9e3/72t6+qLy4ult1u9/zsdruVkJBwW3q92yVufC3QLVzXmW05gW4BAHCH8SvUxMbGKjg4WB0dHV7jHR0dslqtPo+xWq1+1d+ojIwMSdL//M//+Aw1YWFhCgsL+0rnAAAAdw+/7qkJDQ3VzJkz1dTU5BkbHBxUU1OTMjMzfR6TmZnpVS9JjY2Nw9bfqM/f9j1hwoSvNA8AADCD3y8/2e12rVy5Uunp6Zo9e7YqKirU29ur/Px8SdKKFSs0ceJEORwOSdLatWs1f/58bd++XTk5Oaqrq9PRo0dVU1PjmfPixYtqb2/X+fPnJUmnTp2SJM+7nH7/+9+rtrZWDz/8sGJiYvTuu+9q3bp1+t73vqfp06d/5V8CAAC4+/kdapYuXaquri6VlpbK5XIpNTVVDQ0NnpuB29vbZbF8cQFo7ty5qq2tVUlJiTZt2qTJkydr//79mjZtmqfm1Vdf9YQiSVq2bJkkqaysTFu2bFFoaKhef/11T4BKSEjQokWLVFJSctMLBwAAZgkaGhoaCnQTt4Pb7VZUVJR6enr4fJvrMOlGYZPWgjsXzzPcDiP1eebP32+++wkAABiBUAMAAIxAqAEAAEYg1AAAACMQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARQgLdAAAAt0LixtcC3cJ1ndmWE+gWjMKVGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACDcVaqqqqpSYmKjw8HBlZGTo8OHD16zft2+fkpKSFB4erpSUFB04cMBr/8svv6yHHnpIMTExCgoK0vHjx6+a49NPP9WTTz6pmJgYfeMb39CiRYvU0dFxM+0DAAAD+R1q9u7dK7vdrrKyMrW1tWnGjBnKyspSZ2enz/rm5mbl5eWpoKBAx44dU25urnJzc3XixAlPTW9vr+bNm6fnnntu2POuW7dO//Zv/6Z9+/bpzTff1Pnz5/W3f/u3/rYPAAAM5XeoKS8vV2FhofLz8zVlyhRVV1dr9OjR2r17t8/6HTt2KDs7W+vXr1dycrK2bt2qtLQ0VVZWemoee+wxlZaWymaz+Zyjp6dHu3btUnl5uf7iL/5CM2fO1K9//Ws1NzfrnXfe8XcJAADAQH6Fmv7+frW2tnqFD4vFIpvNJqfT6fMYp9N5VVjJysoatt6X1tZWXb582WuepKQk3XvvvcPO09fXJ7fb7bUBAABz+RVquru7NTAwoLi4OK/xuLg4uVwun8e4XC6/6oebIzQ0VNHR0Tc8j8PhUFRUlGdLSEi44fMBAIC7j7HvfiouLlZPT49nO3v2bKBbAgAAt1CIP8WxsbEKDg6+6l1HHR0dslqtPo+xWq1+1Q83R39/vy5duuR1teZa84SFhSksLOyGzwEAAO5ufl2pCQ0N1cyZM9XU1OQZGxwcVFNTkzIzM30ek5mZ6VUvSY2NjcPW+zJz5kzdc889XvOcOnVK7e3tfs0DAADM5deVGkmy2+1auXKl0tPTNXv2bFVUVKi3t1f5+fmSpBUrVmjixIlyOBySpLVr12r+/Pnavn27cnJyVFdXp6NHj6qmpsYz58WLF9Xe3q7z589L+iywSJ9dobFarYqKilJBQYHsdrvGjRunyMhIrV69WpmZmZozZ85X/iUAAIC7n9+hZunSperq6lJpaalcLpdSU1PV0NDguRm4vb1dFssXF4Dmzp2r2tpalZSUaNOmTZo8ebL279+vadOmeWpeffVVTyiSpGXLlkmSysrKtGXLFknSz372M1ksFi1atEh9fX3KysrSL37xi5taNAAAMI/foUaSioqKVFRU5HPfG2+8cdXY4sWLtXjx4mHne/zxx/X4449f85zh4eGqqqpSVVWVP60CAIARwth3PwEAgJHlpq7UAMBXlbjxtUC3cF1ntuUEugUAfuBKDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYga9JAO4SfK0AAFwboeZrwh8cAAACi5efAACAEQg1AADACLz8BADw4KV03M24UgMAAIxAqAEAAEYg1AAAACMQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABghJsKNVVVVUpMTFR4eLgyMjJ0+PDha9bv27dPSUlJCg8PV0pKig4cOOC1f2hoSKWlpZowYYJGjRolm82m06dPe9UkJiYqKCjIa9u2bdvNtA8AAAzkd6jZu3ev7Ha7ysrK1NbWphkzZigrK0udnZ0+65ubm5WXl6eCggIdO3ZMubm5ys3N1YkTJzw1zz//vHbu3Knq6mq1tLQoIiJCWVlZ+vTTT73meuaZZ3ThwgXPtnr1an/bBwAAhvI71JSXl6uwsFD5+fmaMmWKqqurNXr0aO3evdtn/Y4dO5Sdna3169crOTlZW7duVVpamiorKyV9dpWmoqJCJSUl+v73v6/p06frN7/5jc6fP6/9+/d7zTVmzBhZrVbPFhER4f+KAQCAkfwKNf39/WptbZXNZvtiAotFNptNTqfT5zFOp9OrXpKysrI89X/4wx/kcrm8aqKiopSRkXHVnNu2bVNMTIzuv/9+vfDCC7py5cqwvfb19cntdnttAADAXCH+FHd3d2tgYEBxcXFe43FxcTp58qTPY1wul896l8vl2f/52HA1krRmzRqlpaVp3Lhxam5uVnFxsS5cuKDy8nKf53U4HHr66af9WR4AALiL+RVqAslut3v+PX36dIWGhuqJJ56Qw+FQWFjYVfXFxcVex7jdbiUkJNyWXgEAwO3n18tPsbGxCg4OVkdHh9d4R0eHrFarz2OsVus16z//rz9zSlJGRoauXLmiM2fO+NwfFhamyMhIrw0AAJjLr1ATGhqqmTNnqqmpyTM2ODiopqYmZWZm+jwmMzPTq16SGhsbPfWTJk2S1Wr1qnG73WppaRl2Tkk6fvy4LBaLxo8f788SAACAofx++clut2vlypVKT0/X7NmzVVFRod7eXuXn50uSVqxYoYkTJ8rhcEiS1q5dq/nz52v79u3KyclRXV2djh49qpqaGklSUFCQnnrqKf3kJz/R5MmTNWnSJG3evFnx8fHKzc2V9NnNxi0tLVqwYIHGjBkjp9OpdevW6dFHH9XYsWO/pl8FAAC4m/kdapYuXaquri6VlpbK5XIpNTVVDQ0Nnht929vbZbF8cQFo7ty5qq2tVUlJiTZt2qTJkydr//79mjZtmqfmxz/+sXp7e7Vq1SpdunRJ8+bNU0NDg8LDwyV99lJSXV2dtmzZor6+Pk2aNEnr1q3zumcGAACMbDd1o3BRUZGKiop87nvjjTeuGlu8eLEWL1487HxBQUF65pln9Mwzz/jcn5aWpnfeeedmWgUAACME3/0EAACMQKgBAABGINQAAAAjEGoAAIARCDUAAMAIhBoAAGAEQg0AADACoQYAABiBUAMAAIxAqAEAAEYg1AAAACMQagAAgBEINQAAwAiEGgAAYARCDQAAMAKhBgAAGIFQAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIARCDUAAMAIhBoAAGCEmwo1VVVVSkxMVHh4uDIyMnT48OFr1u/bt09JSUkKDw9XSkqKDhw44LV/aGhIpaWlmjBhgkaNGiWbzabTp0971Vy8eFHLly9XZGSkoqOjVVBQoI8//vhm2gcAAAbyO9Ts3btXdrtdZWVlamtr04wZM5SVlaXOzk6f9c3NzcrLy1NBQYGOHTum3Nxc5ebm6sSJE56a559/Xjt37lR1dbVaWloUERGhrKwsffrpp56a5cuX6/3331djY6Pq6+t16NAhrVq16iaWDAAATOR3qCkvL1dhYaHy8/M1ZcoUVVdXa/To0dq9e7fP+h07dig7O1vr169XcnKytm7dqrS0NFVWVkr67CpNRUWFSkpK9P3vf1/Tp0/Xb37zG50/f1779++XJH3wwQdqaGjQr371K2VkZGjevHn6+c9/rrq6Op0/f/7mVw8AAIwR4k9xf3+/WltbVVxc7BmzWCyy2WxyOp0+j3E6nbLb7V5jWVlZnsDyhz/8QS6XSzabzbM/KipKGRkZcjqdWrZsmZxOp6Kjo5Wenu6psdlsslgsamlp0d/8zd9cdd6+vj719fV5fu7p6ZEkud1uf5Z8wwb7Prkl836dbnTtrOX2GolrkcxaD2u5vUbiWiTz1uPvnENDQ9et9SvUdHd3a2BgQHFxcV7jcXFxOnnypM9jXC6Xz3qXy+XZ//nYtWrGjx/v3XhIiMaNG+ep+TKHw6Gnn376qvGEhIThlme8qIpAd/D1YS13JpPWIpm1HtZyZzJpLdKtXc9HH32kqKioa9b4FWruJsXFxV5XiAYHB3Xx4kXFxMQoKCgogJ2NPG63WwkJCTp79qwiIyMD3Q7+Px6XOxOPy52LxyYwhoaG9NFHHyk+Pv66tX6FmtjYWAUHB6ujo8NrvKOjQ1ar1ecxVqv1mvWf/7ejo0MTJkzwqklNTfXUfPlG5CtXrujixYvDnjcsLExhYWFeY9HR0ddeIG6pyMhI/kdwB+JxuTPxuNy5eGxuv+tdofmcXzcKh4aGaubMmWpqavKMDQ4OqqmpSZmZmT6PyczM9KqXpMbGRk/9pEmTZLVavWrcbrdaWlo8NZmZmbp06ZJaW1s9NQcPHtTg4KAyMjL8WQIAADCU3y8/2e12rVy5Uunp6Zo9e7YqKirU29ur/Px8SdKKFSs0ceJEORwOSdLatWs1f/58bd++XTk5Oaqrq9PRo0dVU1MjSQoKCtJTTz2ln/zkJ5o8ebImTZqkzZs3Kz4+Xrm5uZKk5ORkZWdnq7CwUNXV1bp8+bKKioq0bNmyG7ocBQAAzOd3qFm6dKm6urpUWloql8ul1NRUNTQ0eG70bW9vl8XyxQWguXPnqra2ViUlJdq0aZMmT56s/fv3a9q0aZ6aH//4x+rt7dWqVat06dIlzZs3Tw0NDQoPD/fU7NmzR0VFRVq4cKEsFosWLVqknTt3fpW14zYJCwtTWVnZVS8HIrB4XO5MPC53Lh6bO1/Q0I28RwoAAOAOx3c/AQAAIxBqAACAEQg1AADACIQaAABgBEINbgmHw6FZs2ZpzJgxGj9+vHJzc3Xq1KlAt4Uv2bZtm+djFRB4586d06OPPqqYmBiNGjVKKSkpOnr0aKDbGvEGBga0efNmTZo0SaNGjdK3v/1tbd269Ya+iwi3l7Ffk4DAevPNN/Xkk09q1qxZunLlijZt2qSHHnpI//3f/62IiIhAtwdJR44c0T/8wz9o+vTpgW4Fkv7v//5PDzzwgBYsWKB///d/15/92Z/p9OnTGjt2bKBbG/Gee+45vfjii3rppZc0depUHT16VPn5+YqKitKaNWsC3R7+BG/pxm3R1dWl8ePH680339T3vve9QLcz4n388cdKS0vTL37xC/3kJz9RamqqKioqAt3WiLZx40b953/+p956661At4Iv+au/+ivFxcVp165dnrFFixZp1KhR+qd/+qcAdoYv4+Un3BY9PT2SpHHjxgW4E0jSk08+qZycHNlstkC3gv/v1VdfVXp6uhYvXqzx48fr/vvv1y9/+ctAtwV99iGyTU1N+t3vfidJ+q//+i+9/fbb+su//MsAd4Yv4+Un3HKDg4N66qmn9MADD3h9kjQCo66uTm1tbTpy5EigW8Gf+N///V+9+OKLstvt2rRpk44cOaI1a9YoNDRUK1euDHR7I9rGjRvldruVlJSk4OBgDQwM6Nlnn9Xy5csD3Rq+hFCDW+7JJ5/UiRMn9Pbbbwe6lRHv7NmzWrt2rRobG72+hgSBNzg4qPT0dP30pz+VJN1///06ceKEqqurCTUB9s///M/as2ePamtrNXXqVB0/flxPPfWU4uPjeWzuMIQa3FJFRUWqr6/XoUOH9M1vfjPQ7Yx4ra2t6uzsVFpammdsYGBAhw4dUmVlpfr6+hQcHBzADkeuCRMmaMqUKV5jycnJ+pd/+ZcAdYTPrV+/Xhs3btSyZcskSSkpKfrwww/lcDgINXcYQg1uiaGhIa1evVqvvPKK3njjDU2aNCnQLUHSwoUL9d5773mN5efnKykpSRs2bCDQBNADDzxw1cce/O53v9O3vvWtAHWEz33yySdeX9QsScHBwRocHAxQRxgOoQa3xJNPPqna2lr967/+q8aMGSOXyyVJioqK0qhRowLc3cg1ZsyYq+5rioiIUExMDPc7Bdi6des0d+5c/fSnP9WSJUt0+PBh1dTUqKamJtCtjXh//dd/rWeffVb33nuvpk6dqmPHjqm8vFx/93d/F+jW8CW8pRu3RFBQkM/xX//613r88cdvbzO4pgcffJC3dN8h6uvrVVxcrNOnT2vSpEmy2+0qLCwMdFsj3kcffaTNmzfrlVdeUWdnp+Lj45WXl6fS0lKFhoYGuj38CUINAAAwAp9TAwAAjECoAQAARiDUAAAAIxBqAACAEQg1AADACIQaAABgBEINAAAwAqEGAAAYgVADAACMQKgBAABGINQAAAAjEGoAAIAR/h/P9B0OzEY5FgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "plt.bar(range(1,10),acc)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Obtaining dependency information for torchinfo from https://files.pythonhosted.org/packages/72/25/973bd6128381951b23cdcd8a9870c6dcfc5606cb864df8eabd82e529f9c1/torchinfo-1.8.0-py3-none-any.whl.metadata\n",
      "  Using cached torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Using cached torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\tjoeun\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [32, 1]                   --\n",
       "├─Linear: 1-1                            [32, 100]                 200\n",
       "├─ReLU: 1-2                              [32, 100]                 --\n",
       "├─Linear: 1-3                            [32, 1]                   101\n",
       "==========================================================================================\n",
       "Total params: 301\n",
       "Trainable params: 301\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.03\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model)\n",
    "\n",
    "summary(model, input_size=(32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchviz) (2.0.1)\n",
      "Collecting graphviz (from torchviz)\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 0.0/47.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 47.0/47.0 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->torchviz) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->torchviz) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->torchviz) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->torchviz) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->torchviz) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch->torchviz) (1.3.0)\n",
      "Building wheels for collected packages: torchviz\n",
      "  Building wheel for torchviz (setup.py): started\n",
      "  Building wheel for torchviz (setup.py): finished with status 'done'\n",
      "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4157 sha256=71497f23c0fb1bc385eed448a03fb5b1b373b42940df2d1b8cc11560475445fd\n",
      "  Stored in directory: c:\\users\\tjoeun\\appdata\\local\\pip\\cache\\wheels\\29\\65\\6e\\db2515eb1dc760fecd36b40d54df65c1e18534013f1c037e2e\n",
      "Successfully built torchviz\n",
      "Installing collected packages: graphviz, torchviz\n",
      "Successfully installed graphviz-0.20.1 torchviz-0.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\tjoeun\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.0739)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"337pt\" height=\"424pt\"\n",
       " viewBox=\"0.00 0.00 337.00 424.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 420)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-420 333,-420 333,4 -4,4\"/>\n",
       "<!-- 2168700267632 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2168700267632</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"197,-32 133,-32 133,0 197,0 197,-32\"/>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-6.5\" font-family=\"monospace\" font-size=\"10.00\"> (20, 1)</text>\n",
       "</g>\n",
       "<!-- 2166904020896 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2166904020896</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"215,-88 115,-88 115,-68 215,-68 215,-88\"/>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-74.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2166904020896&#45;&gt;2168700267632 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>2166904020896&#45;&gt;2168700267632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165,-67.62C165,-61.02 165,-51.84 165,-43.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.5,-43.14 165,-33.14 161.5,-43.14 168.5,-43.14\"/>\n",
       "</g>\n",
       "<!-- 2166904020416 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2166904020416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-144 0,-144 0,-124 100,-124 100,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2166904020416&#45;&gt;2166904020896 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2166904020416&#45;&gt;2166904020896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M70.03,-123.59C87.98,-115.17 114.55,-102.69 135.03,-93.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.25,-95.9 143.81,-88.48 133.27,-89.56 136.25,-95.9\"/>\n",
       "</g>\n",
       "<!-- 2166925454688 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2166925454688</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77,-212 23,-212 23,-180 77,-180 77,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-198.5\" font-family=\"monospace\" font-size=\"10.00\">2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 2166925454688&#45;&gt;2166904020416 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2166925454688&#45;&gt;2166904020416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-179.55C50,-172.17 50,-163.24 50,-155.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-155.41 50,-145.41 46.5,-155.41 53.5,-155.41\"/>\n",
       "</g>\n",
       "<!-- 2166904020512 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2166904020512</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"212,-144 118,-144 118,-124 212,-124 212,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 2166904020512&#45;&gt;2166904020896 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2166904020512&#45;&gt;2166904020896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165,-123.59C165,-116.86 165,-107.53 165,-99.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.5,-99.3 165,-89.3 161.5,-99.3 168.5,-99.3\"/>\n",
       "</g>\n",
       "<!-- 2166904020560 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2166904020560</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"211,-206 111,-206 111,-186 211,-186 211,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"161\" y=\"-192.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 2166904020560&#45;&gt;2166904020512 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2166904020560&#45;&gt;2166904020512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M161.62,-185.62C162.17,-177.48 162.97,-165.39 163.66,-155.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167.22,-155.32 164.39,-145.11 160.23,-154.86 167.22,-155.32\"/>\n",
       "</g>\n",
       "<!-- 2166904020272 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2166904020272</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"118,-274 18,-274 18,-254 118,-254 118,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"68\" y=\"-260.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2166904020272&#45;&gt;2166904020560 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2166904020272&#45;&gt;2166904020560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.32,-253.54C96.38,-242.86 121.12,-225.31 139.04,-212.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"140.57,-215.08 146.7,-206.44 136.52,-209.37 140.57,-215.08\"/>\n",
       "</g>\n",
       "<!-- 2166925454368 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2166925454368</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"95,-348 41,-348 41,-316 95,-316 95,-348\"/>\n",
       "<text text-anchor=\"middle\" x=\"68\" y=\"-334.5\" font-family=\"monospace\" font-size=\"10.00\">0.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"68\" y=\"-322.5\" font-family=\"monospace\" font-size=\"10.00\"> (100)</text>\n",
       "</g>\n",
       "<!-- 2166925454368&#45;&gt;2166904020272 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2166925454368&#45;&gt;2166904020272</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68,-315.69C68,-306.6 68,-294.95 68,-285.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71.5,-285.32 68,-275.32 64.5,-285.32 71.5,-285.32\"/>\n",
       "</g>\n",
       "<!-- 2166904020320 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2166904020320</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"212,-274 136,-274 136,-254 212,-254 212,-274\"/>\n",
       "<text text-anchor=\"middle\" x=\"174\" y=\"-260.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2166904020320&#45;&gt;2166904020560 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2166904020320&#45;&gt;2166904020560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.14,-253.54C170.26,-244.04 167.32,-229.09 164.93,-216.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.23,-216.57 162.86,-207.44 161.36,-217.93 168.23,-216.57\"/>\n",
       "</g>\n",
       "<!-- 2166904020128 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>2166904020128</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"224,-342 124,-342 124,-322 224,-322 224,-342\"/>\n",
       "<text text-anchor=\"middle\" x=\"174\" y=\"-328.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2166904020128&#45;&gt;2166904020320 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2166904020128&#45;&gt;2166904020320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174,-321.54C174,-312.13 174,-297.39 174,-285.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.5,-285.44 174,-275.44 170.5,-285.44 177.5,-285.44\"/>\n",
       "</g>\n",
       "<!-- 2166936278736 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>2166936278736</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"209,-416 139,-416 139,-384 209,-384 209,-416\"/>\n",
       "<text text-anchor=\"middle\" x=\"174\" y=\"-402.5\" font-family=\"monospace\" font-size=\"10.00\">0.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"174\" y=\"-390.5\" font-family=\"monospace\" font-size=\"10.00\"> (100, 1)</text>\n",
       "</g>\n",
       "<!-- 2166936278736&#45;&gt;2166904020128 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>2166936278736&#45;&gt;2166904020128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174,-383.69C174,-374.6 174,-362.95 174,-353.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.5,-353.32 174,-343.32 170.5,-353.32 177.5,-353.32\"/>\n",
       "</g>\n",
       "<!-- 2166904020464 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>2166904020464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"310,-144 234,-144 234,-124 310,-124 310,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"272\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 2166904020464&#45;&gt;2166904020896 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>2166904020464&#45;&gt;2166904020896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M253.36,-123.59C236.82,-115.24 212.39,-102.92 193.41,-93.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.29,-89.86 184.78,-88.48 192.13,-96.11 195.29,-89.86\"/>\n",
       "</g>\n",
       "<!-- 2166904020080 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>2166904020080</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"329,-206 229,-206 229,-186 329,-186 329,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"279\" y=\"-192.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 2166904020080&#45;&gt;2166904020464 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>2166904020080&#45;&gt;2166904020464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.91,-185.62C276.95,-177.39 275.51,-165.13 274.3,-154.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.7,-154.64 273.06,-145.11 270.75,-155.45 277.7,-154.64\"/>\n",
       "</g>\n",
       "<!-- 2166925453808 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>2166925453808</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"314,-280 244,-280 244,-248 314,-248 314,-280\"/>\n",
       "<text text-anchor=\"middle\" x=\"279\" y=\"-266.5\" font-family=\"monospace\" font-size=\"10.00\">2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"279\" y=\"-254.5\" font-family=\"monospace\" font-size=\"10.00\"> (1, 100)</text>\n",
       "</g>\n",
       "<!-- 2166925453808&#45;&gt;2166904020080 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>2166925453808&#45;&gt;2166904020080</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279,-247.69C279,-238.6 279,-226.95 279,-217.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"282.5,-217.32 279,-207.32 275.5,-217.32 282.5,-217.32\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1f8858b5f70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "pred=model(X_test)\n",
    "\n",
    "make_dot(pred, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchview\n",
      "  Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: torchview\n",
      "Successfully installed torchview-0.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\tjoeun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\tjoeun\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\\pip install torchview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(dot.exe:2452): Pango-WARNING **: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.0739)\n",
       " -->\n",
       "<!-- Title: model Pages: 1 -->\n",
       "<svg width=\"146pt\" height=\"352pt\"\n",
       " viewBox=\"0.00 0.00 146.00 352.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 348)\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-348 142,-348 142,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"119.5,-344 18.5,-344 18.5,-310 119.5,-310 119.5,-344\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"18.5,-310 18.5,-344 80.25,-344 80.25,-310 18.5,-310\"/>\n",
       "<text text-anchor=\"start\" x=\"23.5\" y=\"-329.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"32.88\" y=\"-317.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"80.25,-310 80.25,-344 119.5,-344 119.5,-310 80.25,-310\"/>\n",
       "<text text-anchor=\"start\" x=\"85.25\" y=\"-323.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(20, 1)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"138,-274 0,-274 0,-230 138,-230 138,-274\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-230 0,-274 43,-274 43,-230 0,-230\"/>\n",
       "<text text-anchor=\"start\" x=\"8.38\" y=\"-254.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"5\" y=\"-242.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"43,-252 43,-274 86,-274 86,-252 43,-252\"/>\n",
       "<text text-anchor=\"start\" x=\"52.12\" y=\"-259.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86,-252 86,-274 138,-274 138,-252 86,-252\"/>\n",
       "<text text-anchor=\"start\" x=\"95.88\" y=\"-259.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(20, 1) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"43,-230 43,-252 86,-252 86,-230 43,-230\"/>\n",
       "<text text-anchor=\"start\" x=\"47.62\" y=\"-237.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86,-230 86,-252 138,-252 138,-230 86,-230\"/>\n",
       "<text text-anchor=\"start\" x=\"90.62\" y=\"-237.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(20, 100) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69,-310.16C69,-302.67 69,-293.45 69,-284.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72.5,-284.8 69,-274.8 65.5,-284.8 72.5,-284.8\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"138,-194 0,-194 0,-150 138,-150 138,-194\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-150 0,-194 43,-194 43,-150 0,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"10.62\" y=\"-174.5\" font-family=\"Linux libertine\" font-size=\"10.00\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"5\" y=\"-162.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"43,-172 43,-194 86,-194 86,-172 43,-172\"/>\n",
       "<text text-anchor=\"start\" x=\"52.12\" y=\"-179.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86,-172 86,-194 138,-194 138,-172 86,-172\"/>\n",
       "<text text-anchor=\"start\" x=\"90.62\" y=\"-179.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(20, 100) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"43,-150 43,-172 86,-172 86,-150 43,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"47.62\" y=\"-157.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86,-150 86,-172 138,-172 138,-150 86,-150\"/>\n",
       "<text text-anchor=\"start\" x=\"90.62\" y=\"-157.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(20, 100) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69,-230.1C69,-222.32 69,-213.3 69,-204.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72.5,-204.96 69,-194.96 65.5,-204.96 72.5,-204.96\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"138,-114 0,-114 0,-70 138,-70 138,-114\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-70 0,-114 43,-114 43,-70 0,-70\"/>\n",
       "<text text-anchor=\"start\" x=\"8.38\" y=\"-94.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"5\" y=\"-82.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"43,-92 43,-114 86,-114 86,-92 43,-92\"/>\n",
       "<text text-anchor=\"start\" x=\"52.12\" y=\"-99.5\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86,-92 86,-114 138,-114 138,-92 86,-92\"/>\n",
       "<text text-anchor=\"start\" x=\"90.62\" y=\"-99.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(20, 100) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"43,-70 43,-92 86,-92 86,-70 43,-70\"/>\n",
       "<text text-anchor=\"start\" x=\"47.62\" y=\"-77.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"86,-70 86,-92 138,-92 138,-70 86,-70\"/>\n",
       "<text text-anchor=\"start\" x=\"95.88\" y=\"-77.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(20, 1) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69,-150.1C69,-142.32 69,-133.3 69,-124.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72.5,-124.96 69,-114.96 65.5,-124.96 72.5,-124.96\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"122.5,-34 15.5,-34 15.5,0 122.5,0 122.5,-34\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"15.5,0 15.5,-34 83.25,-34 83.25,0 15.5,0\"/>\n",
       "<text text-anchor=\"start\" x=\"20.5\" y=\"-19.5\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"32.88\" y=\"-7.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"83.25,0 83.25,-34 122.5,-34 122.5,0 83.25,0\"/>\n",
       "<text text-anchor=\"start\" x=\"88.25\" y=\"-13.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(20, 1)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69,-70.28C69,-62.28 69,-53.03 69,-44.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72.5,-44.67 69,-34.67 65.5,-44.67 72.5,-44.67\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1f885f199a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(model, input_data=X_test)\n",
    "\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "plt.scatter(X.cpu().numpy(), y.cpu().numpy(), label='real')\n",
    "\n",
    "plt.scatter(model(X).cpu().detach().numpy(), y.cpu().numpy(), label='pred')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) 단순회귀분석(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.619379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.857290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.628194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.725426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.498926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3.867042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>4.934539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>4.864829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>5.241539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5.823689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x         y\n",
       "0   1  1.619379\n",
       "1   2  1.857290\n",
       "2   3  2.628194\n",
       "3   4  2.725426\n",
       "4   5  3.498926\n",
       "5   6  3.867042\n",
       "6   7  4.934539\n",
       "7   8  4.864829\n",
       "8   9  5.241539\n",
       "9  10  5.823689"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../DATA/regression_test.csv')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.from_numpy(df['x'].values).unsqueeze(1).float()\n",
    "\n",
    "y = torch.from_numpy(df['y'].values).unsqueeze(1).float()\n",
    "\n",
    "################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os    \n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=1, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.8058]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6127], requires_grad=True)\n",
      "tensor([[-1.4184],\n",
      "        [-2.2242],\n",
      "        [-3.0300],\n",
      "        [-3.8357],\n",
      "        [-4.6415],\n",
      "        [-5.4473],\n",
      "        [-6.2531],\n",
      "        [-7.0588],\n",
      "        [-7.8646],\n",
      "        [-8.6704]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# 단순한 신경망\n",
    "\n",
    "model = nn.Linear(in_features=1, out_features=1, bias=True)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# 초기 파라미터\n",
    "\n",
    "print(model.weight)\n",
    "\n",
    "print(model.bias)\n",
    "\n",
    "################################\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "\n",
    "print(model(X)) # predict(학습 전)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 90.316\n",
      "epoch: 1, loss: 88.971\n",
      "epoch: 2, loss: 87.636\n",
      "epoch: 3, loss: 86.312\n",
      "epoch: 4, loss: 84.998\n",
      "epoch: 5, loss: 83.695\n",
      "epoch: 6, loss: 82.404\n",
      "epoch: 7, loss: 81.123\n",
      "epoch: 8, loss: 79.854\n",
      "epoch: 9, loss: 78.597\n",
      "epoch: 10, loss: 77.351\n",
      "epoch: 11, loss: 76.116\n",
      "epoch: 12, loss: 74.894\n",
      "epoch: 13, loss: 73.683\n",
      "epoch: 14, loss: 72.485\n",
      "epoch: 15, loss: 71.299\n",
      "epoch: 16, loss: 70.125\n",
      "epoch: 17, loss: 68.964\n",
      "epoch: 18, loss: 67.814\n",
      "epoch: 19, loss: 66.678\n",
      "epoch: 20, loss: 65.554\n",
      "epoch: 21, loss: 64.442\n",
      "epoch: 22, loss: 63.343\n",
      "epoch: 23, loss: 62.257\n",
      "epoch: 24, loss: 61.184\n",
      "epoch: 25, loss: 60.123\n",
      "epoch: 26, loss: 59.075\n",
      "epoch: 27, loss: 58.040\n",
      "epoch: 28, loss: 57.017\n",
      "epoch: 29, loss: 56.007\n",
      "epoch: 30, loss: 55.010\n",
      "epoch: 31, loss: 54.026\n",
      "epoch: 32, loss: 53.054\n",
      "epoch: 33, loss: 52.095\n",
      "epoch: 34, loss: 51.148\n",
      "epoch: 35, loss: 50.215\n",
      "epoch: 36, loss: 49.293\n",
      "epoch: 37, loss: 48.384\n",
      "epoch: 38, loss: 47.488\n",
      "epoch: 39, loss: 46.604\n",
      "epoch: 40, loss: 45.732\n",
      "epoch: 41, loss: 44.872\n",
      "epoch: 42, loss: 44.025\n",
      "epoch: 43, loss: 43.189\n",
      "epoch: 44, loss: 42.366\n",
      "epoch: 45, loss: 41.554\n",
      "epoch: 46, loss: 40.755\n",
      "epoch: 47, loss: 39.967\n",
      "epoch: 48, loss: 39.191\n",
      "epoch: 49, loss: 38.426\n",
      "epoch: 50, loss: 37.673\n",
      "epoch: 51, loss: 36.931\n",
      "epoch: 52, loss: 36.201\n",
      "epoch: 53, loss: 35.481\n",
      "epoch: 54, loss: 34.773\n",
      "epoch: 55, loss: 34.076\n",
      "epoch: 56, loss: 33.390\n",
      "epoch: 57, loss: 32.715\n",
      "epoch: 58, loss: 32.050\n",
      "epoch: 59, loss: 31.396\n",
      "epoch: 60, loss: 30.753\n",
      "epoch: 61, loss: 30.120\n",
      "epoch: 62, loss: 29.497\n",
      "epoch: 63, loss: 28.884\n",
      "epoch: 64, loss: 28.282\n",
      "epoch: 65, loss: 27.689\n",
      "epoch: 66, loss: 27.107\n",
      "epoch: 67, loss: 26.534\n",
      "epoch: 68, loss: 25.971\n",
      "epoch: 69, loss: 25.417\n",
      "epoch: 70, loss: 24.873\n",
      "epoch: 71, loss: 24.338\n",
      "epoch: 72, loss: 23.813\n",
      "epoch: 73, loss: 23.296\n",
      "epoch: 74, loss: 22.789\n",
      "epoch: 75, loss: 22.290\n",
      "epoch: 76, loss: 21.801\n",
      "epoch: 77, loss: 21.320\n",
      "epoch: 78, loss: 20.847\n",
      "epoch: 79, loss: 20.384\n",
      "epoch: 80, loss: 19.928\n",
      "epoch: 81, loss: 19.481\n",
      "epoch: 82, loss: 19.042\n",
      "epoch: 83, loss: 18.611\n",
      "epoch: 84, loss: 18.188\n",
      "epoch: 85, loss: 17.773\n",
      "epoch: 86, loss: 17.366\n",
      "epoch: 87, loss: 16.966\n",
      "epoch: 88, loss: 16.574\n",
      "epoch: 89, loss: 16.190\n",
      "epoch: 90, loss: 15.813\n",
      "epoch: 91, loss: 15.443\n",
      "epoch: 92, loss: 15.080\n",
      "epoch: 93, loss: 14.724\n",
      "epoch: 94, loss: 14.375\n",
      "epoch: 95, loss: 14.033\n",
      "epoch: 96, loss: 13.698\n",
      "epoch: 97, loss: 13.370\n",
      "epoch: 98, loss: 13.048\n",
      "epoch: 99, loss: 12.732\n",
      "epoch: 100, loss: 12.423\n",
      "epoch: 101, loss: 12.120\n",
      "epoch: 102, loss: 11.824\n",
      "epoch: 103, loss: 11.533\n",
      "epoch: 104, loss: 11.249\n",
      "epoch: 105, loss: 10.970\n",
      "epoch: 106, loss: 10.697\n",
      "epoch: 107, loss: 10.430\n",
      "epoch: 108, loss: 10.169\n",
      "epoch: 109, loss: 9.913\n",
      "epoch: 110, loss: 9.662\n",
      "epoch: 111, loss: 9.417\n",
      "epoch: 112, loss: 9.177\n",
      "epoch: 113, loss: 8.943\n",
      "epoch: 114, loss: 8.713\n",
      "epoch: 115, loss: 8.489\n",
      "epoch: 116, loss: 8.269\n",
      "epoch: 117, loss: 8.054\n",
      "epoch: 118, loss: 7.844\n",
      "epoch: 119, loss: 7.639\n",
      "epoch: 120, loss: 7.438\n",
      "epoch: 121, loss: 7.242\n",
      "epoch: 122, loss: 7.050\n",
      "epoch: 123, loss: 6.863\n",
      "epoch: 124, loss: 6.680\n",
      "epoch: 125, loss: 6.501\n",
      "epoch: 126, loss: 6.327\n",
      "epoch: 127, loss: 6.156\n",
      "epoch: 128, loss: 5.989\n",
      "epoch: 129, loss: 5.827\n",
      "epoch: 130, loss: 5.668\n",
      "epoch: 131, loss: 5.513\n",
      "epoch: 132, loss: 5.361\n",
      "epoch: 133, loss: 5.213\n",
      "epoch: 134, loss: 5.069\n",
      "epoch: 135, loss: 4.928\n",
      "epoch: 136, loss: 4.791\n",
      "epoch: 137, loss: 4.657\n",
      "epoch: 138, loss: 4.526\n",
      "epoch: 139, loss: 4.399\n",
      "epoch: 140, loss: 4.275\n",
      "epoch: 141, loss: 4.153\n",
      "epoch: 142, loss: 4.035\n",
      "epoch: 143, loss: 3.920\n",
      "epoch: 144, loss: 3.808\n",
      "epoch: 145, loss: 3.698\n",
      "epoch: 146, loss: 3.592\n",
      "epoch: 147, loss: 3.488\n",
      "epoch: 148, loss: 3.387\n",
      "epoch: 149, loss: 3.288\n",
      "epoch: 150, loss: 3.192\n",
      "epoch: 151, loss: 3.098\n",
      "epoch: 152, loss: 3.007\n",
      "epoch: 153, loss: 2.919\n",
      "epoch: 154, loss: 2.832\n",
      "epoch: 155, loss: 2.748\n",
      "epoch: 156, loss: 2.666\n",
      "epoch: 157, loss: 2.587\n",
      "epoch: 158, loss: 2.509\n",
      "epoch: 159, loss: 2.434\n",
      "epoch: 160, loss: 2.361\n",
      "epoch: 161, loss: 2.289\n",
      "epoch: 162, loss: 2.220\n",
      "epoch: 163, loss: 2.153\n",
      "epoch: 164, loss: 2.087\n",
      "epoch: 165, loss: 2.023\n",
      "epoch: 166, loss: 1.961\n",
      "epoch: 167, loss: 1.901\n",
      "epoch: 168, loss: 1.843\n",
      "epoch: 169, loss: 1.786\n",
      "epoch: 170, loss: 1.730\n",
      "epoch: 171, loss: 1.677\n",
      "epoch: 172, loss: 1.625\n",
      "epoch: 173, loss: 1.574\n",
      "epoch: 174, loss: 1.525\n",
      "epoch: 175, loss: 1.477\n",
      "epoch: 176, loss: 1.431\n",
      "epoch: 177, loss: 1.386\n",
      "epoch: 178, loss: 1.342\n",
      "epoch: 179, loss: 1.300\n",
      "epoch: 180, loss: 1.259\n",
      "epoch: 181, loss: 1.219\n",
      "epoch: 182, loss: 1.180\n",
      "epoch: 183, loss: 1.143\n",
      "epoch: 184, loss: 1.106\n",
      "epoch: 185, loss: 1.071\n",
      "epoch: 186, loss: 1.037\n",
      "epoch: 187, loss: 1.004\n",
      "epoch: 188, loss: 0.972\n",
      "epoch: 189, loss: 0.940\n",
      "epoch: 190, loss: 0.910\n",
      "epoch: 191, loss: 0.881\n",
      "epoch: 192, loss: 0.853\n",
      "epoch: 193, loss: 0.825\n",
      "epoch: 194, loss: 0.799\n",
      "epoch: 195, loss: 0.773\n",
      "epoch: 196, loss: 0.748\n",
      "epoch: 197, loss: 0.724\n",
      "epoch: 198, loss: 0.700\n",
      "epoch: 199, loss: 0.678\n",
      "epoch: 200, loss: 0.656\n",
      "epoch: 201, loss: 0.635\n",
      "epoch: 202, loss: 0.614\n",
      "epoch: 203, loss: 0.594\n",
      "epoch: 204, loss: 0.575\n",
      "epoch: 205, loss: 0.557\n",
      "epoch: 206, loss: 0.539\n",
      "epoch: 207, loss: 0.521\n",
      "epoch: 208, loss: 0.505\n",
      "epoch: 209, loss: 0.488\n",
      "epoch: 210, loss: 0.473\n",
      "epoch: 211, loss: 0.458\n",
      "epoch: 212, loss: 0.443\n",
      "epoch: 213, loss: 0.429\n",
      "epoch: 214, loss: 0.415\n",
      "epoch: 215, loss: 0.402\n",
      "epoch: 216, loss: 0.389\n",
      "epoch: 217, loss: 0.377\n",
      "epoch: 218, loss: 0.365\n",
      "epoch: 219, loss: 0.354\n",
      "epoch: 220, loss: 0.342\n",
      "epoch: 221, loss: 0.332\n",
      "epoch: 222, loss: 0.321\n",
      "epoch: 223, loss: 0.311\n",
      "epoch: 224, loss: 0.302\n",
      "epoch: 225, loss: 0.293\n",
      "epoch: 226, loss: 0.284\n",
      "epoch: 227, loss: 0.275\n",
      "epoch: 228, loss: 0.267\n",
      "epoch: 229, loss: 0.259\n",
      "epoch: 230, loss: 0.251\n",
      "epoch: 231, loss: 0.244\n",
      "epoch: 232, loss: 0.236\n",
      "epoch: 233, loss: 0.229\n",
      "epoch: 234, loss: 0.223\n",
      "epoch: 235, loss: 0.216\n",
      "epoch: 236, loss: 0.210\n",
      "epoch: 237, loss: 0.204\n",
      "epoch: 238, loss: 0.199\n",
      "epoch: 239, loss: 0.193\n",
      "epoch: 240, loss: 0.188\n",
      "epoch: 241, loss: 0.183\n",
      "epoch: 242, loss: 0.178\n",
      "epoch: 243, loss: 0.173\n",
      "epoch: 244, loss: 0.168\n",
      "epoch: 245, loss: 0.164\n",
      "epoch: 246, loss: 0.160\n",
      "epoch: 247, loss: 0.156\n",
      "epoch: 248, loss: 0.152\n",
      "epoch: 249, loss: 0.148\n",
      "epoch: 250, loss: 0.144\n",
      "epoch: 251, loss: 0.141\n",
      "epoch: 252, loss: 0.138\n",
      "epoch: 253, loss: 0.134\n",
      "epoch: 254, loss: 0.131\n",
      "epoch: 255, loss: 0.128\n",
      "epoch: 256, loss: 0.125\n",
      "epoch: 257, loss: 0.123\n",
      "epoch: 258, loss: 0.120\n",
      "epoch: 259, loss: 0.118\n",
      "epoch: 260, loss: 0.115\n",
      "epoch: 261, loss: 0.113\n",
      "epoch: 262, loss: 0.110\n",
      "epoch: 263, loss: 0.108\n",
      "epoch: 264, loss: 0.106\n",
      "epoch: 265, loss: 0.104\n",
      "epoch: 266, loss: 0.102\n",
      "epoch: 267, loss: 0.101\n",
      "epoch: 268, loss: 0.099\n",
      "epoch: 269, loss: 0.097\n",
      "epoch: 270, loss: 0.096\n",
      "epoch: 271, loss: 0.094\n",
      "epoch: 272, loss: 0.092\n",
      "epoch: 273, loss: 0.091\n",
      "epoch: 274, loss: 0.090\n",
      "epoch: 275, loss: 0.088\n",
      "epoch: 276, loss: 0.087\n",
      "epoch: 277, loss: 0.086\n",
      "epoch: 278, loss: 0.085\n",
      "epoch: 279, loss: 0.084\n",
      "epoch: 280, loss: 0.083\n",
      "epoch: 281, loss: 0.082\n",
      "epoch: 282, loss: 0.081\n",
      "epoch: 283, loss: 0.080\n",
      "epoch: 284, loss: 0.079\n",
      "epoch: 285, loss: 0.078\n",
      "epoch: 286, loss: 0.077\n",
      "epoch: 287, loss: 0.076\n",
      "epoch: 288, loss: 0.076\n",
      "epoch: 289, loss: 0.075\n",
      "epoch: 290, loss: 0.074\n",
      "epoch: 291, loss: 0.074\n",
      "epoch: 292, loss: 0.073\n",
      "epoch: 293, loss: 0.072\n",
      "epoch: 294, loss: 0.072\n",
      "epoch: 295, loss: 0.071\n",
      "epoch: 296, loss: 0.071\n",
      "epoch: 297, loss: 0.070\n",
      "epoch: 298, loss: 0.070\n",
      "epoch: 299, loss: 0.069\n",
      "epoch: 300, loss: 0.069\n",
      "epoch: 301, loss: 0.068\n",
      "epoch: 302, loss: 0.068\n",
      "epoch: 303, loss: 0.067\n",
      "epoch: 304, loss: 0.067\n",
      "epoch: 305, loss: 0.067\n",
      "epoch: 306, loss: 0.066\n",
      "epoch: 307, loss: 0.066\n",
      "epoch: 308, loss: 0.066\n",
      "epoch: 309, loss: 0.065\n",
      "epoch: 310, loss: 0.065\n",
      "epoch: 311, loss: 0.065\n",
      "epoch: 312, loss: 0.065\n",
      "epoch: 313, loss: 0.064\n",
      "epoch: 314, loss: 0.064\n",
      "epoch: 315, loss: 0.064\n",
      "epoch: 316, loss: 0.064\n",
      "epoch: 317, loss: 0.063\n",
      "epoch: 318, loss: 0.063\n",
      "epoch: 319, loss: 0.063\n",
      "epoch: 320, loss: 0.063\n",
      "epoch: 321, loss: 0.063\n",
      "epoch: 322, loss: 0.063\n",
      "epoch: 323, loss: 0.062\n",
      "epoch: 324, loss: 0.062\n",
      "epoch: 325, loss: 0.062\n",
      "epoch: 326, loss: 0.062\n",
      "epoch: 327, loss: 0.062\n",
      "epoch: 328, loss: 0.062\n",
      "epoch: 329, loss: 0.062\n",
      "epoch: 330, loss: 0.061\n",
      "epoch: 331, loss: 0.061\n",
      "epoch: 332, loss: 0.061\n",
      "epoch: 333, loss: 0.061\n",
      "epoch: 334, loss: 0.061\n",
      "epoch: 335, loss: 0.061\n",
      "epoch: 336, loss: 0.061\n",
      "epoch: 337, loss: 0.061\n",
      "epoch: 338, loss: 0.061\n",
      "epoch: 339, loss: 0.061\n",
      "epoch: 340, loss: 0.060\n",
      "epoch: 341, loss: 0.060\n",
      "epoch: 342, loss: 0.060\n",
      "epoch: 343, loss: 0.060\n",
      "epoch: 344, loss: 0.060\n",
      "epoch: 345, loss: 0.060\n",
      "epoch: 346, loss: 0.060\n",
      "epoch: 347, loss: 0.060\n",
      "epoch: 348, loss: 0.060\n",
      "epoch: 349, loss: 0.060\n",
      "epoch: 350, loss: 0.060\n",
      "epoch: 351, loss: 0.060\n",
      "epoch: 352, loss: 0.060\n",
      "epoch: 353, loss: 0.060\n",
      "epoch: 354, loss: 0.060\n",
      "epoch: 355, loss: 0.059\n",
      "epoch: 356, loss: 0.059\n",
      "epoch: 357, loss: 0.059\n",
      "epoch: 358, loss: 0.059\n",
      "epoch: 359, loss: 0.059\n",
      "epoch: 360, loss: 0.059\n",
      "epoch: 361, loss: 0.059\n",
      "epoch: 362, loss: 0.059\n",
      "epoch: 363, loss: 0.059\n",
      "epoch: 364, loss: 0.059\n",
      "epoch: 365, loss: 0.059\n",
      "epoch: 366, loss: 0.059\n",
      "epoch: 367, loss: 0.059\n",
      "epoch: 368, loss: 0.059\n",
      "epoch: 369, loss: 0.059\n",
      "epoch: 370, loss: 0.059\n",
      "epoch: 371, loss: 0.059\n",
      "epoch: 372, loss: 0.059\n",
      "epoch: 373, loss: 0.059\n",
      "epoch: 374, loss: 0.059\n",
      "epoch: 375, loss: 0.059\n",
      "epoch: 376, loss: 0.059\n",
      "epoch: 377, loss: 0.059\n",
      "epoch: 378, loss: 0.059\n",
      "epoch: 379, loss: 0.059\n",
      "epoch: 380, loss: 0.059\n",
      "epoch: 381, loss: 0.059\n",
      "epoch: 382, loss: 0.059\n",
      "epoch: 383, loss: 0.058\n",
      "epoch: 384, loss: 0.058\n",
      "epoch: 385, loss: 0.058\n",
      "epoch: 386, loss: 0.058\n",
      "epoch: 387, loss: 0.058\n",
      "epoch: 388, loss: 0.058\n",
      "epoch: 389, loss: 0.058\n",
      "epoch: 390, loss: 0.058\n",
      "epoch: 391, loss: 0.058\n",
      "epoch: 392, loss: 0.058\n",
      "epoch: 393, loss: 0.058\n",
      "epoch: 394, loss: 0.058\n",
      "epoch: 395, loss: 0.058\n",
      "epoch: 396, loss: 0.058\n",
      "epoch: 397, loss: 0.058\n",
      "epoch: 398, loss: 0.058\n",
      "epoch: 399, loss: 0.058\n",
      "epoch: 400, loss: 0.058\n",
      "epoch: 401, loss: 0.058\n",
      "epoch: 402, loss: 0.058\n",
      "epoch: 403, loss: 0.058\n",
      "epoch: 404, loss: 0.058\n",
      "epoch: 405, loss: 0.058\n",
      "epoch: 406, loss: 0.058\n",
      "epoch: 407, loss: 0.058\n",
      "epoch: 408, loss: 0.058\n",
      "epoch: 409, loss: 0.058\n",
      "epoch: 410, loss: 0.058\n",
      "epoch: 411, loss: 0.058\n",
      "epoch: 412, loss: 0.058\n",
      "epoch: 413, loss: 0.058\n",
      "epoch: 414, loss: 0.058\n",
      "epoch: 415, loss: 0.058\n",
      "epoch: 416, loss: 0.058\n",
      "epoch: 417, loss: 0.058\n",
      "epoch: 418, loss: 0.058\n",
      "epoch: 419, loss: 0.058\n",
      "epoch: 420, loss: 0.058\n",
      "epoch: 421, loss: 0.058\n",
      "epoch: 422, loss: 0.058\n",
      "epoch: 423, loss: 0.058\n",
      "epoch: 424, loss: 0.057\n",
      "epoch: 425, loss: 0.057\n",
      "epoch: 426, loss: 0.057\n",
      "epoch: 427, loss: 0.057\n",
      "epoch: 428, loss: 0.057\n",
      "epoch: 429, loss: 0.057\n",
      "epoch: 430, loss: 0.057\n",
      "epoch: 431, loss: 0.057\n",
      "epoch: 432, loss: 0.057\n",
      "epoch: 433, loss: 0.057\n",
      "epoch: 434, loss: 0.057\n",
      "epoch: 435, loss: 0.057\n",
      "epoch: 436, loss: 0.057\n",
      "epoch: 437, loss: 0.057\n",
      "epoch: 438, loss: 0.057\n",
      "epoch: 439, loss: 0.057\n",
      "epoch: 440, loss: 0.057\n",
      "epoch: 441, loss: 0.057\n",
      "epoch: 442, loss: 0.057\n",
      "epoch: 443, loss: 0.057\n",
      "epoch: 444, loss: 0.057\n",
      "epoch: 445, loss: 0.057\n",
      "epoch: 446, loss: 0.057\n",
      "epoch: 447, loss: 0.057\n",
      "epoch: 448, loss: 0.057\n",
      "epoch: 449, loss: 0.057\n",
      "epoch: 450, loss: 0.057\n",
      "epoch: 451, loss: 0.057\n",
      "epoch: 452, loss: 0.057\n",
      "epoch: 453, loss: 0.057\n",
      "epoch: 454, loss: 0.057\n",
      "epoch: 455, loss: 0.057\n",
      "epoch: 456, loss: 0.057\n",
      "epoch: 457, loss: 0.057\n",
      "epoch: 458, loss: 0.057\n",
      "epoch: 459, loss: 0.057\n",
      "epoch: 460, loss: 0.057\n",
      "epoch: 461, loss: 0.057\n",
      "epoch: 462, loss: 0.057\n",
      "epoch: 463, loss: 0.057\n",
      "epoch: 464, loss: 0.057\n",
      "epoch: 465, loss: 0.057\n",
      "epoch: 466, loss: 0.057\n",
      "epoch: 467, loss: 0.057\n",
      "epoch: 468, loss: 0.056\n",
      "epoch: 469, loss: 0.056\n",
      "epoch: 470, loss: 0.056\n",
      "epoch: 471, loss: 0.056\n",
      "epoch: 472, loss: 0.056\n",
      "epoch: 473, loss: 0.056\n",
      "epoch: 474, loss: 0.056\n",
      "epoch: 475, loss: 0.056\n",
      "epoch: 476, loss: 0.056\n",
      "epoch: 477, loss: 0.056\n",
      "epoch: 478, loss: 0.056\n",
      "epoch: 479, loss: 0.056\n",
      "epoch: 480, loss: 0.056\n",
      "epoch: 481, loss: 0.056\n",
      "epoch: 482, loss: 0.056\n",
      "epoch: 483, loss: 0.056\n",
      "epoch: 484, loss: 0.056\n",
      "epoch: 485, loss: 0.056\n",
      "epoch: 486, loss: 0.056\n",
      "epoch: 487, loss: 0.056\n",
      "epoch: 488, loss: 0.056\n",
      "epoch: 489, loss: 0.056\n",
      "epoch: 490, loss: 0.056\n",
      "epoch: 491, loss: 0.056\n",
      "epoch: 492, loss: 0.056\n",
      "epoch: 493, loss: 0.056\n",
      "epoch: 494, loss: 0.056\n",
      "epoch: 495, loss: 0.056\n",
      "epoch: 496, loss: 0.056\n",
      "epoch: 497, loss: 0.056\n",
      "epoch: 498, loss: 0.056\n",
      "epoch: 499, loss: 0.056\n",
      "tensor([[1.3060],\n",
      "        [1.8283],\n",
      "        [2.3505],\n",
      "        [2.8727],\n",
      "        [3.3950],\n",
      "        [3.9172],\n",
      "        [4.4395],\n",
      "        [4.9617],\n",
      "        [5.4839],\n",
      "        [6.0062]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "for step in range(500):\n",
    "\n",
    "    prediction = model(X)\n",
    "\n",
    "    loss = criterion(input=prediction, target=y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"epoch: {step}, loss: {loss:.3f}\")\n",
    "\n",
    "################################\n",
    "\n",
    "print(model(X)) # predict(학습 후)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05579313635826111, 0.5222377181053162, 0.7837941646575928)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "def mse_loss(preds, trues):  \n",
    "\n",
    "    return torch.sum((preds - trues)**2) / preds.view(-1).shape[0]\n",
    "\n",
    "mse_loss(model(X), y)\n",
    "\n",
    "################################\n",
    "\n",
    "loss.data.item(), model.weight.data.item(), model.bias.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0nUlEQVR4nO3deXiU5d238TMJSxCTgaCBUCNGxAcRF0BRwAV3rGLVqkXBvdYiVrHVVp9aKbWC2mpdqLG40iKgry0itkAtKlaRRRErxSoiKo8GUaJJQBMhmfePq4AICAkzuScz5+c45pDrzj1z/2yOOl+uNSsej8eRJElKgOyoC5AkSenDYCFJkhLGYCFJkhLGYCFJkhLGYCFJkhLGYCFJkhLGYCFJkhLGYCFJkhKmWWM/sK6ujg8//JC8vDyysrIa+/GSJKkB4vE4VVVVdOzYkezsrfdLNHqw+PDDDykuLm7sx0qSpARYvnw5u+2221Z/3ujBIi8vDwiF5efnN/bjJUlSA1RWVlJcXLzhe3xrGj1YrB/+yM/PN1hIktTEbGsag5M3JUlSwhgsJElSwhgsJElSwhgsJElSwhgsJElSwhgsJElSwhgsJElSwhgsJElSwhgsJElSwhgsJElSwtQ7WHzwwQcMGTKEdu3a0apVK/bbbz9efvnlZNQmSZKamHqdFfLpp5/Sr18/jjrqKKZNm8auu+7KkiVLaNu2bbLqkyRJTUi9gsUtt9xCcXExDz300IZrJSUlCS9KkiQ1TfUaCnnyySc56KCDOPPMMyksLKRHjx7cd9993/iempoaKisrN3lJkqT0VK9g8c4771BaWkqXLl2YMWMGQ4cO5YorrmDcuHFbfc/o0aOJxWIbXsXFxTtctCRJCl5+GX76U4jHo64kyIrHt7+UFi1acNBBBzF79uwN16644grmz5/PSy+9tMX31NTUUFNTs6FdWVlJcXExFRUV5Ofn70DpkiRlrngc7roLrrkG1q6FcePgvPOS97zKykpisdg2v7/rNceiqKiIbt26bXJtn3324c9//vNW39OyZUtatmxZn8dIkqRv8OmncNFF8MQToX366XDKKZGWtEG9hkL69evHm2++ucm1t956i06dOiW0KEmStGVz5kCPHiFUtGgBd98Njz8ObdpEXVlQr2Bx1VVXMWfOHEaNGsXbb7/NhAkTGDt2LMOGDUtWfZIk6b/GjoXDD4f33oPOnWH2bLj8csjKirqyjeoVLA4++GAmT57MxIkT6d69OzfeeCN33HEHgwcPTlZ9kiTpv7p0gdpaOOssWLAAevWKuqLN1WvyZiJs7+QPSZIEn3226TDHK69Az56N30uxvd/fnhUiSVIKqquD0aOhpATeemvj9V69Umvo4+sMFpIkpZiVK+HEE+F//zf0WEycuO331NbFeWnpKqYs/ICXlq6iti6ajS3qtdxUkiQl13PPwTnnQFkZtGoFY8bAhRd+83umLypj5NTFlFVUb7hWFMtlxMBuDOhelNyCv8YeC0mSUkBtLfzqV3DMMSFUdOsG8+eH/Sq+aehj+qIyho5fsEmoAFhRUc3Q8QuYvqgsyZVvymAhSVIKuP9+GDEizK248EKYNw/23feb31NbF2fk1MVsadBj/bWRUxc36rCIwUKSpBRw0UVw7LHwxz/Cgw9C69bbfs+8ZeWb9VR8VRwoq6hm3rLyxBW6DQYLSZIisG4d3HtvOOcDoHlz+Pvf4dxzt/8zVlZtPVQ05L5EMFhIktTIPvggzKUYOhR+/vON1+u7jLQwLzeh9yWCwUKSpEY0fToceCA8/zzsvHPY7KqhepcUUBTLZWt5JIuwOqR3SUHDH1JPBgtJkhrB2rVw3XVhf4pPPgkHiS1YAIMGNfwzc7KzGDEwnDr+9XCxvj1iYDdyshtvRy2DhSRJSbZ8OfTvDzffHNrDhoUDxLp02fHPHtC9iNIhPekQ23S4o0Msl9IhPRt9Hws3yJIkKcm++AL+9S/Iz4cHHoAzzkjs5w/oXsRx3Towb1k5K6uqKcwLwx+N2VOxnsFCkqQkiMc3Tsbce2947DH4n/+BPfdMzvNysrPo07ldcj68HhwKkSQpwd59Fw47DJ59duO1E09MXqhIJQYLSZISaPLkMDFz9my4/PKwk2YmMVhIkpQANTVwxRVw+unhRNJDD4W//Q2yM+ybNsP+dSVJSrylS6FfP7j77tC+5pqwT0WnTtHWFQUnb0qStAOWLQubXFVWQrt2MG4cnHRS1FVFx2AhSdIO2GOPECSWL4eJE2G33aKuKFoGC0mS6umtt2CXXaCgICwpvf9+aNECmvmt6hwLSZLqY8IE6NUrHHMej4drO+1kqFjPYCFJ0nb4/HO45BIYPBhWr4aKivBPbcpgIUnSNrzxBhxySBjyyMqCESPgH/+AvLyoK0s9dtxIkvQNxo2Dyy4LPRYdOsAjj8DRR0ddVeqyx0KSpK1YvRpuuCGEimOPhYULDRXbYo+FJElbsfPOMGkSzJwJ110HOTlRV5T6DBaSJP1XPB6ONW/eHM4/P1zr0ye8tH0MFpIkAVVV8MMfhuWkublhi+699oq6qqbHYCFJyngLF8JZZ8GSJWG445e/zIwjzpPBYCFJyljxONx7L1x1VTidtLg4zKno2zfqypoug4UkKSPF43DOOSFIAAwcCA89FA4SU8O53FSSlJGysmDvvcNW3LffDlOmGCoSwR4LSVLGiMfhs8+gbdvQvuEGOOMM2G+/SMtKK/ZYSJIywqefwumnwzHHQHV1uJaTY6hINIOFJCntzZ0LPXrAE0/Av/8Nc+ZEXVH6MlhIktJWPA633QaHHQbvvQedO8Ps2dC/f9SVpS/nWEiS0tKqVXDBBfDUU6F91lkwdizEYpGWlfbssZAkpaVLLw2homVLKC0Ny0oNFclnj4UkKS399rfwwQdhA6wDDoi6msxhj4UkKS18/DGMG7exvcceYT6FoaJx2WMhSWryZs0Ku2h++CEUFsKJJ4brWVnR1pWJ7LGQJDVZtbVw441w9NEhVOyzTzjvQ9Gxx0KS1CStWAFDhsDMmaF9wQUwZgy0bl2/z6mtizNvWTkrq6opzMuld0kBOdl2dTSUwUKS1OQ880wY+vjoI9hpp7Dq47zz6v850xeVMXLqYsoqqjdcK4rlMmJgNwZ0L0pgxZnDoRBJUpPz4YchVHTvDq+80vBQMXT8gk1CBcCKimqGjl/A9EVlCao2s9hjIUlqEuLxjZMxhwwJ7TPOgFat6v9ZtXVxRk5dTHxLzwGygJFTF3Nctw4Oi9STPRaSpJQ3Y0Y462Plyo3Xzj23YaECYN6y8s16Kr4qDpRVVDNvWXnDHpDBDBaSpJS1bh1cdx0MGACvvQY33ZSYz11ZtfVQ0ZD7tJFDIZKklLR8OZx9Nrz4YmhfdhnccktiPrswLzeh92kjeywkSSnnr3+FAw8MoSI/Hx57DH7/e8hN0Pd875ICimK5bG32RBZhdUjvkoLEPDCDGCwkSSll4kQ4+WQoL4devWDBAjjzzMQ+Iyc7ixEDuwFsFi7Wt0cM7ObEzQYwWEiSUspJJ8Fee8GVV4Yei86dk/OcAd2LKB3Skw6xTbtBOsRyKR3S030sGigrHo9vabVN0lRWVhKLxaioqCA/P78xHy1JSlEvvQSHHrpxOWlVFeTlNc6z3Xlz+2zv97c9FpKkyNTUwPDh0Ldv2I57vcYKFRCGRfp0bsd3DvwWfTq3M1TsIFeFSJIi8c47cNZZYedMCGd/qOkzWEiSGt3jj8PFF0NlJRQUwB//GOZWqOlzKESS1Giqq8N+FGeeGUJFv36wcKGhIp0YLCRJjeb112Hs2PDn666D556D4uJIS1KCORQiSWo0Bx8Md90VlpCecELU1SgZ7LGQJCXN55/DsGGwaNHGa5ddZqhIZ/UKFr/85S/Jysra5NW1a9dk1SZJasLeeAMOOQTuuSec+VFbG3VFagz1HgrZd999+cc//rHxA5o5miJJ2tS4caFn4vPPoX17uOMOyMmJuio1hnqngmbNmtGhQ4dk1CJJauLWrAlDH+PGhfYxx8D48eDXRuao9xyLJUuW0LFjR/bcc08GDx7M+++//43319TUUFlZuclLkpR+PvwwTM4cNw6ys+HGG2HGDENFpqlXsDjkkEN4+OGHmT59OqWlpSxbtozDDz+cqqqqrb5n9OjRxGKxDa9i1xVJynC1dXFeWrqKKQs/4KWlq6ita9Qjm5KmfXsoLISOHeGZZ+D66x3+yEQ7dAjZZ599RqdOnbj99tu5+OKLt3hPTU0NNTU1G9qVlZUUFxd7CJmkjDR9URkjpy6mrKJ6w7WiWC4jBnZrkqdpVlVBixbQsmVol5VBs2aw667R1qXEa5RDyNq0acPee+/N22+/vdV7WrZsSX5+/iYvScpE0xeVMXT8gk1CBcCKimqGjl/A9EVlEVXWMAsXQq9ecPXVG68VFRkqMt0OBYvVq1ezdOlSioqaXsqWpMZUWxdn5NTFbKmLeP21kVMXN4lhkXgcSkvDMedLlsATT8Cnn0ZdlVJFvYLF1VdfzaxZs3j33XeZPXs2p512Gjk5OZx99tnJqk+S0sK8ZeWb9VR8VRwoq6hm3rLyxiuqASoqYNCgsJS0pgZOPjn0XLRtG3VlShX1Wm76f//3f5x99tmsWrWKXXfdlcMOO4w5c+awq/1ekvSNVlZtPVQ05L4ovPJKOOb8nXfCPIrRN8fp+51yXlheTeFnufQuKSAnOyvqMhWxegWLSZMmJasOSUprhXm5Cb2vsX3xBXz727ByJXTqBMNHfcKj773GmPvTYxKqEsezQiSpEfQuKaAolsvW/j6fRfhi7l1S0JhlbbdWreDee+G00+A3j6zgzn/NTZtJqEosg4UkNYKc7CxGDOwGsFm4WN8eMbBbSg0lzJ0LM2dubJ92Gjz2/+Lc/vy/02ISqpLDYCFJjWRA9yJKh/SkQ2zT4Y4OsVxKh/RMmSGEeBxuuw0OOyxM1Pzgg40/m/9uekxCVfJ4gpgkNaIB3Ys4rlsH5i0rZ2VVNYV5qTXpcdUquOACeOqp0D7qKNh5540/T4dJqEoug4UkNbKc7Cz6dG4XdRmbefHFcLz58uVhJ8077oBLL4Wsr2Sepj4JVcnnUIgkZbh4HG65BY48MoSKLl1gzhz44Q83DRXQ9CehKvkMFpKU4bKy4M03obYWzjkn7Fdx4IFbvrcpTkJV4zJYSFKGqqvb+OcxY2DiRBg/HvLyvvl9TWUSqqKxQ6ebNsT2no4mSUqO2loYNSosJ33ySchu4F8xa+viKTsJVYm3vd/fTt6UpAyyYgUMGbJxf4q//S2c99EQqToJVdFyKESSMsTMmWHuxMyZsNNO8PDDDQ8V0tYYLCQpzdXWwg03wHHHwUcfQffuMH8+nH9+1JUpHTkUIklp7pJL4KGHwp+//324887QYyElgz0WkpTmLr8c2rWDRx6B++4zVCi57LGQpDSzbl0Y6ujTJ7R79oT33oPWraOtS5nBHgtJSiPLl0P//uG1YMHG64YKNRaDhSSlib/+Naz6ePFFyM2FsrKoK1ImMlhIUhO3di1cfXVYOlpeDr16hd6Kk06KujJlIudYSFIT9u67MGhQ2EUT4Ior4NZbw+mkUhQMFpLUhP35zyFUtGkDDz4Ip50WdUXKdAYLSWrCrroqbHp12WWwxx5RVyM5x0KSmpR33glnfXz+eWhnZ4ehD0OFUoU9FpLURDz+OFx8MVRWQkEB3HVX1BVJm7PHQpJSXHU1DBsGZ54ZQkXfvnDNNVFXJW2ZwUKSUtiSJWEHzXvuCe1rr4XnnoPi4kjLkrbKoRBJSlFPPw2nnw6rV8Muu8Cf/gQDBkRdlfTNDBaSlKK6dQs7aPbqBRMmQMeOUVckbZvBQpJSyMqVUFgY/vytb8E//wl77QXN/K+1mgjnWEhSihg3DvbcEyZP3nita1dDhZoWg4UkRWzNGrjggvBaswYeeSTqiqSGM1hIUoQWLYKDDw69FdnZ8KtfwaOPRl2V1HB2sElSBOLxcLbH5ZeHfSqKisIEzf79o65M2jEGC0mKwLx58P3vhz8ff3xYSrp+0qbUlBksJCkChxwCw4eHMPGzn4VhECkdGCwkqRHE4/DAA/Dtb2/cj+J3v4u2JikZzMiSlGSVlTBoEFxyCZxzDqxbF3VFUvLYYyFJSfTKK/C978HSpWE/ipNPdthD6c1gIUlJEI/DmDFw9dXw5ZfQqRNMmgSHHhp1ZVJyGSwkKcEqKuCii+AvfwntU08NS0vbto20LKlR2CEnSQnWrBn85z/QvDnceWcIGIYKZQp7LCQpAeLx8MrOhtat4bHH4Isv4KCDoq5Malz2WEjSDlq1Ck45BX77243X9t3XUKHMZLCQpB0wezb06AFPPRXO+fjkk6grkqJlsJCkBqirg1tugSOOgOXLoUsXeOEF2GWXqCuTouUcC0mqp48/hvPOg+nTQ/vss+EPf4C8vGjrklKBwUKS6qGmJpzzsWwZ5ObCXXeFw8SysqKuTEoNDoVIUj20bBkOD+vaNZxQesklhgrpqwwWkrQNH30EixdvbP/oR2Gr7v32i64mKVUZLCTpGzzzDBx4IHznO+EwMQg9FDvtFGlZUsoyWEjSFtTWwogRcOyxsGJFGAJZtSrqqqTU5+RNSfqaDz+EwYPhuedC++KLwyRNeymkbTNYSNJX/P3vMGRIWFLaunVYRjp4cNRVSU2HwUKS/iseh9/9LoSKAw4I533svXfUVUlNi3MsJOm/srJg3Dj46U/hpZcMFVJDGCwkZbS//S0EifUKC8NW3a1aRVeT1JQ5FCIpI61dCz//OfzmN6F92GHhhFJJO8ZgISnjvPceDBoEc+aE9o9+BCecEG1NUrowWEjKKFOmwIUXwqefQiwGDz4Ip58edVVS+nCOhaSMMXIknHpqCBW9e8OrrxoqpEQzWEjKGAcfHFZ+/OQn8M9/QklJ1BVJ6cehEElp7aOPoH378OdvfzscJta1a7Q1Selsh3osbr75ZrKyshg+fHiCypGkxKiuhssvh332CZM11zNUSMnV4GAxf/58/vCHP7D//vsnsh5J2mFLlkDfvvD734f5FNOnR12RlDkaFCxWr17N4MGDue+++2jbtm2ia5KkBps0CXr2DBMzd9klbIB16aVRVyVljgYFi2HDhnHSSSdx7LHHbvPempoaKisrN3lJUqJ98UUIEGefDatXw+GHw8KFcOKJUVcmZZZ6B4tJkyaxYMECRo8evV33jx49mlgstuFVXFxc7yIlaVtuvx3Gjg2rPq6/Hp55Br71rairkjJPvYLF8uXLufLKK3nkkUfIzc3drvdcd911VFRUbHgtX768QYVK0jf5yU9gwACYMQNuvBGaueZNikRWPB6Pb+/NTzzxBKeddho5OTkbrtXW1pKVlUV2djY1NTWb/GxLKisricViVFRUkJ+f3/DKJWW0NWvgnnvgxz+GbfxnR1ICbO/3d70y/THHHMPrr7++ybULL7yQrl278rOf/WyboUKSEuHf/4azzgp7UqxZA7/8ZdQVSVqvXsEiLy+P7t27b3KtdevWtGvXbrPrkpRo62rjjPjNGn47cie+rM6mqChO//5ZUZcl6SschZTUJEyet4IfXBrnk4VFAOTu8TEdz3mD6l26AEXRFidpg3rNsUgE51hIqq97J3/Mj77finXlO0NWHW0Of4v8Q5eS/d/OitIhPRnQ3XAhJdP2fn97CJmklFZbF6f0uaXUVrYiJ+8L2p8zh1ifpWRlwfq/FY2cupjaukb9O5KkrXAoRFJKqq0Nqz3mLSunotUqdj3tZVp0qCBnp7Wb3BcHyiqqmbesnD6d20VTrKQN7LGQlHIWLIDu3WHOHFhZVQ1Aqz0/2SxUfNX6+yRFy2AhKWXE4zBmDPTpA//5D/z0p1CYt32b8W3vfZKSy6EQSSnhs8/g4ovhL38J7VNOgYceglibAopiuayoqGZLsyiygA6xXHqXFDRitZK2xh4LSZGbNw969AihonlzuOMOeOIJKCiAnOwsRgzsBoQQ8VXr2yMGdiMn2/0spFRgsJAUqVdfhcMOg3ffhZISePFFuPLKcJjYegO6F1E6pCcdYpsOd3SI5brUVEoxDoVIitSBB8K3vx0ODbv/fmjTZsv3DehexHHdOjBvWTkrq6opzAvDH/ZUSKnFYCGp0c2bB/vsA3l5oWdi4kTIzd20l2JLcrKzXFIqpTiHQiQ1mro6uPVW6NsXLr00rAIBaNVq26FCUtNgj4WkRvHxx3D++TBtWmjH47B2LbRoEW1dkhLLHgtJSff882EuxbRpYchj7FiYMMFQIaUjg4WkpKmrg5tugqOOgg8/hP/5H5g7Fy65xKEPKV0ZLCQlTXl52Emzrg7OPRdefhn23z/qqiQlk3MsJCXNLruEIY9334ULLrCXQsoEBgtJCVNbCzfeGIY8zj47XDvqqGhrktS4DBaSEqKsDAYPhmefhZ13hqOPhvbto65KUmNzjoWkHfb002HVx7PPQuvWUFpqqJAylcFCUoOtWwfXXw8nnAArV4aJmS+/DEOGRF2ZpKg4FCKpQdauhWOPDXtUQNhJ83e/C7toSspc9lhIapDmzaF373Dex8SJcO+9hgpJBgtJ9bB2bdiae71Ro+C112DQoOhqkpRaDBaStsv778ORR8Ipp4SAAaHXoqQk2rokpRaDhaRtevLJsOrjpZfgjTdg8eKoK5KUqgwWUgaorYvz0tJVTFn4AS8tXUVtXXy73vfll3DVVfCd78Cnn8LBB8Orr8IBByS5YElNlqtCpDQ3fVEZI6cupqyiesO1olguIwZ2Y0D3oq2+b9ky+N73YP780L7qKrj5Zk8klfTN7LGQ0tj0RWUMHb9gk1ABsKKimqHjFzB9UdlW33vhhSFUtG0LU6bA7bcbKiRtm8FCSlO1dXFGTl3MlgY91l8bOXXxVodFxo6F448PQx+nnJK0MiWlGYOFlKbmLSvfrKfiq+JAWUU185aVA/D22/DAAxt/vvfeMGMGdOqU5EIlpRXnWEhpamXV1kPF1+979FG45BJYswY6d4b+/ZNbm6T0ZbCQ0lRhXu4276lbm83Dt7bjiYmhfdhhsNdeSS5MUlpzKERKU71LCiiK5ZK1lZ+vW9WaVRMO44mJuWRlwc9/Hk4n3W23Ri1TUpoxWEhpKic7ixEDuwFsFi7WLO7Ih+MO4/MVeRQWhrkUv/41NLMPU9IOMlhIaWxA9yJKh/SkQ2zTYZG8ZrnE1zbjqKNg4UI47rho6pOUfvz7iZTmBnQv4rhuHZi9pJzyL6opzMvl4D0KmPwXOP10yMmJukJJ6cRgIaW5eBz+OC6L3/62HS+8EDa8AjjzzGjrkpSeHAqR0tjq1XD++XDRReHgsN//PuqKJKU7eyykNPWvf8FZZ8Gbb0J2Ntx4I1x7bdRVSUp3BgspzcTjcN99cOWVUF0N3/oWTJwIhx8edWWSMoFDIVKaufNOuPTSECpOPDGs+jBUSGosBgspzZx3Huy5J9x6Kzz1FOyyS9QVScokDoVITVw8Dk8/HfaiyMqCggL4978hd9s7ektSwtljITVhn30WJmiecAI8+ODG64YKSVGxx0JqoubPh+99D5Ytg+bNw5wKSYqaPRZSExOPwx13QL9+IVTssQe88AIMGxZ1ZZJkj4XUpJSXh82upkwJ7dNPhwcegDZtIi1Lkjawx0JqQv71L3jySWjRAsaMgccfN1RISi32WEhNSP/+cPfd0KcP9OwZdTWStDl7LKQU9sknYYLmkiUbrw0bZqiQlLrssZBS1D//CWefDR98AP/3f2GCZlZW1FVJ0jezx0JKMXV1MGoUHHVUCBV77w2lpYYKSU2DPRZSClm5Es49F/7+99AeMiSEip13jrYuSdpeBgspRbz5ZuilKCuDVq3Cqo8LL7SnQlLTYrCQUkRJCRQXQ9u28NhjsO++UVckSfVnsJAi9NFH4dCw5s3D3hRPPAH5+dC6ddSVSVLDOHlTisg//gH77w+/+MXGa0VFhgpJTZvBQmpk69bB9dfD8ceHyZrTp3uAmKT0YbCQGtEHH8DRR8NNN4XDxH7wA3jpJY85l5Q+nGMhNZJp0+C888JumjvvDPfdB4MGRV2VJCWWwUJqBOXlYWvuqiro0QMefRS6dIm6KklKvHoNhZSWlrL//vuTn59Pfn4+ffr0Ydq0acmqTUobBQVwzz3hnI/Zsw0VktJXVjwej2/vzVOnTiUnJ4cuXboQj8cZN24cv/nNb3j11VfZdzsX3VdWVhKLxaioqCA/P7/BhUupbupUiMXgiCOirkSSdtz2fn/XK1hsSUFBAb/5zW+4+OKLE1qY1FR9+SVcdx3cfntYPvraa7DrrlFXJUk7Znu/vxs8x6K2tpb/9//+H2vWrKFPnz5bva+mpoaamppNCpPS1bJlYULmvHmh/b3vhV4LScoU9Q4Wr7/+On369KG6upqdd96ZyZMn061bt63eP3r0aEaOHLlDRUpNwV/+AhddBBUV0KYNPPwwfOc7UVclSY2r3kMhX375Je+//z4VFRU8/vjj3H///cyaNWur4WJLPRbFxcUOhShtrFsHP/4x3H13aB96KEyaBJ06RVuXJCVS0oZCWrRowV577QVAr169mD9/PnfeeSd/+MMftnh/y5YtadmyZX0fIzUZOTlhB02Aa64Jm181bx5tTZIUlR3ex6Kurm6THgkpU6xbB82ahWPNx44NwyDHHx91VZIUrXoFi+uuu44TTzyR3XffnaqqKiZMmMBzzz3HjBkzklWflHK++AKuugo+/hgefzwEi/x8Q4UkQT2DxcqVKznvvPMoKysjFoux//77M2PGDI477rhk1SellDffhLPOgn/9KwSKuXPDnApJUlCvYPHAAw8kqw4p5Y0fDz/8IaxZE/alGD/eUCFJX+fpptI2fP45XHwxnHtuCBX9+8PChQ59SNKWGCykbTjjDHjwwTD0MWIE/OMf0LFj1FVJUmrydFNpG66/Hl5/HcaNg6OPjroaSUptBgvpa1avhldegSOPDO2+feHtt8HtWCRp2xwKkb7i9dfh4IPhxBPh3//eeN1QIUnbx2AhAfE43Hcf9O4N//kPtG0LVVVRVyVJTY/BQhmvqgoGD4Yf/ACqq0NvxcKFLiWVpIYwWCijvfoq9OwJEyeGMz9uuQWeeirsUyFJqj8nbyqjTZkSJmYWF4cTSfv23fiz2ro485aVs7KqmsK8XHqXFJCTnRVdsZLUBBgslNGuvx5qa8PZHwUFG69PX1TGyKmLKauo3nCtKJbLiIHdGNC9KIJKJalpcChEGWX+/LDh1foDeZs1gxtv3DxUDB2/YJNQAbCiopqh4xcwfVFZI1YsSU2LwUIZIR6HO++Efv3gz3+GUaO2fF9tXZyRUxcT39Jn/PefI6cuprZuS3dIkgwWSnvl5XDaaTB8OKxdC6efHoY+tmTesvLNeiq+Kg6UVVQzb1l5UmqVpKbOYKG0NmcO9OgRJmm2aAF33w2PPw5t2mz5/pVVWw8VDblPkjKNkzeVtiZOhPPOg3XroHNnePRR6NXrm99TmJe7XZ+9vfdJUqYxWCjhUmWZ5qGHQuvWcMIJYVfN/Pxtv6d3SQFFsVxWVFRvcZ5FFtAhFv6dJEmbM1gooaJeprl8ediTAqCkJGyAtcce4cjz7ZGTncWIgd0YOn4BWbBJuFj/ESMGdnM/C0naCudYKGGiXKZZVwejR4chj+nTN14vKdn+ULHegO5FlA7pSYfYpsMdHWK5lA7p6T4WkvQN7LFQQmxrmWYWYZnmcd06JPxv+ytXwrnnwt//HtrTpsGAATv2mQO6F3Fctw4pMaQjSU2JwUIJUZ9lmn06t0vYc597Ds45B8rKoFUrGDMGLrwwMZ+dk52V0FolKRM4FKKEaOxlmrW18KtfwTHHhFDRrVvYVfOii+o/9CFJShyDhRKisZdp/v3vMGJEmFtx4YUwbx7su29CPlqStAMcClFCNPYyzRNPhGHDoHfvsFeFJCk12GOhhFi/TBM2LstcLxHLNNetg5tvho8/3nhtzBhDhSSlGoOFEiZZyzQ/+CDMpbjuuhAk4p7/JUkpy6EQJVSil2lOnx6Wkn7yCey8M5x/vpMzJSmVGSyUcIlYprl2LdxwQxj+gHCQ2KOPQpcuCShQkpQ0BgulnLIyOOMMmD07tIcNg9/+FnI990uSUp7BQilnp51gxQqIxeCBB+C73426IknS9jJYKCWsXQvNmoX5E7EYTJ4c5lTsuWfUlUmS6sNVIYrcu+/C4YfDPfdsvLb//oYKSWqKDBaK1OTJYWLm3Lnw61/D559HXZEkaUcYLBSJmhq44go4/XT47DM49FCYMyfMr5AkNV0GCzW6pUuhXz+4++7QvuYaeP556NQp2rokSTvOyZtqVBUV4XyP8nJo1w7GjYOTToq6KklSothjoUYVi4UeisMOg4ULDRWSlG6y4vHGPXmhsrKSWCxGRUUF+fn5jfloReStt8Lx5l27hnZdXXg1s79MkpqM7f3+tsdCSTVhAvTqFXbSXL/iIzvbUCFJ6cpgoaT4/HO45BIYPBhWr4Zdd4U1a6KuSpKUbAYLJdwbb8Ahh8D994edNG+4Af7xjxAuJEnpzQ5pJdS4cXDZZaHHon17eOQROOaYqKuSJDUWeyyUMHV18OCDIVQceyy89pqhQpIyjT0WSpjs7DBZc8IE+MlPQluSlFn8T78aLB4P8yiuvXbjtW99K+xTYaiQpMxkj4UapKoKLr0UJk4M7W9/G444ItqaJEnRM1io3hYuhLPOgiVLICcHRo0KO2lKkmSw0HaLx+Hee+Gqq8LppMXFMGkS9O0bdWWSpFRhsNB2+8EPwpwKgIED4aGHwkFikiSt5xQ7bbcBA6B5c7j9dpgyxVAhSdqcPRbaqngc3n8fOnUK7e9+N8yrWN+WJOnr7LHQFn36aQgSBx8MH3648bqhQpL0TQwW2szcudCjB0yeDBUVMG9e1BVJkpoKg4U2iMfhttvC0tH33oPOnWH2bDj11KgrkyQ1Fc6xEACrVsEFF8BTT4X2WWfBffdBfn6kZUmSmhh7LASETa6eegpatgx7VUyaZKiQJNWfPRYC4Fe/gqVLYeRIOOCAqKuRJDVV9lhkqI8/DmGiri60W7eGJ54wVEiSdow9Fhlo1iw455ywjHSnneDqq6OuSJKULuyxyCC1tXDjjXD00SFU7LNP2E1TkqREscciQ6xYAUOGwMyZoX3BBTBmTBgCkSQpUQwWGeD558Py0Y8+CkMfpaVw3nlRVyVJSkf1GgoZPXo0Bx98MHl5eRQWFnLqqafy5ptvJqs2JchOO0F5OXTvDq+8YqiQJCVPvYLFrFmzGDZsGHPmzOHpp59m7dq1HH/88axZsyZZ9amBvvxy458POgimTQtbdXftGl1NkqT0lxWPx+MNffPHH39MYWEhs2bN4ogjjtiu91RWVhKLxaioqCDfHZiSYsYMuOSScLR5jx5RVyNJSgfb+/29Q6tCKioqACgoKNjqPTU1NVRWVm7yUnKsWwfXXRdWeixfHnbTlCSpMTU4WNTV1TF8+HD69etH9+7dt3rf6NGjicViG17FxcUNfaS+wfLl0L8/3HxzaF92GfzpT5GWJEnKQA0eChk6dCjTpk3jhRdeYLfddtvqfTU1NdTU1GxoV1ZWUlxc7FBIAv31r2FCZnl5ON/jgQfgjDOirkqSlE62dyikQctNL7/8cp566imef/75bwwVAC1btqRly5YNeYy2w8yZcPLJ4c8HHQSPPgp77hltTZKkzFWvYBGPx/nRj37E5MmTee655ygpKUlWXdpORx0FJ5wQVnvccks4nVSSpKjUK1gMGzaMCRMmMGXKFPLy8lixYgUAsViMVq1aJaVAbW7GDDj88LA/RXY2TJ0KzZtHXZUkSfWcvFlaWkpFRQX9+/enqKhow+vRRx9NVn0ZpbYuzktLVzFl4Qe8tHQVtXWbTn+pqYHhw8Oqjyuv3HjdUCFJShX1HgpRckxfVMbIqYspq6jecK0olsuIgd0Y0L2Id94J23K/8kr4WZs24cjzbI+RkySlEM8KSQHTF5UxdPwCvh7bVlRUM3T8As4p7MuYkW2prISCAvjjH+GkkyIpVZKkb2SwiFhtXZyRUxdvFioA6tZl8+kz+zDq1bYA9OsHEyeCW4FIklKVHekRm7esfJPhj6+q/aI5a/7TEYDzfvg5zz1nqJAkpTZ7LCK2smrLoQKgWV4Nu5z8KgCnX7obzZrt1FhlSZLUIPZYRKwwL3fDn+vWZrNq2n58vqT9hmut9vyEVnt+ssl9kiSlKnssIta7pICiWC7vL23Gx1N6sPaTfD5f0oHc3Z8hu2UtWUCHWC69S7Z+0JskSanCYBGxnOws+sV7MfePOxNf24zs1tXscvLCDaECYMTAbuRkZ33j50iSlAoMFhFaswYuvxwefrgNAPmdy8kfsICcncOhbR2+so+FJElNgcEiIlVVcMgh8MYbYZOrX/4SfnZtW155vwcrq6opzAvDH/ZUSJKaEoNFRPLyoH9/qKiACRPgyCMBsujTuV3ElUmS1HCuCmlEVVWwcuXG9u23w8KF60OFJElNn8GikSxcCL16waBBUFsbruXmwq67RlqWJEkJZbBIsngcSkvh0ENhyZLwev/9qKuSJCk5DBZJVFEReiguuywceX7yyaHnoqQk6sokSUoOg0WSvPJKGPp47DFo1gxuuw2efBLaOTdTkpTG0mJVSG1dnHnLylNmmWZdHXz/+7B0KXTqBI8+GpaWSpKU7pp8sJi+qIyRUxdvckJoUcQbS2Vnwx//CKNGwT33QNu2kZQhSVKja9JDIdMXlTF0/ILNjh1fUVHN0PELmL6orNFqmTsXHnhgY3u//WDiREOFJCmzNNlgUVsXZ+TUxcS38LP110ZOXUxt3ZbuSJx4PMyfOOww+OEPQ8CQJClTNdlgMW9Z+WY9FV8VB8oqqpm3rDxpNaxaBaecAldfDevWwWmnQdeuSXucJEkpr8kGi5VVWw8VDbmvvmbPhh494KmnoGXLsFfFo49CLJaUx0mS1CQ02WBRmJeb0Pvq4/bb4YgjYPly6NIF5swJwyBZnhcmScpwTTZY9C4poCiWy9a+y7MIq0N6lxQk/NnZ2WFb7nPOCftVHHhgwh8hSVKT1GSDRU52FiMGdgPYLFysb48Y2C1h+1nU1Gz885VXwrRpMH58OKVUkiQFTTZYAAzoXkTpkJ50iG063NEhlkvpkJ4J2ceithZuvDH0SlRVhWtZWTBggEMfkiR9XZPfIGtA9yKO69YhKTtvfvQRDB4MM2eG9qRJcMklO/yxkiSlrSYfLCAMi/TpnNhDOGbODKHio49gp53CDprnn5/QR0iSlHaa9FBIMtTWwogRcNxxIVR07w4vv2yokCRpexgsvub66+FXvwo7an7/+2EnzX32iboqSZKaBoPF1wwfHvameOQRuO++MAwiSZK2T1rMsdgR69bB1KlhO26A9u1h8WJolvH/y0iSVH8Z3WOxfDn07w+nnx5OIl3PUCFJUsNkbLD461/D3hQvvgj5+dCiRdQVSZLU9GVcsFi7Fq65Bk4+GcrLoVcvWLAAvvvdqCuTJKnpy6hO//feg0GDwqFhAFdcAbfeGk4nlSRJOy6jgsWiRSFUtGkDDz0Ep54adUWSJKWXjAoWJ50UdtA88UTYY4+oq5EkKf2k9RyLd96B448PQyDrDR1qqJAkKVnSNlg8/jj06AFPPw3DhkVdjSRJmSHtgkV1NVx2GZx5JlRWQr9+UFoadVWSJGWGtAoWS5ZAnz4bg8S118Kzz0JxcbR1SZKUKdJm8ubcuXDssbB6NeyyC/zpTzBgQNRVSZKUWdImWBxwAHTuDLEYTJgA3/pW1BVJkpR50iZY5ObCjBnQrp1nfUiSFJW0+gpu3z7qCiRJymxpNXlTkiRFy2AhSZISxmAhSZISxmAhSZISxmAhSZISxmAhSZISxmAhSZISxmAhSZISxmAhSZISxmAhSZISxmAhSZISxmAhSZISxmAhSZISptFPN43H4wBUVlY29qMlSVIDrf/eXv89vjWNHiyqqqoAKC4ubuxHS5KkHVRVVUUsFtvqz7Pi24oeCVZXV8eHH35IXl4eWVlZjfnoJqGyspLi4mKWL19Ofn5+1OVkPH8fqcffSWrx95Fakvn7iMfjVFVV0bFjR7Kztz6TotF7LLKzs9ltt90a+7FNTn5+vv8nTSH+PlKPv5PU4u8jtSTr9/FNPRXrOXlTkiQljMFCkiQljMEixbRs2ZIRI0bQsmXLqEsR/j5Skb+T1OLvI7Wkwu+j0SdvSpKk9GWPhSRJShiDhSRJShiDhSRJShiDhSRJShiDRYoYPXo0Bx98MHl5eRQWFnLqqafy5ptvRl2W/uvmm28mKyuL4cOHR11Kxvrggw8YMmQI7dq1o1WrVuy33368/PLLUZeVkWpra/nFL35BSUkJrVq1onPnztx4443bPENCifP8888zcOBAOnbsSFZWFk888cQmP4/H49xwww0UFRXRqlUrjj32WJYsWdIotRksUsSsWbMYNmwYc+bM4emnn2bt2rUcf/zxrFmzJurSMt78+fP5wx/+wP777x91KRnr008/pV+/fjRv3pxp06axePFibrvtNtq2bRt1aRnplltuobS0lDFjxvDGG29wyy23cOutt3L33XdHXVrGWLNmDQcccAC///3vt/jzW2+9lbvuuot7772XuXPn0rp1a0444QSqq6uTXpvLTVPUxx9/TGFhIbNmzeKII46IupyMtXr1anr27Mk999zDr3/9aw488EDuuOOOqMvKONdeey0vvvgi//znP6MuRcDJJ59M+/bteeCBBzZc++53v0urVq0YP358hJVlpqysLCZPnsypp54KhN6Kjh078pOf/ISrr74agIqKCtq3b8/DDz/MoEGDklqPPRYpqqKiAoCCgoKIK8lsw4YN46STTuLYY4+NupSM9uSTT3LQQQdx5plnUlhYSI8ePbjvvvuiLitj9e3bl5kzZ/LWW28B8Nprr/HCCy9w4oknRlyZAJYtW8aKFSs2+e9WLBbjkEMO4aWXXkr68xv9EDJtW11dHcOHD6dfv35079496nIy1qRJk1iwYAHz58+PupSM984771BaWsqPf/xj/vd//5f58+dzxRVX0KJFC84///yoy8s41157LZWVlXTt2pWcnBxqa2u56aabGDx4cNSlCVixYgUA7du33+R6+/btN/wsmQwWKWjYsGEsWrSIF154IepSMtby5cu58sorefrpp8nNzY26nIxXV1fHQQcdxKhRowDo0aMHixYt4t577zVYROCxxx7jkUceYcKECey7774sXLiQ4cOH07FjR38fcigk1Vx++eU89dRTPPvssx4vH6FXXnmFlStX0rNnT5o1a0azZs2YNWsWd911F82aNaO2tjbqEjNKUVER3bp12+TaPvvsw/vvvx9RRZntmmuu4dprr2XQoEHst99+nHvuuVx11VWMHj066tIEdOjQAYCPPvpok+sfffTRhp8lk8EiRcTjcS6//HImT57MM888Q0lJSdQlZbRjjjmG119/nYULF254HXTQQQwePJiFCxeSk5MTdYkZpV+/fpstv37rrbfo1KlTRBVlts8//5zs7E2/PnJycqirq4uoIn1VSUkJHTp0YObMmRuuVVZWMnfuXPr06ZP05zsUkiKGDRvGhAkTmDJlCnl5eRvGwWKxGK1atYq4usyTl5e32fyW1q1b065dO+e9ROCqq66ib9++jBo1irPOOot58+YxduxYxo4dG3VpGWngwIHcdNNN7L777uy77768+uqr3H777Vx00UVRl5YxVq9ezdtvv72hvWzZMhYuXEhBQQG77747w4cP59e//jVdunShpKSEX/ziF3Ts2HHDypGkiislAFt8PfTQQ1GXpv868sgj41deeWXUZWSsqVOnxrt37x5v2bJlvGvXrvGxY8dGXVLGqqysjF955ZXx3XffPZ6bmxvfc8894z//+c/jNTU1UZeWMZ599tktfmecf/758Xg8Hq+rq4v/4he/iLdv3z7esmXL+DHHHBN/8803G6U297GQJEkJ4xwLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMAYLSZKUMP8fuyvbkjq0vhoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################\n",
    "\n",
    "def display_results(model, x, y):\n",
    "\n",
    "    pred = model(x)\n",
    "\n",
    "    plt.scatter(x.data.numpy(), y.data.numpy())\n",
    "\n",
    "    plt.plot(x.data.numpy(), pred.data.numpy(), 'b--')\n",
    "\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "display_results(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) 다중회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=24, out_features=12, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=12, out_features=3, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[ 0.5614,  0.1684, -0.2660]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2388], requires_grad=True)\n",
      "epoch 0, loss 2953.231689453125\n",
      "epoch 50, loss 501.0291442871094\n",
      "epoch 100, loss 374.3981628417969\n",
      "epoch 150, loss 363.02069091796875\n",
      "epoch 200, loss 353.9225769042969\n",
      "epoch 250, loss 346.05621337890625\n",
      "epoch 300, loss 339.94110107421875\n",
      "epoch 350, loss 332.91314697265625\n",
      "epoch 400, loss 323.9660949707031\n",
      "epoch 450, loss 311.5118103027344\n",
      "epoch 500, loss 301.8925476074219\n",
      "epoch 550, loss 292.87591552734375\n",
      "epoch 600, loss 280.0024108886719\n",
      "epoch 650, loss 259.9172058105469\n",
      "epoch 700, loss 245.048828125\n",
      "epoch 750, loss 236.04312133789062\n",
      "epoch 800, loss 230.585205078125\n",
      "epoch 850, loss 225.7568817138672\n",
      "epoch 900, loss 221.5509033203125\n",
      "epoch 950, loss 217.18109130859375\n",
      "epoch 1000, loss 213.98715209960938\n",
      "epoch 1050, loss 211.30288696289062\n",
      "epoch 1100, loss 209.03416442871094\n",
      "epoch 1150, loss 207.0231475830078\n",
      "epoch 1200, loss 204.9217529296875\n",
      "epoch 1250, loss 203.1005401611328\n",
      "epoch 1300, loss 201.5458221435547\n",
      "epoch 1350, loss 200.337890625\n",
      "epoch 1400, loss 199.0345001220703\n",
      "epoch 1450, loss 197.9102783203125\n",
      "epoch 1500, loss 196.88917541503906\n",
      "epoch 1550, loss 195.99301147460938\n",
      "epoch 1600, loss 194.40379333496094\n",
      "epoch 1650, loss 190.54995727539062\n",
      "epoch 1700, loss 178.8810272216797\n",
      "epoch 1750, loss 160.3074493408203\n",
      "epoch 1800, loss 142.5647735595703\n",
      "epoch 1850, loss 129.22451782226562\n",
      "epoch 1900, loss 118.0125961303711\n",
      "epoch 1950, loss 108.0429916381836\n",
      "epoch 2000, loss 99.32334899902344\n",
      "epoch 2050, loss 93.04702758789062\n",
      "epoch 2100, loss 88.43521881103516\n",
      "epoch 2150, loss 84.82015991210938\n",
      "epoch 2200, loss 81.20991516113281\n",
      "epoch 2250, loss 78.85912322998047\n",
      "epoch 2300, loss 75.7132568359375\n",
      "epoch 2350, loss 72.86227416992188\n",
      "epoch 2400, loss 71.0179443359375\n",
      "epoch 2450, loss 67.1336441040039\n",
      "epoch 2500, loss 64.41730499267578\n",
      "epoch 2550, loss 62.90046310424805\n",
      "epoch 2600, loss 60.81674575805664\n",
      "epoch 2650, loss 59.41219711303711\n",
      "epoch 2700, loss 57.22158432006836\n",
      "epoch 2750, loss 56.065711975097656\n",
      "epoch 2800, loss 56.001914978027344\n",
      "epoch 2850, loss 54.318241119384766\n",
      "epoch 2900, loss 53.611454010009766\n",
      "epoch 2950, loss 52.59465789794922\n",
      "epoch 3000, loss 51.68953323364258\n",
      "epoch 3050, loss 49.8990592956543\n",
      "epoch 3100, loss 49.686649322509766\n",
      "epoch 3150, loss 47.92367172241211\n",
      "epoch 3200, loss 48.27690887451172\n",
      "epoch 3250, loss 45.595558166503906\n",
      "epoch 3300, loss 45.02494430541992\n",
      "epoch 3350, loss 45.524749755859375\n",
      "epoch 3400, loss 40.29913330078125\n",
      "epoch 3450, loss 37.12678146362305\n",
      "epoch 3500, loss 34.269893646240234\n",
      "epoch 3550, loss 31.490406036376953\n",
      "epoch 3600, loss 28.78426742553711\n",
      "epoch 3650, loss 26.071590423583984\n",
      "epoch 3700, loss 24.184326171875\n",
      "epoch 3750, loss 22.98552894592285\n",
      "epoch 3800, loss 22.0673885345459\n",
      "epoch 3850, loss 21.41382598876953\n",
      "epoch 3900, loss 17.777690887451172\n",
      "epoch 3950, loss 18.15241050720215\n",
      "epoch 4000, loss 17.041688919067383\n",
      "epoch 4050, loss 15.042089462280273\n",
      "epoch 4100, loss 15.2216157913208\n",
      "epoch 4150, loss 14.656218528747559\n",
      "epoch 4200, loss 13.306892395019531\n",
      "epoch 4250, loss 13.312435150146484\n",
      "epoch 4300, loss 12.334308624267578\n",
      "epoch 4350, loss 12.032849311828613\n",
      "epoch 4400, loss 11.372079849243164\n",
      "epoch 4450, loss 11.022100448608398\n",
      "epoch 4500, loss 11.714895248413086\n",
      "epoch 4550, loss 10.41967487335205\n",
      "epoch 4600, loss 12.59436321258545\n",
      "epoch 4650, loss 11.379803657531738\n",
      "epoch 4700, loss 10.907068252563477\n",
      "epoch 4750, loss 9.500049591064453\n",
      "epoch 4800, loss 9.213309288024902\n",
      "epoch 4850, loss 9.01392936706543\n",
      "epoch 4900, loss 9.197233200073242\n",
      "epoch 4950, loss 8.390353202819824\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../DATA/ozone2.csv\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "################################\n",
    "\n",
    "df.columns\n",
    "\n",
    "################################\n",
    "\n",
    "X = df[['Solar.R', 'Wind', 'Temp']]\n",
    "\n",
    "y = df[[\"Ozone\"]]\n",
    "\n",
    "################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "################################\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array(X_train_scaled,dtype=np.float32)\n",
    "\n",
    "y_train = np.array(y_train,dtype=np.float32)\n",
    "\n",
    "X_test = np.array(X_test_scaled,dtype=np.float32)\n",
    "\n",
    "y_test = np.array(y_test,dtype=np.float32)\n",
    "\n",
    "################################\n",
    "\n",
    "import torch\n",
    "\n",
    "inputs = torch.from_numpy(X_train)\n",
    "\n",
    "targets = torch.from_numpy(y_train)\n",
    "\n",
    "################################\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "output_dim = 64  \n",
    "\n",
    "model = nn.Sequential(\n",
    "\n",
    "    nn.Linear(input_dim,24),\n",
    "\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(24,12),\n",
    "\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(12,3),\n",
    "\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(3,1)\n",
    "\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "print(model[6].weight)\n",
    "\n",
    "print(model[6].bias)\n",
    "\n",
    "################################\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.01)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "iteration_number = 5000\n",
    "\n",
    "for iteration in range(iteration_number):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    results = model(inputs)\n",
    "\n",
    "    \n",
    "\n",
    "    loss = mse(results, targets)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_list.append(loss.data)\n",
    "\n",
    "    if(iteration % 50 == 0):\n",
    "\n",
    "        print('epoch {}, loss {}'.format(iteration, loss.data))\n",
    "\n",
    "################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(iteration_number),loss_list)\n",
    "\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "################################\n",
    "\n",
    "input_x_test = torch.from_numpy(X_test)\n",
    "\n",
    "predicted = model(input_x_test.float()).data.numpy()\n",
    "\n",
    "predicted[0:5]\n",
    "\n",
    "################################\n",
    "\n",
    "loss.data.item() # mse\n",
    "\n",
    "################################\n",
    "\n",
    "np.sqrt(loss.data.item()) # rmse\n",
    "\n",
    "################################\n",
    "\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "plt.plot(y_test, label='real')\n",
    "\n",
    "plt.plot(model(X_test).detach().numpy(), label='pred')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
