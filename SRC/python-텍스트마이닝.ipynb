{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A. 데이터 마이닝과 텍스트 마이닝\n",
    "\n",
    "1) 데이터 마이닝\n",
    "\n",
    "① 데이터에서 의미있는 정보를 추출하는 기술\n",
    "\n",
    "② 고급 통계 분석과 모델링 기법을 적용하여 데이터 안의 패턴과 관계를 찾아내는 과정\n",
    "\n",
    "2) 텍스트 마이닝\n",
    "\n",
    "① 비정형 텍스트 데이터에서 분석 도구를 이용하여 패턴을 탐구하여 새롭고 의미있는 정보를 찾아내는 과정 또는 기술\n",
    "\n",
    "② 비정형 텍스트 데이터를 정형화 및 특징을 추출하는 과정이 요구됨\n",
    "\n",
    "③ 자연어 처리 기술에 기반한 텍스트 데이터 가공 기술\n",
    "\n",
    "④ 모든 문서의 단어들을 하나하나의 변수로 보기 때문에 변수의 갯수가 매우 많아지게 됨.  즉, 일반적인 데이터마이닝에 비해 데이터의 차원이 훨씬 커지게 됨.\n",
    "\n",
    "\n",
    "텍스트분류\\\n",
    "감정분석\\\n",
    "텍스트 생성(chatGPT)\\\n",
    "텍스트 요약\\\n",
    "토빅 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#말뭉치(코퍼스, corpus) : 텍스트마이닝에 적용되는 텍스트 데이터 집합\n",
    "\n",
    "#사전 처리\n",
    "\n",
    "#대소문자 통일\n",
    "\n",
    "#영문 텍스트 데이터는 대문자 또는 소문자로 변환하는 것이 좋음(보통 소문자로 변경)\n",
    "\n",
    "s=\"Hello World\"\n",
    "\n",
    "print(s.lower()) #소문자로 변환\n",
    "\n",
    "print(s.upper()) #대문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#숫자, 문장부호, 특수문자 제거\n",
    "\n",
    "# 단어가 아니기 때문에 분석에 불필요한 경우가 대부분임\n",
    "\n",
    "# 삭제할 경우 분석 결과가 왜곡된다고 생각될 경우 남겨둘 필요도 있음\n",
    "\n",
    "# 날짜,수치,백분율 등의 숫자는 각각의 문장에서는 의미가 있지만\n",
    "\n",
    "# 전체 문서 집합에서는 크게 의미가 없는 경우가 많으므로 지우는 것이 일반적\n",
    "\n",
    "import re\n",
    "\n",
    "#숫자 제거\n",
    "\n",
    "p=re.compile(\"[0-9]+\")\n",
    "\n",
    "result=p.sub(\"\",\"올해 들어 서울 지역의 부동산 가격이 30% 하락했습니다\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . , ? ! 등의 문장부호들은 삭제하는 것이 일반적\n",
    "\n",
    "# 각 문장에서는 특수한 역할을 수행할 수 있으나\n",
    "\n",
    "# 전체 말뭉치의 관점에서는 의미를 부여하기 어려운 경우가 대부분임.\n",
    "\n",
    "# - ( ) 등의 특수문자도 보통 삭제함\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(input_data):\n",
    "\n",
    "    #텍스트에 포함되어 있는 숫자와 특수문자 제거\n",
    "\n",
    "    p=re.compile(\"[0-9_!@#$%^&*]\")\n",
    "\n",
    "    result=p.sub(\"\",input_data)\n",
    "    \n",
    "    return result\n",
    "\n",
    "txt = \"올해 들어 서울 지역의 부동산 가격이 30% 하락했습니다!#$_$123\"\n",
    "\n",
    "print(txt)\n",
    "\n",
    "print(clean_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 처리\n",
    "\n",
    "#불용어 : 빈번하게 사용되지만 구체적인 의미를 찾기 어려운 단어들\n",
    "\n",
    "#영어의 경우 the, a, an 등의 관사는 많이 사용되지만\n",
    "\n",
    "#텍스트마이닝에서는 특별한 의미를 부여하기 힘든 경우가 많음\n",
    "\n",
    "#NLTK : 파이썬에서 많이 사용되는 텍스트마이닝 패키지\n",
    "\n",
    "# 언어별로 불용어 리스트 제공, *한국어는 지원하지 않음\n",
    "\n",
    "# 한국어 불용어 리스트를 제공하는 패키지는 아직 없으며\n",
    "\n",
    "# 직접 만들거나 다른 분석가들이 작성한 리스트를 활용해야 함\n",
    "\n",
    "words=[\"추석\",\"연휴\",\"민족\",\"대이동\",\"시작\",\"늘어\",\"교통량\",\"교통사고\",\"특히\",\"자동차\", \"고장\",\"상당수\",\"나타\",\"것\",\"기자\"]\n",
    "\n",
    "#불용어\n",
    "\n",
    "stopwords=[\"가다\",\"늘어\",\"나타\",\"것\",\"기자\"]\n",
    "\n",
    "#불용어 제거\n",
    "\n",
    "[i for i in words if i not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어 불용어 처리\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words=[\"chief\",\"justice\",\"roberts\",\",\",\"president\",\"carter\",\",\",\"president\",\"clinton\",\"president\",\"bush\",\"obama\",\"fellow\",\"americans\",\"and\",\"people\",\"of\",\"the\",\"world\",\"thank\",\"you\"]\n",
    "\n",
    "[w for w in words if not w in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\") #문장 tokenizer 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어근 동일화 처리 : 비슷한 어근 처리(stemming)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stm = PorterStemmer()\n",
    "\n",
    "txt=\"cook cooker cooking cooks cookery\"\n",
    "\n",
    "words=word_tokenize(txt)\n",
    "\n",
    "for w in words:\n",
    "\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stm=PorterStemmer()\n",
    "\n",
    "#어근이 동일한 키워드 정리\n",
    "\n",
    "txt=\"pythoning pythons Python pythoners pythoned\"\n",
    "\n",
    "words=word_tokenize(txt)\n",
    "\n",
    "for w in words:\n",
    "\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LancasterStemmer : PorterStemmer와 비슷하지만 좀더 나은 성능\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "stm=LancasterStemmer()\n",
    "\n",
    "txt=\"cook cooker cooking cooks cookery\"\n",
    "\n",
    "words=word_tokenize(txt)\n",
    "\n",
    "for w in words:\n",
    "\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stm=LancasterStemmer()\n",
    "\n",
    "txt=\"pythoning pythons Python pythoners pythoned\"\n",
    "\n",
    "words=word_tokenize(txt)\n",
    "\n",
    "for w in words:\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porter나 Lancaster가 처리하지 못하는 경우에는\n",
    "\n",
    "#RegexpStemmer를 사용하여 특정한 표현식을 일괄적으로 제거함\n",
    "\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "\n",
    "stm = RegexpStemmer('ing')\n",
    "\n",
    "print(stm.stem('cooking'))\n",
    "\n",
    "print(stm.stem('cookery'))\n",
    "\n",
    "print(stm.stem('ingleside'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stm=RegexpStemmer(\"python\")\n",
    "\n",
    "txt=\"pythoning pythons Python pythoners pythoned\"\n",
    "\n",
    "words=word_tokenize(txt)\n",
    "\n",
    "for w in words:\n",
    "\n",
    "    print(stm.stem(w),end=\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram : n번 연이어 등장하는 단어들의 연쇄\n",
    "\n",
    "# 2회 바이그램, 3회 트라이그램, 보편적으로 영어에만 적용되며 바이그램이 주로 사용됨\n",
    "\n",
    "txt = 'Hello'\n",
    "\n",
    "# 2-gram이므로 문자열의 끝에서 한 글자 앞까지만 반복함\n",
    "\n",
    "for i in range(len(txt) - 1):            \n",
    "\n",
    "    # 현재 문자와 그다음 문자 출력\n",
    "\n",
    "    print(txt[i], txt[i + 1], sep='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'this is python script'\n",
    "\n",
    "# 공백을 기준으로 문자열을 분리하여 리스트로 저장\n",
    "\n",
    "words = txt.split()                \n",
    "\n",
    "# 2-gram이므로 리스트의 마지막에서 요소 한 개 앞까지만 반복함\n",
    "\n",
    "for i in range(len(words) - 1):      \n",
    "\n",
    "    # 현재 문자열과 그다음 문자열 출력\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porter나 Lancaster가 처리하지 못하는 경우에는\n",
    "\n",
    "#RegexpStemmer를 사용하여 특정한 표현식을 일괄적으로 제거함\n",
    "\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "\n",
    "stm = RegexpStemmer('ing')\n",
    "\n",
    "print(stm.stem('cooking'))\n",
    "\n",
    "print(stm.stem('cookery'))\n",
    "\n",
    "print(stm.stem('ingleside'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stm=RegexpStemmer(\"python\")\n",
    "\n",
    "txt=\"pythoning pythons Python pythoners pythoned\"\n",
    "\n",
    "words=word_tokenize(txt)\n",
    "\n",
    "for w in words:\n",
    "\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram : n번 연이어 등장하는 단어들의 연쇄\n",
    "\n",
    "# 2회 바이그램, 3회 트라이그램, 보편적으로 영어에만 적용되며 바이그램이 주로 사용됨\n",
    "\n",
    "txt = 'Hello'\n",
    "\n",
    "# 2-gram이므로 문자열의 끝에서 한 글자 앞까지만 반복함\n",
    "\n",
    "for i in range(len(txt) - 1):            \n",
    "\n",
    "    # 현재 문자와 그다음 문자 출력\n",
    "\n",
    "    print(txt[i], txt[i + 1], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'this is python script'\n",
    "\n",
    "# 공백을 기준으로 문자열을 분리하여 리스트로 저장\n",
    "\n",
    "words = txt.split()                \n",
    "\n",
    "# 2-gram이므로 리스트의 마지막에서 요소 한 개 앞까지만 반복함\n",
    "\n",
    "for i in range(len(words) - 1):      \n",
    "\n",
    "    # 현재 문자열과 그다음 문자열 출력\n",
    "     print(words[i], words[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'hello'\n",
    "\n",
    "two_gram = zip(txt, txt[1:])\n",
    "\n",
    "for i in two_gram:\n",
    "\n",
    "    print(i[0], i[1], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'this is python script'\n",
    "\n",
    "words = txt.split()\n",
    "\n",
    "list(zip(words, words[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence=\"I love you.  Good morning.  Good bye.\"\n",
    "\n",
    "grams=ngrams(sentence.split(),2)\n",
    "\n",
    "for gram in grams:\n",
    "\n",
    "    print(gram,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"I love you.  Good morning.  Good bye.\"\n",
    "\n",
    "grams=ngrams(sentence.split(),3)\n",
    "\n",
    "for gram in grams:\n",
    "\n",
    "    print(gram,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한국어 처리\n",
    "\n",
    "#헌법 말뭉치\n",
    "from konlpy.corpus import kolaw\n",
    "\n",
    "#말뭉치에 포함된 파일 목록\n",
    "\n",
    "print(kolaw.fileids())\n",
    "\n",
    "c = kolaw.open('constitution.txt').read()\n",
    "print(c[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#국회의안 말뭉치\n",
    "\n",
    "from konlpy.corpus import kobill\n",
    "\n",
    "print(kobill.fileids())\n",
    "\n",
    "d = kobill.open('1809890.txt').read()\n",
    "\n",
    "print(d[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사 분석\n",
    "\n",
    "# POS 태깅(Part-Of-Speech)\n",
    "\n",
    "# 모든 언어에 명사,동사,형용사,부사는 공통적으로 존재함\n",
    "\n",
    "#한나눔 패키지 - KAIST Semantic Web Research Center 개발\n",
    "\n",
    "#    http://semanticweb.kaist.ac.kr/hannanum/\n",
    "\n",
    "#pip install konlpy\n",
    "\n",
    "#pip install tweepy==3.10.0\n",
    "\n",
    "#pip install jpype1==1.0.2\n",
    "\n",
    "#jdk8 버전 설치(java 최신버전은 잘 호환되지 않을 수 있음)\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "han=Hannanum()\n",
    "\n",
    "txt=\"\"\"원/달러 환율이 3년 5개월 만에 최고치로 마감하고,\n",
    "\n",
    "위안화 환율이 11년 만에 달러당 7위안을 넘었다.\n",
    "\n",
    "원/엔 재정환율 역시 100엔당 30원 가까이 뛰었다.\"\"\"\n",
    "\n",
    "#형태소 분석\n",
    "\n",
    "print(han.morphs(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(han.nouns(txt)) #명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#형태소와 품사\n",
    "\n",
    "print(han.pos(txt))\n",
    "print(han.tagset) #품사 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#꼬꼬마 패키지 - 서울대학교 IDS(Intelligent Data Systems) 연구실 개발\n",
    "\n",
    "#    http://kkma.snu.ac.kr/\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkm=Kkma()\n",
    "\n",
    "print(kkm.morphs(txt)) #형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kkm.nouns(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kkm.pos(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kkm.tagset) #품사 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트위터 패키지 - 트위터에서 개발, 2017년 이후 오픈코리안텍스트로 이름이 바뀜\n",
    "\n",
    "#  https://github.com/twitter\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt=Okt()\n",
    "\n",
    "print(okt.morphs(txt)) #형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt.nouns(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt.pos(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okt.tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어구 추출\n",
    "\n",
    "print(okt.phrases(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 출현 빈도\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "from nltk import Text\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "kolaw = Text(okt.nouns(c), name=\"kolaw\")  # 뭐하는것?\n",
    "kolaw.plot(30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러가 발생할 경우 \n",
    "\n",
    "# visual studio 재배포 패키지 설치 후 (vc_redist_x64.exe)\n",
    "\n",
    "# https://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud\n",
    "\n",
    "# python 버전에 맞는 whl 파일 다운로드\n",
    "\n",
    "# 로컬디렉토리에 있는 파일로 설치\n",
    "\n",
    "# pip install wordcloud-버전-cp39-cp39-win_amd64.whl\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "font_path = 'c:/windows/fonts/malgun.ttf'\n",
    "\n",
    "wc = WordCloud(width = 1000, height = 600, background_color=\"white\", font_path=font_path)\n",
    "\n",
    "plt.imshow(wc.generate_from_frequencies(kolaw.vocab()))\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어 품사 분석\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "a=\"I love you.\"\n",
    "\n",
    "tags=pos_tag(a.split())\n",
    "\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=(20,16)\n",
    "\n",
    "plt.rcParams[\"font.size\"]=15\n",
    "\n",
    "res=urllib.request.urlopen('http://python.org/')\n",
    "\n",
    "html=res.read()\n",
    "\n",
    "#알파벳,숫자,_ 문자들만 선택\n",
    "\n",
    "tokens=re.split('\\W+',html.decode('utf-8'))\n",
    "\n",
    "clean=BeautifulSoup(html,'html.parser').get_text()  \n",
    "\n",
    "tokens=[token for token in clean.split()]\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "clean_tokens= [token for token in tokens\n",
    "\n",
    "               if len(token.lower())>1 and (token.lower() not in stop)]\n",
    "\n",
    "tagged=nltk.pos_tag(clean_tokens)\n",
    "\n",
    "#보통명사, 고유명사만 추출\n",
    "\n",
    "allnoun=[word for word,pos in tagged if pos in ['NN','NNP']]\n",
    "\n",
    "freq_result = nltk.FreqDist(allnoun)\n",
    "\n",
    "freq_result.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) NLTK 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 말뭉치(corpus) : 자연어 분석 작업을 위해 만든 문서 집합\n",
    "\n",
    "import nltk\n",
    "\n",
    "# NLTK 패키지에서 제공하는 샘플 말뭉치 다운로드, 시간이 많이 걸림\n",
    "\n",
    "nltk.download(\"book\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "#저작권이 만료된 문학작품이 포함된 말뭉치\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제인 오스틴의 엠마 문서\n",
    "\n",
    "emma_raw = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
    "\n",
    "print(emma_raw[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 문서를 분석하기 위해서는 우선 긴 문자열을 분석을 위한 작은 단위로 나누어야 한다.\n",
    "\n",
    "# 이 문자열 단위를 토큰(token)이라고 하고\n",
    "\n",
    "# 이렇게 문자열을 토큰으로 나누는 작업을 토큰 생성(tokenizing)이라고 함\n",
    "\n",
    "# 영문의 경우에는 문장, 단어 등을 토큰으로 사용하거나 정규 표현식을 쓸 수 있다.\n",
    "\n",
    "# 문자열을 토큰으로 분리하는 함수를 토큰 생성 함수(tokenizer)라고 한다.\n",
    "\n",
    "# 토큰 생성 함수는 문자열을 입력받아 토큰 문자열의 리스트를 출력한다.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(sent_tokenize(emma_raw[:1000])[3]) #3번 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(emma_raw[50:100]) #50~99 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 : 일정한 의미가 있는 가장 작은 말의 단위\n",
    "\n",
    "# 형태소 분석(morphological analysis) :\n",
    "\n",
    "#   단어로부터 어근, 접두사, 접미사, 품사 등 다양한 언어적 속성을 파악하고\n",
    "\n",
    "#   이를 이용하여 형태소를 찾아내거나 처리하는 작업\n",
    "\n",
    "# 어간 추출(stemming), 원형 복원(lemmatizing), 품사 부착(Part-Of-Speech tagging)\n",
    "\n",
    "# 어간 추출(stemming) : 단어의 접미사나 어미를 제거\n",
    "\n",
    "# 어간 추출법은 단순히 어미를 제거할 뿐이므로 단어의 원형의 정확히 찾아주지는 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "st1 = PorterStemmer()\n",
    "\n",
    "st2 =  LancasterStemmer()\n",
    "\n",
    "words = [\"fly\", \"flies\", \"flying\", \"flew\", \"flown\"]\n",
    "\n",
    "print( [st1.stem(w) for w in words] )\n",
    "\n",
    "print( [st2.stem(w) for w in words] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원형 복원 : 같은 의미를 가지는 여러 단어를 사전형으로 통일하는 작업\n",
    "\n",
    "#품사를 지정하는 경우 좀 더 정확한 원형을 찾을 수 있다.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "words = [\"fly\", \"flies\", \"flying\", \"flew\", \"flown\"]\n",
    "\n",
    "#동사원형\n",
    "[lm.lemmatize(w, pos=\"v\") for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사의 예\n",
    "\n",
    "# NNP: 단수 고유명사\n",
    "\n",
    "# VB: 동사\n",
    "\n",
    "# VBP: 동사 현재형\n",
    "\n",
    "# NN: 명사\n",
    "\n",
    "#품사 목록\n",
    "\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사에 대한 설명\n",
    "\n",
    "nltk.help.upenn_tagset(\"VB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 단어의 사용 빈도를 그래프로 출력\n",
    "\n",
    "from nltk import Text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "retokenize = RegexpTokenizer(\"[\\w]+\") #특수문자 제거\n",
    "\n",
    "#retokenize.tokenize(emma_raw[50:100])\n",
    "\n",
    "text = Text(retokenize.tokenize(emma_raw))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=(15,10)\n",
    "\n",
    "plt.rcParams[\"font.size\"]=15\n",
    "\n",
    "text.plot(20) #상위 20개의 단어 출력\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어가 사용된 위치를 시각화\n",
    "\n",
    "#소설 엠마의 각 등장인물에 대해 적용\n",
    "text.dispersion_plot([\"Emma\", \"Knightley\", \"Frank\", \"Jane\", \"Harriet\", \"Robert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어가 사용된 위치를 표시\n",
    "\n",
    "#해당 단어의 앞뒤에 사용된 단어\n",
    "\n",
    "text.concordance(\"Emma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#같은 문맥에서 주어진 단어 대신 사용된 횟수가 높은 단어들\n",
    "\n",
    "text.similar(\"Emma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#두 단어가 공통적으로 들어갈 수 있는 문맥 확인\n",
    "\n",
    "text.common_contexts(['Emma','she'])\n",
    "\n",
    "# 밑줄 부분에 Emma와 she가 들어갈 경우 같은 의미가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "stopwords = [\"Mr.\", \"Mrs.\", \"Miss\", \"Mr\", \"Mrs\", \"Dear\"]\n",
    "\n",
    "emma_tokens = pos_tag(retokenize.tokenize(emma_raw))\n",
    "\n",
    "# NNP(고유대명사)이면서 필요없는 단어(stop words)는 제거\n",
    "\n",
    "names_list = [t[0] for t in emma_tokens if t[1] == \"NNP\" and t[0] not in stopwords]\n",
    "\n",
    "#FreqDist : 문서에 사용된 단어(토큰)의 사용빈도 정보를 담는 클래스\n",
    "\n",
    "#Emma 말뭉치에서 사람의 이름만 모아서 FreqDist 클래스 객체 생성\n",
    "\n",
    "fd_names = FreqDist(names_list)\n",
    "\n",
    "#전체 단어의 수, \"Emma\"라는 단어의 출현 횟수, 확률\n",
    "\n",
    "fd_names.N(), fd_names[\"Emma\"], fd_names.freq(\"Emma\")\n",
    "\n",
    "#most_common 메서드를 사용하면 가장 출현 횟수가 높은 단어를 찾는다.\n",
    "\n",
    "fd_names.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width=1000, height=600, background_color=\"white\", random_state=0)\n",
    "\n",
    "plt.imshow(wc.generate_from_frequencies(fd_names))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 단어빈도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어빈도분석 : 전체 문서 또는 문서별 단어의 출현 빈도\n",
    "\n",
    "f = open('./DATA/warandpeace.txt',encoding='utf-8')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(lines[1][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#숫자,특수문자 제거\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "#불용어 사전\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#모든 단어를 소문자로 변환\n",
    "words=''\n",
    "for line in lines:\n",
    "    words +=  line.lower()\n",
    "#print(words)\n",
    "\n",
    "\n",
    "#단어 단위로 토큰화\n",
    "tokens = tokenizer.tokenize(words)\n",
    "\n",
    "#불용어 제거\n",
    "tokens2 = [i for i in list(tokens) if not i in stop_words]\n",
    "\n",
    "#글자수 1인 단어들 제거\n",
    "tokens3= [i for i in tokens2 if len(i)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#출현 빈도가 높은 단어들\n",
    "\n",
    "pd.Series(tokens3).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#워드 클라우드\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#폰트 경로, 워드클라우드의 가로,세로 사이즈\n",
    "wordcloud = WordCloud(\n",
    "    font_path = 'c:/windows/fonts/malgun.ttf',\n",
    "    width = 800,\n",
    "    height = 800,\n",
    "    background_color=\"white\"\n",
    ")\n",
    "\n",
    "#단어별 출현횟수 저장\n",
    "\n",
    "count = Counter(tokens3)\n",
    "\n",
    "print(count)\n",
    "\n",
    "wordcloud = wordcloud.generate_from_frequencies(count)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뉴스 기사를 하나 찾아서 news1.txt로 저장\n",
    "\n",
    "f = open(\"./DATA/news1.txt\", encoding='utf8')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한나눔 형태소 분석기\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "han = Hannanum()\n",
    "\n",
    "temp = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "\n",
    "    #명사만 추출\n",
    "\n",
    "    a=lines[i].strip()\n",
    "\n",
    "    temp.append(han.nouns(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2차원 리스트를 1차원 리스트로 변환하는 함수\n",
    "\n",
    "def flatten(items):\n",
    "\n",
    "    flatList = []\n",
    "\n",
    "    for elem in items:\n",
    "\n",
    "        if type(elem) == list:\n",
    "\n",
    "            for e in elem:\n",
    "\n",
    "                flatList.append(e)\n",
    "\n",
    "        else:\n",
    "\n",
    "            flatList.append(elem)\n",
    "\n",
    "    return flatList\n",
    "\n",
    "word_list=flatten(temp)\n",
    "\n",
    "# 두글자 이상인 단어만 추출\n",
    "\n",
    "word_list=pd.Series([x for x in word_list if len(x)>1])\n",
    "\n",
    "#단어별 출현 빈도\n",
    "\n",
    "word_list.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 처리\n",
    "\n",
    "stopwords=['전날','오전']\n",
    "\n",
    "word_list2=[i for i in word_list if i not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 뉴스를 워드클라우드로 출력\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud=WordCloud( font_path='c:/windows/fonts/malgun.ttf', width=800,height=800,background_color='white')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count=Counter(word_list2)\n",
    "\n",
    "wordcloud=wordcloud.generate_from_frequencies(count)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 정수인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트를 숫자로 바꾸는 기법\n",
    "\n",
    "text = '''모처럼 전국에 비가 내리고 있습니다.\n",
    "\n",
    "대부분 밤까지 계속되기 때문에 종일 우산이 필요하겠는데요.\n",
    "\n",
    "비의 양도 많고 바람도 강하게 불기 때문에 작은 우산 말고 큰 우산 챙기는 게 더 좋습니다.\n",
    "\n",
    "특히 제주와 남해안에서 비바람이 강합니다.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 문장 토큰화\n",
    "\n",
    "text = sent_tokenize(text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#명사만 추출하는 방법\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt=Okt()\n",
    "\n",
    "text2=[]\n",
    "\n",
    "for txt in text:\n",
    "\n",
    "    t=okt.nouns(txt)\n",
    "\n",
    "    text2.append(t)\n",
    "\n",
    "    \n",
    "\n",
    "text2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사와 형용사를 추출하는 방법\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt=Okt()\n",
    "\n",
    "text2 = []\n",
    "\n",
    "for txt in text:\n",
    "\n",
    "    morph = okt.pos(txt)\n",
    "\n",
    "    text2.append(morph)\n",
    "    \n",
    "text3 = []\n",
    "\n",
    "for text in text2:\n",
    "\n",
    "    line=[]\n",
    "\n",
    "    for word, tag in text:\n",
    "\n",
    "        if tag in ['Noun','Adjective']:\n",
    "\n",
    "            line.append(word)\n",
    "\n",
    "    text3.append(line)\n",
    "\n",
    "print(text3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "\n",
    "sentences = []\n",
    "\n",
    "stop_words = ['더', '게']\n",
    "\n",
    "for txt in text3:\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for word in txt:\n",
    "\n",
    "        # 불용어 제거\n",
    "\n",
    "        if word not in stop_words: #불용어가 아니면\n",
    "\n",
    "            result.append(word)\n",
    "\n",
    "            if word not in vocab: #새로운 단어이면\n",
    "\n",
    "                vocab[word] = 0 # 출현횟수 0으로\n",
    "\n",
    "            vocab[word] += 1 #출현횟수 증가\n",
    "\n",
    "    sentences.append(result)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어:출현빈도\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[\"우산\"]) # 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어에 일련번호 부여\n",
    "\n",
    "word_to_index = {}\n",
    "\n",
    "i=0\n",
    "\n",
    "for word in vocab :\n",
    "\n",
    "    if vocab[word] > 1 : # 빈도수가 1보다 큰 단어들만 추가\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "        word_to_index[word] = i #단어에 번호를 매김\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out-Of-Vocabulary 단어 집합에 없는 단어\n",
    "\n",
    "#출현빈도수가 낮은 단어들은 word_to_index에 없으므로\n",
    "\n",
    "# word_to_index에 OOV라는 단어를 추가하고 단어 집합에 없는 단어들은 OOV로 처리\n",
    "\n",
    "word_to_index['OOV'] = len(word_to_index) + 1\n",
    "\n",
    "encoded = []\n",
    "\n",
    "for s in sentences: #문장들을 반복\n",
    "\n",
    "    temp = []\n",
    "\n",
    "    for w in s: #문장의 단어들을 반복\n",
    "\n",
    "        try:\n",
    "\n",
    "            #단어의 고유번호를 리스트에 추가\n",
    "\n",
    "            temp.append(word_to_index[w])\n",
    "\n",
    "        except:\n",
    "\n",
    "            #존재하지 않는 단어는 OOV의 인덱스를 추가\n",
    "\n",
    "            temp.append(word_to_index['OOV'])\n",
    "\n",
    "    encoded.append(temp)\n",
    "\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#2차원 데이터를 1차원으로 바꾸고\n",
    "\n",
    "words = np.hstack(sentences)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter(words) # 단어의 출현빈도를 쉽게 계산하는 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[\"우산\"]) # 단어의 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "\n",
    "# 출현빈도가 높은 상위 5개의 단어\n",
    "\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "for (word, frequency) in vocab :\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "    word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정수인코딩의 단점: 단어의 순서가 없음\n",
    "#원핫인코딩 : 단어의 순서에 맞게 배열\n",
    "\n",
    "from konlpy.tag import Okt  \n",
    "\n",
    "okt=Okt()  \n",
    "\n",
    "#토근화(형태소 분석)\n",
    "\n",
    "token=okt.morphs(\"나는 학교에 간다 나는 집에 간다\")  \n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#중복된 단어는 제외하고 단어를 key로 고유한 숫자 인덱스 부여\n",
    "\n",
    "word2index={}\n",
    "\n",
    "for idx,voca in enumerate(token):\n",
    "\n",
    "    if voca not in word2index.keys():\n",
    "\n",
    "        word2index[voca]=len(word2index)\n",
    "\n",
    "print(word2index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원핫인코딩 함수\n",
    "\n",
    "def one_hot_encoding(word, word2index):\n",
    "\n",
    "    #전체 단어 갯수만큼 0으로 채운 리스트\n",
    "\n",
    "    one_hot_vector = [0]*(len(word2index))\n",
    "\n",
    "    #해당하는 단어의 인덱스를 찾아서\n",
    "\n",
    "    index=word2index[word]\n",
    "\n",
    "    #1로 설정(나머지는 0)\n",
    "\n",
    "    one_hot_vector[index]=1\n",
    "\n",
    "    return one_hot_vector\n",
    "\n",
    "key_list=word2index.keys()\n",
    "\n",
    "print(key_list)\n",
    "\n",
    "for key in key_list:\n",
    "\n",
    "    print(key, one_hot_encoding(key,word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#케라스에서 지원하는 원핫인코딩 함수\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text=\"나는 학교에 간다 나는 집에 간다\"\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "# 각 단어에 대한 정수 인코딩\n",
    "\n",
    "t.fit_on_texts([text])\n",
    "\n",
    "print(t.word_index)\n",
    "\n",
    "#각 단어에 매핑된 숫자로 변환된 리스트\n",
    "\n",
    "sub_text=\"나는 집에 간다\"\n",
    "# 사전에 없는 단어 추가\n",
    "sub_text=\"나는 집에 간다 즐\"\n",
    "\n",
    "encoded=t.texts_to_sequences([sub_text])[0]\n",
    "\n",
    "print('encoded: ', encoded)\n",
    "\n",
    "#원핫인코딩\n",
    "\n",
    "one_hot = to_categorical(encoded)\n",
    "\n",
    "print('one hot: ', one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원핫인코딩의 단점:\n",
    "\n",
    "# 단어 갯수가 많아지면 변수의 갯수가 많아지게 됨\n",
    "\n",
    "# 메모리 활용의 비효율성: 변수가 100개 있다면 99개의 0과 1개의 0으로 구성됨\n",
    "\n",
    "# 비슷한 단어들의 유사성을 표현하기 어려움\n",
    "\n",
    "#   강아지 [0,1,1]와 개 [1,0,0] 이라면 비슷한 단어이지만 유사성을 찾기 어려움\n",
    "\n",
    "# 유사성을 찾기 위한 방법으로 LSA, RNN, Word2Vec 등의 방법이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) BOW(bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words(BoW) : 단어의 등장 순서를 고려하지 않는 빈도수 기반의 텍스트 데이터의 수치화 방법\n",
    "\n",
    "# 가방에 단어들을 넣으면 순서가 중요하지 않음\n",
    "\n",
    "# 각 단어에 고유한 인덱스를 부여하고\n",
    "\n",
    "# 각 인덱스의 위치에 단어의 출현 횟수를 저장\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import re  \n",
    "\n",
    "okt=Okt()  \n",
    "\n",
    "token=re.sub(\"[.!#~]\",\"\",\n",
    "\n",
    "             '비가 오니 마음이 차분해지네요. 요즘 너무 더웠어요. 비가 오니 마음이 기쁘네요.')\n",
    "\n",
    "#형태소 분석\n",
    "\n",
    "token=okt.morphs(token)  \n",
    "print(\"token: \",token)\n",
    "\n",
    "word2index={}  #단어 사전(단어와 숫자 인덱스)\n",
    "\n",
    "bow=[]  #단어 가방(단어와 출현 횟수)\n",
    "\n",
    "for voca in token:  \n",
    "\n",
    "    #사전에 없는 단어 추가\n",
    "\n",
    "    if voca not in word2index.keys():  \n",
    "\n",
    "        word2index[voca]=len(word2index)  # 라벨 인코딩\n",
    "\n",
    "        #단어의 인덱스와 출현횟수(기본값:1)\n",
    "\n",
    "        bow.insert(len(word2index)-1,1) # 처음나왔으면 1 지정\n",
    "\n",
    "    else:\n",
    "\n",
    "        #재등장하는 단어의 인덱스\n",
    "\n",
    "        index=word2index.get(voca)\n",
    "\n",
    "        #단어 카운트 증가\n",
    "        bow[index]=bow[index]+1\n",
    "    \n",
    "print(word2index)\n",
    "#단어의 출현 횟수\n",
    "\n",
    "print(bow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩한 벡터를 만드는 클래스\n",
    "\n",
    "corpus=['모처럼 전국에 비가 내리고 있습니다.']\n",
    "\n",
    "line=['전국에 비가']\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "vector.fit(corpus)\n",
    "\n",
    "print(vector.vocabulary_)\n",
    "\n",
    "print(vector.transform(line).toarray()) #단어가 출현한 위치에 1 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['''모처럼 전국에 비가 내리고 있습니다.\n",
    "\n",
    "대부분 밤까지 계속되기 때문에 종일 우산이 필요하겠는데요.\n",
    "\n",
    "비의 양도 많고 바람도 강하게 불기 때문에 작은 우산 말고 큰 우산 챙기는 게 더 좋습니다. ''']\n",
    "\n",
    "#문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩한 벡터를 만드는 클래스\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수 계산\n",
    "\n",
    "#단어들의 출현 횟수\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())  \n",
    "\n",
    "# 만들어진 단어와 인덱스\n",
    "\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATA/news1.txt', encoding='utf8') as f:\n",
    "\n",
    "    corpus=f.read()\n",
    "\n",
    "txt=okt.nouns(corpus)\n",
    "\n",
    "txt2=[' '.join(txt)]\n",
    "\n",
    "txt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vect = CountVectorizer().fit(txt2)\n",
    "\n",
    "#bow의 출현횟수 합계\n",
    "\n",
    "cnt = vect.transform(txt2).toarray().sum(axis=0)\n",
    "\n",
    "idx = np.argsort(-cnt) #카운트 내림차순 정렬\n",
    "\n",
    "cnt = cnt[idx]\n",
    "\n",
    "# x축의 단어이름\n",
    "\n",
    "feature_name = np.array(vect.get_feature_names_out())[idx]\n",
    "\n",
    "plt.bar(range(len(cnt)), cnt)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(list(zip(feature_name,cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 추가\n",
    "\n",
    "vector = CountVectorizer(stop_words=['더'])\n",
    "\n",
    "print(vector.fit_transform(txt2).toarray())\n",
    "\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF: term frequency 문장 안에서, 출현빈도\n",
    "# df: document frequency 몇개 문장에서 출현하는지, 모든 문장마다 나오면 커짐\n",
    "# idf: df의 역수, 모든 문장마다 나오면 커지고 이것의 역수를 하면 작아짐\n",
    "# TF와 idf를 이용해서 문장에서는 자주 나오되, 모든 문장마다 나오면 중요도를 감소시킴\n",
    "# 출현빈도가 높다고해서 무조건 중요한건 아니기 때문에 TF와 IDF를 조합해서 사용\n",
    "\n",
    "docs = [\n",
    "\n",
    "  'python 데이터 python 프로그래밍',\n",
    "\n",
    "  '데이터 분석',\n",
    "\n",
    "  '빅 데이터 분석',\n",
    "\n",
    "  '데이터 전처리 텍스트 전처리'\n",
    "\n",
    "]\n",
    "\n",
    "vocab=list() #단어사전 리스트\n",
    "\n",
    "for doc in docs: #문서\n",
    "\n",
    "    for w in doc.split(): #단어\n",
    "\n",
    "        vocab.append(w) #단어 추가\n",
    "\n",
    "        \n",
    "\n",
    "vocab=list(set(vocab)) #중복 단어를 제거한 리스트\n",
    "\n",
    "vocab.sort() #오름차순 정렬\n",
    "\n",
    "print(vocab)\n",
    "#print(vocab.count)\n",
    "\n",
    "N = len(docs) # 총 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "\n",
    "# tf(Term Frequency) : 단어의 빈도\n",
    "# 특정 문서 d에서 특정 단어 t의 출현빈도\n",
    "def tf(t, d):\n",
    "\n",
    "    return d.count(t)\n",
    "\n",
    "\n",
    "\n",
    "# df(Document Frequency) : 특정 단어가 등장한 문서의 수, 모든 문서에 자주 나오는 단어는 중요한 단어가 아닐 수 있음\n",
    "#   논문에서 abstract, 서론, 본론, 결론, 참고문헌 항상 나오는 단어들 - 중요한 의미가 없음\n",
    "# idf(Inverse Document Frequency) : df의 반비례하는 수\n",
    "#   log( n / (1+df(t)))\n",
    "#   희귀 단어의 경우 분모가 매우 작아지게 되어 idf 값이 너무 커지게 되므로 로그를 취함\n",
    "#   한번도 등장하지 않는 단어라면 분모가 0이 되므로 1을 더하게 됨\n",
    "\n",
    "def idf(t):\n",
    "\n",
    "    df = 0\n",
    "\n",
    "    for doc in docs:\n",
    "\n",
    "        df += t in doc   # 문장에서 사전에 있는 단어가 \n",
    "\n",
    "    return log(N/(df + 1))\n",
    "\n",
    "#tf와 idf를 곱한 값\n",
    "\n",
    "def tfidf(t, d):\n",
    "\n",
    "    return tf(t,d)* idf(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N): #문서 반복(문장)\n",
    "\n",
    "    d = docs[i]\n",
    "    print(d)\n",
    "\n",
    "print(\"-\"*50)\n",
    "for j in range(len(vocab)): #단어 반복\n",
    "\n",
    "    t = vocab[j]\n",
    "    print(t)\n",
    "\n",
    "    df = 0\n",
    "\n",
    "t = vocab[1]\n",
    "for doc in docs:\n",
    "\n",
    "    print(t in doc)\n",
    "    df += t in doc\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. 문장별로 사전에 있는 단어가 출현한 빈도 계산\n",
    "import pandas as pd\n",
    "\n",
    "result = []\n",
    "\n",
    "N = len(docs) # 총 문서의 수\n",
    "\n",
    "for i in range(N): #문서 반복(문장)\n",
    "\n",
    "    result.append([])\n",
    "\n",
    "    d = docs[i] # 한 문장\n",
    "\n",
    "    for j in range(len(vocab)): #단어 반복\n",
    "\n",
    "        t = vocab[j]\n",
    "\n",
    "        #1차원 배열의 끝에 추가\n",
    "\n",
    "        result[-1].append(tf(t, d))  # 문장에서 \"사전의 단어\" 빈도 계산\n",
    "\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.DataFrame(result, columns = vocab)\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 단어의 idf 계산\n",
    "# 몇개의 문장에 출현하는지\n",
    "\n",
    "result = []\n",
    "\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "\n",
    "    result.append(idf(t))\n",
    "\n",
    "df_idf = pd.DataFrame(result, index = vocab, columns = [\"IDF\"])\n",
    "\n",
    "df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 단어들의 tfidf 계산 - tfidf가 크면 중요도가 높고, 작으면 중요도가 낮음\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(N):\n",
    "\n",
    "    result.append([])\n",
    "\n",
    "    d = docs[i]\n",
    "\n",
    "    for j in range(len(vocab)):\n",
    "\n",
    "        t = vocab[j]\n",
    "\n",
    "        result[-1].append(tfidf(t,d))\n",
    "\n",
    "df_tfidf = pd.DataFrame(result, columns = vocab)\n",
    "\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "\n",
    "    'you know I want your love',\n",
    "\n",
    "    'I like you',\n",
    "\n",
    "    'what should I do ',    \n",
    "\n",
    "]\n",
    "\n",
    "#DTM(Document Term Matrix, 문서 단어 행렬)\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수 계산\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "\n",
    "print(tfidfv.vocabulary_)\n",
    "\n",
    "#사이킷런의 TF-IDF 계산 방식이 약간 달라서 차이가 나지만 일반적으로 많이 사용하는 함수\n",
    "\n",
    "#  tf-idf에 L2 정규화를 적용하여 값을 조정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 코사인 유사도\n",
    "\n",
    "* 각도가 유사할수록 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#유클리드 거리\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def dist(x,y):  \n",
    "\n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "doc0 = np.array((1,1,0,1))\n",
    "\n",
    "doc1 = np.array((2,3,0,1))\n",
    "\n",
    "doc2 = np.array((1,2,3,1))\n",
    "\n",
    "print(dist(doc0,doc1)) #doc0과 doc1의 거리\n",
    "\n",
    "print(dist(doc0,doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#자카드 유사도: 두 문서의 총 단어 집합에서 공통적으로 출현한 단어의 비율\n",
    "\n",
    "doc1 = \"python 파이썬 데이터\"\n",
    "\n",
    "doc2 = \"빅데이터 python 파이썬\"\n",
    "\n",
    "# 토큰화\n",
    "\n",
    "tokenized_doc1 = doc1.split()\n",
    "\n",
    "tokenized_doc2 = doc2.split()\n",
    "\n",
    "print(tokenized_doc1)\n",
    "\n",
    "print(tokenized_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합집합\n",
    "\n",
    "union = set(tokenized_doc1).union(set(tokenized_doc2))\n",
    "\n",
    "print(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#교집합\n",
    "\n",
    "intersection = set(tokenized_doc1).intersection(set(tokenized_doc2))\n",
    "\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(intersection)/len(union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "# 두 벡터 간의 코사인 각도를 이용하여 구하는 두 벡터의 유사도\n",
    "\n",
    "# 두 벡터의 방향이 완전히 같으면 1, 90도이면 0, 반대 방향이면 -1\n",
    "\n",
    "# 1에 가까울수록 유사도가 높다고 판단함\n",
    "# dot: 행렬곱셈(내적의 경우 포함)\n",
    "# inner: 내적만\n",
    "\n",
    "\n",
    "from numpy import dot\n",
    "\n",
    "a=[0,1,1]\n",
    "\n",
    "b=[1,0,2]\n",
    "\n",
    "# 배열의 곱( 0x1 + 1x0 + 1x2)\n",
    "\n",
    "dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "a=[0,1,1]\n",
    "\n",
    "b=[1,0,2]\n",
    "\n",
    "print(norm(a)) #a의 제곱합의 제곱근\n",
    "\n",
    "print(sqrt(2))\n",
    "\n",
    "print(norm(b))\n",
    "\n",
    "print(sqrt(5))\n",
    "\n",
    "print(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a,b의 코사인 유사도\n",
    "\n",
    "# 내적을 크기로 나눠줌\n",
    "print( dot(a,b) / (norm(a)*norm(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A, B):\n",
    "\n",
    "    return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc1=np.array([0,1,1,1])\n",
    "\n",
    "doc2=np.array([1,0,1,1])\n",
    "\n",
    "doc3=np.array([2,0,2,3])\n",
    "\n",
    "doc4=np.array([0,2,2,2])\n",
    "\n",
    "print(cos_sim(doc1, doc2)) #문서1과 문서2의 코사인 유사도\n",
    "\n",
    "print(cos_sim(doc1, doc3)) #문서1과 문서3의 코사인 유사도\n",
    "\n",
    "print(cos_sim(doc2, doc3)) #문서2과 문서3의 코사인 유사도\n",
    "\n",
    "print(cos_sim(doc1, doc4)) #문서1과 문서4의 코사인 유사도\n",
    "\n",
    "#코사인 유사도는 단순한 빈도수보다도 두 벡터의 방향이 완전히 동일한 경우에는 1(유사도가 최대)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전구축 및 출현빈도수로 정수 인코딩\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "\n",
    " '매우 좋은 영화네요 매우 추천해요',\n",
    "\n",
    " '매우 볼만한 영화네요.',\n",
    "\n",
    " '조금 볼만한 영화네요 조금 추천해요',\n",
    "\n",
    " '별로 볼 내용이 없는 것 같아요 추천하지 않아요',\n",
    "\n",
    "]\n",
    "\n",
    "#DTM(Document Term Matrix, 문서 단어 행렬)\n",
    "vector = CountVectorizer() # 분리하되, 빈도수 적용\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수 계산\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF로 인코딩\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)  \n",
    "\n",
    "tfidf_matrix = tfidfv.fit_transform(corpus)  \n",
    "\n",
    "print(tfidfv)  \n",
    "print(\"=\"*30)\n",
    "print(tfidfv.transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 매트릭스 4*4\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel  \n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)  \n",
    "\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#줄거리가 유사한 영화 추천\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#https://www.kaggle.com/rounakbanik/the-movies-dataset\n",
    "\n",
    "df = pd.read_csv('./DATA/movies_metadata.csv',low_memory=False)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# 일부 컬럼에 자료형이 혼합된 경우 메모리 사용량이 증가할 수 있으므로 low_memory=False 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.head(10000) #1만개의 행으로 실습\n",
    "\n",
    "# overview(줄거리) 필드의 결측값이 있는 행의 수\n",
    "\n",
    "df['overview'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측값을 빈값으로 채움\n",
    "\n",
    "df['overview'] = df['overview'].fillna('')\n",
    "\n",
    "df['overview'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어가 3만개, 단어빈도출현으로하면 0값이 너무 많이 들어가 메모리 낭비\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# overview에 대해서 tf-idf 수행\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(df['overview'])\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "#단어 개수 32350\n",
    "for idx,value in enumerate(tfidf_matrix[0].toarray()[0]):  \n",
    "\n",
    "    if value>0:\n",
    "\n",
    "        print(idx, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "#tfidf에서는 dot product를 구하면 코사인 유사도를 얻을 수 있음\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "linear_kernel([[1,2]],[[1,2]]) # 1x1 + 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영화 제목과 인덱스, drop_duplicates() 중복값 제거\n",
    "\n",
    "indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "\n",
    "print(indices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영화제목을 입력하면 인덱스가 리턴됨\n",
    "\n",
    "idx = indices['Toy Story']\n",
    "\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "\n",
    "    # 영화의 제목으로 인덱스 조회\n",
    "\n",
    "    idx = indices[title]\n",
    "\n",
    "    # 해당 영화와의 유사도 계산\n",
    "\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    \n",
    "\n",
    "    # 유사도에 따라 정렬, key 정렬기준 필드(두번째값 기준 정렬), reverse 내림차순\n",
    "\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 가장 유사한 10개의 영화 리스트\n",
    "\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    print(sim_scores)\n",
    "\n",
    "    # 리스트의 0번 인덱스\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # 가장 유사한 10개의 영화의 제목\n",
    "\n",
    "    return df['title'].iloc[movie_indices]\n",
    "\n",
    "#가장 유사한 영화 목록\n",
    "\n",
    "get_recommendations('Toy Story')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 추천시스템(컨텐트 기반 필터링)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#컨텐트 기반 필터링(Content based filtering) :\n",
    "\n",
    "# 장르를 기준으로 유사한 영화를 추천\n",
    "\n",
    "#  사용자가 특정 아이템을 선호하는 경우 그 아이템과 비슷한 컨텐츠를 가진 다른 아이템을 추천하는 방식\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./DATA/tmdb_5000_movies.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genres : 영화 장르\n",
    "\n",
    "# keywords : 영화의 키워드\n",
    "\n",
    "# original_language : 영화 언어\n",
    "\n",
    "# title : 제목\n",
    "\n",
    "# vote_average : 평점 평균\n",
    "\n",
    "# vote_count : 평점 카운트\n",
    "\n",
    "# popularity : 인기도\n",
    "\n",
    "# overview : 줄거리\n",
    "\n",
    "#필요한 필드만 선택\n",
    "\n",
    "df = df[['id','genres', 'vote_average', 'vote_count','popularity','title',  \n",
    "\n",
    "             'keywords', 'overview']]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['genres', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = \"{'a': 3, 'b': 5}\"\n",
    "\n",
    "print(type(dict1)) #스트링\n",
    "\n",
    "#print(dict1['a']) #스트링이므로 에러가 발생함\n",
    "\n",
    "#print(dict1['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2 = eval(dict1) #자료형을 스트링에서 딕셔너리로 변환\n",
    "\n",
    "print(type(dict2))  \n",
    "\n",
    "print(dict2['a'])\n",
    "\n",
    "print(dict2['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스트링을 딕셔너리 타입으로 변경\n",
    "\n",
    "# eval('문자열') - 문자열 내에 포함된 파이썬 명령어를 실행하는 함수\n",
    "\n",
    "df['genres'] = df['genres'].apply(eval)\n",
    "\n",
    "df['keywords'] = df['keywords'].apply(eval)\n",
    "\n",
    "#딕셔너리 내부의 하위 변수들을 합쳐서 문자열 변수 1개에 저장\n",
    "\n",
    "df['genres'] = df['genres'].apply(\n",
    "\n",
    "    lambda x : [d['name'] for d in x]).apply(lambda x : \" \".join(x))\n",
    "\n",
    "df['keywords'] = df['keywords'].apply(\n",
    "\n",
    "    lambda x : [d['name'] for d in x]).apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "#장르에 대한 출현빈도 계산\n",
    "\n",
    "c_vector_genres = count_vector.fit_transform(df['genres']).toarray()\n",
    "\n",
    "c_vector_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#코사인 유사도 계산\n",
    "\n",
    "genre_c_sim = cosine_similarity(c_vector_genres, c_vector_genres).argsort()[:, ::-1]\n",
    "\n",
    "cosine_similarity(c_vector_genres, c_vector_genres)[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영화 추천 함수\n",
    "\n",
    "def get_recommend_movie_list(data, movie_title, top=5):\n",
    "    # 특정 영화 검색\n",
    "\n",
    "    target_movie_index = data[data['title'] == movie_title].index.values\n",
    "\n",
    "    #코사인 유사도 상위 5행\n",
    "\n",
    "    sim_index = genre_c_sim[target_movie_index, :top].reshape(-1)\n",
    "\n",
    "    #아이디가 같은 self row 제외\n",
    "\n",
    "    sim_index = sim_index[sim_index != target_movie_index]\n",
    "\n",
    "    #data frame으로 만들고 vote_count으로 정렬한 뒤 return\n",
    "\n",
    "    result = data.iloc[sim_index].sort_values('vote_average', ascending=False)[:10]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['title'] == 'Superman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommend_movie_list(df, movie_title='Superman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) 추천시스템(아이템 기반 협업 필터링)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#협업 필터링 방식 - 사용자 기반, 아이템 기반(주로 아이템 기반 필터링을 사용함)\n",
    "\n",
    "# 사용자 기반 협업 필터링:\n",
    "\n",
    "#   사용자의 행동 양식을 기반으로 추천(평점, 상품 구매 이력 등)\n",
    "\n",
    "#   x축에 아이템, y축에 사용자를 표시\n",
    "\n",
    "#   어떤 사용자와 비슷한 성향의 고객들이 구매한 상품을 추천\n",
    "\n",
    "# 아이템 기반 협업 필터링(Item based collaborative Filtering):\n",
    "\n",
    "#   x축에 사용자, y축에 아이템을 표시하고 유사한 아이템을 추천해 주는 방식\n",
    "\n",
    "#   주로 코사인 유사도를 사용하여 계산함\n",
    "\n",
    "#   어떤 상품과 비슷한 상품을 추천\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./DATA/ratings_small.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pivot_table('rating', index = 'userId', columns = 'movieId')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x축 movieId, y축 userId\n",
    "\n",
    "#사용자 아이디별로 영화에 대한 평점을 볼 수 있음\n",
    "\n",
    "#영화 제목을 가져오기 위해 데이터프레임 조인\n",
    "\n",
    "df_ratings= pd.read_csv('./DATA/ratings_small.csv')\n",
    "\n",
    "df_movies = pd.read_csv('./DATA/tmdb_5000_movies.csv')\n",
    "\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필드명 id를 movieId로 변경\n",
    "\n",
    "df_movies.rename(columns = {'id': 'movieId'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings의 movieId와 movies의 movieId가 같은 행끼리 조인\n",
    "\n",
    "df_ratings_movies = pd.merge(df_ratings, df_movies, on = 'movieId')\n",
    "\n",
    "# 영화코드와 영화제목을 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x축 사용자아이디, y축 영화제목의 피벗테이블을 만들고\n",
    "\n",
    "# NaN은 0으로 채움\n",
    "\n",
    "df2 = df_ratings_movies.pivot_table('rating', index = 'userId', columns = 'title').fillna(0)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아이템 기반 협업 필터링에서는 row가 아이템이어야 하므로 x,y축을 바꿈\n",
    "\n",
    "df2 = df2.transpose()\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#코사인 유사도 계산\n",
    "\n",
    "movie_sim = cosine_similarity(df2, df2)\n",
    "\n",
    "print(movie_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코사인유사도 벡터를 데이터프레임으로 변환\n",
    "\n",
    "df_movie_sim = pd.DataFrame(data = movie_sim, index = df2.index, columns = df2.index)\n",
    "\n",
    "df_movie_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#킹콩 영화와 코사인 유사도가 비슷한 5개의 영화 추천\n",
    "\n",
    "df_movie_sim[\"King Kong\"].sort_values(ascending=False)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 텍스트 클러스트링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) KMeans\n",
    "* 라벨 숨기고 클러스트링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "han = Hannanum()\n",
    "\n",
    "df = pd.read_csv('./DATA/군집분석데이터.csv',encoding=\"ms949\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for i in df['기사내용']:\n",
    "\n",
    "    #명사만 추출\n",
    "    docs.append(han.nouns(i))\n",
    "\n",
    "for i in range(len(docs)):\n",
    "\n",
    "    #명사들 사이에 공백을 붙여서 열거\n",
    "    docs[i] = ' '.join(docs[i])\n",
    "\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "# BOW (Bag of Words)를 이용한 문서-단어 행렬 생성\n",
    "\n",
    "#문서집합에서 단어 토큰을 생성하고 각 단어의 수를 세어서 인코딩한 벡터를 생성(행은 문서번호, 컬럼은 각 단어)\n",
    "\n",
    "vec = CountVectorizer()\n",
    "#vec = TfidfVectorizer()\n",
    "\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "df2 = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())\n",
    "\n",
    "print(df2.shape)\n",
    "df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#군집 개수를 3으로 설정\n",
    "kmeans = KMeans(n_clusters=3,random_state=10).fit(df2)\n",
    "\n",
    "print(kmeans.labels_)\n",
    "print(len(kmeans.labels_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#플로팅을 위하여 ca 기법(주성분분석)으로 차원을 2차원으로 축소\n",
    "\n",
    "pca = PCA(n_components=2,random_state=10)\n",
    "\n",
    "components = pca.fit_transform(df2)\n",
    "print(components.shape)\n",
    "\n",
    "df3 = pd.DataFrame(data = components, columns = ['component 1', 'component 2'])\n",
    "\n",
    "df3.index=df['검색어']\n",
    "\n",
    "print(df3)\n",
    "\n",
    "kmeans.labels_ == 0\n",
    "\n",
    "# x축 : first, y축 : second 번호로 나타낸 후 시각화\n",
    "\n",
    "plt.scatter(df3.iloc[kmeans.labels_ == 0, 0],\n",
    "\n",
    "            df3.iloc[kmeans.labels_ == 0, 1], s = 10, c = 'red',  label = 'cluster1')\n",
    "\n",
    "plt.scatter(df3.iloc[kmeans.labels_ == 1, 0],\n",
    "\n",
    "            df3.iloc[kmeans.labels_ == 1, 1], s = 10, c = 'blue', label = 'cluster2')\n",
    "\n",
    "plt.scatter(df3.iloc[kmeans.labels_ == 2, 0],\n",
    "\n",
    "            df3.iloc[kmeans.labels_ == 2, 1], s = 10, c = 'green', label = 'cluster3')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) KMeans - 뉴스분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 뉴스 데이터 분석\n",
    "\n",
    "#출처 : https://teddylee777.github.io/machine-learning/sklearn-kmeans-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./DATA/news.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#클래스별 샘플 갯수\n",
    "\n",
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ^[  ] start, [^ ] not , 한글/영문자만 허용\n",
    "\n",
    "def preprocessing(sentence):\n",
    "\n",
    "    sentence =re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "#content 필드에 preprocessing 함수 적용\n",
    "\n",
    "df['content_cleaned'] = df['content'].apply(preprocessing)\n",
    "\n",
    "content = df['content_cleaned'].tolist()\n",
    "\n",
    "content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 인코딩한 벡터를 생성\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000) #단어수 제한\n",
    "\n",
    "#vectorizer = TfidfVectorizer(max_features=1000) #단어수 제한\n",
    "\n",
    "X = vectorizer.fit_transform(content)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# l2 정규화(TfidfVectorizer의 경우 생략)\n",
    "\n",
    "X = normalize(X)\n",
    "\n",
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# k-means 알고리즘 적용\n",
    "\n",
    "kmeans = KMeans(n_clusters=3,random_state=10).fit(X)\n",
    "\n",
    "# labels에 merge\n",
    "\n",
    "#df['labels'] = kmeans.labels_\n",
    "\n",
    "print(kmeans.labels_)\n",
    "\n",
    "print(pd.DataFrame(kmeans.labels_).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#그래프 출력을 위해 주성분 분석\n",
    "\n",
    "pca=PCA(n_components=2, random_state=10)\n",
    "\n",
    "components=pca.fit_transform(X.toarray())\n",
    "\n",
    "df3=pd.DataFrame(data=components, columns=['component 1','component 2'])\n",
    "\n",
    "df3.index=df['category']\n",
    "\n",
    "plt.scatter(df3.iloc[kmeans.labels_ == 0, 0], df3.iloc[kmeans.labels_ == 0, 1], s=10, c='r', label='cluster 1')\n",
    "\n",
    "plt.scatter(df3.iloc[kmeans.labels_ == 1, 0], df3.iloc[kmeans.labels_ == 1, 1], s=10, c='b', label='cluster 2')\n",
    "\n",
    "plt.scatter(df3.iloc[kmeans.labels_ == 2, 0], df3.iloc[kmeans.labels_ == 2, 1], s=10, c='g', label='cluster 3')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 계층적 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트리 형태의 군집으로 나누는 방법\n",
    "\n",
    "# 개별대상간의 거리에 의하여 가장 가까이에 있는 대상들로부터 시작하여 결합하여\n",
    "\n",
    "# 트리 모양의 계층구조를 형성하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./DATA/군집분석데이터.csv',encoding=\"ms949\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum() \n",
    "\n",
    "docs = []\n",
    "\n",
    "for i in df['기사내용']:\n",
    "\n",
    "    #명사 추출\n",
    "\n",
    "    docs.append(hannanum.nouns(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어에 공백을 추가    \n",
    "\n",
    "for i in range(len(docs)):\n",
    "\n",
    "    docs[i] = ' '.join(docs[i])\n",
    "\n",
    "#print(docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "df2 = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#계층적 클러스터링 : 하나의 데이터 샘플을 하나의 클러스터로 보고 가장 유사도가 높은 클러스터를 합치면서 클러스터 갯수를 줄여가는 방법\n",
    "\n",
    "#클러스터수를 3으로 설정\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3)  \n",
    "\n",
    "cluster.fit_predict(df2)  \n",
    "\n",
    "#덴드로그램 출력, 트리를 나타내는 다이어그램\n",
    "\n",
    "plt.figure(figsize=(10, 7))  \n",
    "\n",
    "result=shc.linkage(df2)\n",
    "\n",
    "shc.dendrogram(result)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#그래프 출력을 위해 주성분 분석\n",
    "\n",
    "pca=PCA(n_components=2, random_state=10)\n",
    "\n",
    "components=pca.fit_transform(df2)\n",
    "\n",
    "df3=pd.DataFrame(data=components, columns=['component 1','component 2'])\n",
    "\n",
    "df3.index=df['검색어']\n",
    "\n",
    "plt.scatter(df3.iloc[cluster.labels_ == 0, 0], df3.iloc[cluster.labels_ == 0, 1], s=10, c='r', label='cluster 1')\n",
    "\n",
    "plt.scatter(df3.iloc[cluster.labels_ == 1, 0], df3.iloc[cluster.labels_ == 1, 1], s=10, c='b', label='cluster 2')\n",
    "\n",
    "plt.scatter(df3.iloc[cluster.labels_ == 2, 0], df3.iloc[cluster.labels_ == 2, 1], s=10, c='g', label='cluster 3')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 토픽 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 구조화되지 않은 방대한 문헌 집단에서 주제를 찾아내기 위한 알고리즘\n",
    "\n",
    "- 맥락과 관련된 단서들을 이용하여 의미를 가진 단어들을 클러스터링하여 주제를 추론하는 모델\n",
    "\n",
    "- 구조화되지 않은 대량의 텍스트로부터 숨겨져 있는 주제 구조를 발견하기 위한 통계적 추론 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 잠재의미분석(LSA)\n",
    "특이값 분해(Singular Value Decomposition, SVD)\n",
    "\n",
    "    차원축소 방법 중 하나\n",
    "\n",
    "m x n 행렬을   m x m 직교행렬, n x n 직교행렬, m x n 직사각 대각행렬  3개의 곱으로 분해하는 작업\n",
    "\n",
    "행렬 연산을 통해 데이터의 차원을 축소하고 중요한 특징들을 추출하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups  \n",
    "\n",
    "#시간이 오래 걸림\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))  \n",
    "\n",
    "documents = dataset.data \n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.target_names) #뉴스 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "# 알파벳 이외의 문자 제거\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# 길이가 3이하인 단어 제거\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(\n",
    "\n",
    "    lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# 소문자 변환\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 감성분석\n",
    "감성분석 : 텍스트에 나타난 주관적 요소를 분석하여 긍정,부정의 요소 및 그 정도를 판별하여 정량화하는 기법\n",
    "\n",
    "긍정과 부정을 판별할 뿐 아니라 긍정,부정의 대상이 되는 단어 또는 개체를 추출하고 감성을 표현하는 이의 의도나 입장을 분석하는 것도 포함하는 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.단어사전기반 분석 - 감성사전을 이용하여 각 단어의 감정 분류와 그 정도를 알 수 있어야 함\n",
    "\n",
    "#   텍스트와 감성지수가 사전에 정의되어 있어야 함\n",
    "\n",
    "import glob\n",
    "\n",
    "# pip install afinn\n",
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "#imdb 데이터셋 5만건의 학습용,검증용 데이터셋 긍정,부정 리뷰로 라벨링되어 있음.\n",
    "\n",
    "#긍정리뷰데이터 20번째 내용\n",
    "\n",
    "# glob.glob 특정한 패턴의 파일만 선택하는 함수\n",
    "\n",
    "pos_review=(glob.glob(\"D:/CLASS/train/pos/*.txt\"))[20]\n",
    "\n",
    "f = open(pos_review)\n",
    "\n",
    "#파일을 읽음\n",
    "\n",
    "lines1 = f.readlines()[0]\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#감성분석 객체\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "#텍스트 전처리 후 감성점수 산출\n",
    "\n",
    "afinn.score(lines1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=list(glob.glob('D:/CLASS/train/pos/*.txt')[:10])\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습용 긍정리뷰 10개 파일만 테스트\n",
    "\n",
    "afinn=Afinn() #감성분석 함수\n",
    "\n",
    "for i in files:\n",
    "\n",
    "    f=open(i) #파일 오픈\n",
    "\n",
    "    lines1=f.readlines()[0] #리스트의 첫번째 문자열\n",
    "\n",
    "    print(afinn.score(lines1)) #감성점수\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#부정리뷰데이터 20번째 내용\n",
    "\n",
    "neg_review=(glob.glob(\"D:/CLASS/train/neg/*.txt\"))[20]\n",
    "\n",
    "f = open(neg_review)\n",
    "\n",
    "lines2 = f.readlines()[0]\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn.score(lines2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=list(glob.glob('D:/CLASS/train/neg/*.txt')[:10])\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습용 부정리뷰 10개 파일만 테스트\n",
    "\n",
    "afinn=Afinn() #감성분석 함수\n",
    "\n",
    "for i in files:\n",
    "\n",
    "    f=open(i) #파일 오픈\n",
    "\n",
    "    lines1=f.readlines()[0] #리스트의 첫번째 문자열\n",
    "\n",
    "    print(afinn.score(lines1)) #감성점수\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.기계학습으로 감성분석(시간이 매우 오래 걸림)\n",
    "\n",
    "import glob\n",
    "\n",
    "#긍정 텍스트 로딩\n",
    "\n",
    "pos_review=(glob.glob(\"D:/CLASS/train/pos/*.txt\")[:100])\n",
    "\n",
    "lines_pos=[]\n",
    "\n",
    "for i in pos_review:\n",
    "\n",
    "    try:\n",
    "\n",
    "        f = open(i)\n",
    "\n",
    "        temp = f.readlines()[0]\n",
    "\n",
    "        lines_pos.append(temp)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    except :\n",
    "\n",
    "        continue\n",
    "\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#부정 텍스트 로딩\n",
    "\n",
    "neg_review=(glob.glob(\"D:/CLASS/train/neg/*.txt\")[:100])\n",
    "\n",
    "lines_neg=[]\n",
    "\n",
    "for i in neg_review:\n",
    "\n",
    "    try:\n",
    "\n",
    "        f = open(i)\n",
    "\n",
    "        temp = f.readlines()[0]\n",
    "\n",
    "        lines_neg.append(temp)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    except :\n",
    "\n",
    "        continue\n",
    "\n",
    "len(lines_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#긍정,부정 리뷰를 합침\n",
    "\n",
    "total_text=lines_pos+lines_neg\n",
    "\n",
    "len(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#긍정,부정 클래스 라벨링\n",
    "\n",
    "x = np.array([\"pos\", \"neg\"])\n",
    "\n",
    "class_Index=np.repeat(x, [len(lines_pos), len(lines_neg)], axis=0)\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어들에 Tfidf 가중치를 부여한 후 문서-단어 매트릭스로 바꿈\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=stop_words).fit(total_text)\n",
    "\n",
    "X_train_vectorized = vect.transform(total_text)\n",
    "\n",
    "X_train_vectorized.index = class_Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#데이터프레임으로 변환\n",
    "\n",
    "df=pd.DataFrame(X_train_vectorized.toarray(), columns=vect.vocabulary_.keys())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,value in enumerate(X_train_vectorized[0].toarray()[0]):  \n",
    "\n",
    "    if value>0:\n",
    "\n",
    "        print(idx, value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#로지스틱 회귀 모형\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit = LogisticRegression(random_state=10)\n",
    "\n",
    "logit.fit(X_train_vectorized, class_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#긍정 리뷰들을 하나씩 불러와서 실험\n",
    "\n",
    "def pos_review(model):\n",
    "\n",
    "    count_all=0\n",
    "\n",
    "    count=0\n",
    "\n",
    "    num=100\n",
    "\n",
    "    tests1=[]\n",
    "\n",
    "    for idx in range(0,num):\n",
    "\n",
    "        pos_review_test=(glob.glob(\"D:/CLASS/test/pos/*.txt\"))[idx]\n",
    "\n",
    "        f = open(pos_review_test, 'r',encoding=\"utf-8\")\n",
    "\n",
    "        tests1.append(f.readlines())\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    for test in tests1:\n",
    "\n",
    "        pred = model.predict(vect.transform(test))\n",
    "\n",
    "        result=pred[0]\n",
    "\n",
    "        if result==\"pos\":\n",
    "\n",
    "            count+=1\n",
    "\n",
    "        count_all += 1\n",
    "\n",
    "    rate= count*100/count_all\n",
    "\n",
    "    print(f\"분류정확도:{rate:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#부정 리뷰들을 하나씩 불러와서 실험\n",
    "\n",
    "def neg_review(model):\n",
    "\n",
    "    count_all=0\n",
    "\n",
    "    count=0\n",
    "\n",
    "    num=100\n",
    "\n",
    "    tests2=[]\n",
    "\n",
    "    for idx in range(0,num):\n",
    "\n",
    "        neg_review_test=(glob.glob(\"D:/CLASS/test/neg/*.txt\"))[idx]\n",
    "\n",
    "        f = open(neg_review_test, 'r',encoding=\"utf-8\")\n",
    "\n",
    "        tests2.append(f.readlines())\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    tests1=[]\n",
    "\n",
    "    for idx in range(0,num):\n",
    "\n",
    "        pos_review_test=(glob.glob(\"D:/CLASS/test/pos/*.txt\"))[idx]\n",
    "\n",
    "        f = open(pos_review_test, 'r',encoding=\"utf-8\")\n",
    "\n",
    "        tests1.append(f.readlines())\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    for test in tests1:\n",
    "\n",
    "        pred = model.predict(vect.transform(test))\n",
    "\n",
    "        result=pred[0]\n",
    "\n",
    "        if result==\"pos\":\n",
    "\n",
    "            count+=1\n",
    "\n",
    "        count_all += 1\n",
    "\n",
    "    rate= count*100/count_all\n",
    "\n",
    "    print(f\"분류정확도:{rate:.1f}%\")\n",
    "\n",
    "###############################################\n",
    "\n",
    "#부정 리뷰들을 하나씩 불러와서 실험\n",
    "\n",
    "def neg_review(model):\n",
    "\n",
    "    count_all=0\n",
    "\n",
    "    count=0\n",
    "\n",
    "    num=100\n",
    "\n",
    "    tests2=[]\n",
    "\n",
    "    for idx in range(0,num):\n",
    "\n",
    "        neg_review_test=(glob.glob(\"D:/CLASS/test/neg/*.txt\"))[idx]\n",
    "\n",
    "        f = open(neg_review_test, 'r',encoding=\"utf-8\")\n",
    "\n",
    "        tests2.append(f.readlines())\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    for test in tests2:\n",
    "\n",
    "        preds = model.predict(vect.transform(test))\n",
    "\n",
    "        result=preds[0]\n",
    "\n",
    "        if result==\"neg\":\n",
    "\n",
    "            count+=1\n",
    "\n",
    "        count_all+=1\n",
    "\n",
    "    rate= count*100/count_all\n",
    "\n",
    "    print(\"예측정확도:{0:.1f}%\".format(rate))\n",
    "\n",
    "pos_review(logit)\n",
    "\n",
    "neg_review(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#의사결정나무 모형\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=10)\n",
    "\n",
    "tree.fit(X_train_vectorized, class_Index)\n",
    "\n",
    "pos_review(tree)\n",
    "\n",
    "neg_review(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#랜덤포레스트\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#10개의 트리로 구성된 랜덤 포레스트\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=10, random_state=10)\n",
    "\n",
    "forest.fit(X_train_vectorized, class_Index)\n",
    "\n",
    "pos_review(forest)\n",
    "\n",
    "neg_review(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "knn.fit(X_train_vectorized, class_Index)\n",
    "\n",
    "pos_review(knn)\n",
    "\n",
    "neg_review(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#인공신경망\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(random_state=10)\n",
    "\n",
    "mlp.fit(X_train_vectorized, class_Index)\n",
    "\n",
    "pos_review(mlp)\n",
    "\n",
    "neg_review(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM 모형\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(random_state=10)\n",
    "\n",
    "svm.fit(X_train_vectorized, class_Index)\n",
    "\n",
    "pos_review(svm)\n",
    "\n",
    "neg_review(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "#train = [ ('샌드위치 좋아해요', 'pos'), ('너무 좋은 곳이예요', 'pos'), ('너무 맛있어요', 'pos'), ('내가 제일 좋아하는 곳이예요', 'pos'), ('친한 친구예요', 'pos'), ('이 사람은 믿을 수 없어요', 'neg'), ('별로 안좋은 곳이네요', 'neg'), (\"맛이 별로네요\", 'neg'), ('경치가 별로예요', 'neg'), ('별로 볼게 없네요', 'neg') ]\n",
    "\n",
    "\n",
    "\n",
    "#test = [ ('최고의 음료수', 'pos'), ('별로 안좋아요', 'neg'), ('이번주는 컨디션이 안좋아요', 'neg'), ('너무 좋아요', 'pos'), ('나와 가장 친해요', 'pos'), (\"믿을 수 없어요\", 'neg')]\n",
    "\n",
    "\n",
    "\n",
    "train = [\n",
    "\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "\n",
    "    ('This is my best work.', 'pos'),\n",
    "\n",
    "    ('What an awesome view', 'pos'),\n",
    "\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "\n",
    "    ('My boss is horrible.', 'neg')\n",
    "\n",
    "]\n",
    "\n",
    "test = [\n",
    "\n",
    "    ('The beer was good.', 'pos'),\n",
    "\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "\n",
    "    ('I am not feeling dandy today.', 'neg'),\n",
    "\n",
    "    ('I feel amazing!', 'pos'),\n",
    "\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "cl = NaiveBayesClassifier(train)\n",
    "\n",
    "#print(cl.classify('맛있는 음식이네요'))\n",
    "\n",
    "#print(cl.classify(\"피자맛이 별로네요\"))\n",
    "\n",
    "\n",
    "\n",
    "print(cl.classify('Their burgers are amazing'))  \n",
    "\n",
    "print(cl.classify(\"I don't like their pizza.\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#여러 문장을 종합하여 부정으로 분류\n",
    "\n",
    "#blob = TextBlob(\"맛있는 음식이네요. 피자는 별로네요. 서비스는 좋네요.\", classifier=cl)\n",
    "\n",
    "\n",
    "blob = TextBlob(\"The beer was amazing. But the hangover was horrible. My boss was not happy.\", classifier=cl)\n",
    "\n",
    "blob.classify()  # \"neg\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#개별 문장으로 분류\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "\n",
    "    print(sentence, '==>', sentence.classify())\n",
    "\n",
    "# \"pos\", \"neg\", \"neg\"\n",
    "\n",
    "for row in test:\n",
    "\n",
    "    print(row[0],'==>', cl.classify(row[0]))\n",
    "\n",
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.show_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most Informative Features\n",
    "\n",
    "contains(this) = True              #neg : pos    =      2.3 : 1.0 this가 포함된 경우 부정:긍정=2.3:1.0\n",
    "\n",
    "contains(this) = False            # pos : neg    =      1.8 : 1.0 this가 포함되지 않은 경우 긍정:부정=1.8:1.0\n",
    "\n",
    "contains(an) = False            # neg : pos    =      1.6 : 1.0\n",
    "\n",
    "contains(This) = False            # neg : pos    =      1.6 : 1.0\n",
    "\n",
    "contains(I) = True             # neg : pos    =      1.4 : 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 연관어분석(단어 유사도)\n",
    "1) 동시출현빈도 기반\n",
    "\n",
    "두 개의 단어가 주어진 문맥에서 서로 얼마나 연관되어 있는지\n",
    "\n",
    "1.동시출현 기반 연관어 분석 - 대상어와 다른 단어들이 같은 문맥 내에서 동시에 출현한 횟수를 세는 방법\n",
    "\n",
    " 두 단어가 같은 문맥 내에서 함께 나타나는 빈도가 높을수록 강한 연관관계가 성립한다는 가정\n",
    "\n",
    " 한 문장에서 한 단어가 여러번 중복되어 나타날 경우 1회로 계산하거나 n회로 계산, 1회로 계산하는 것이 일반적인 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# 긍정리뷰 100개 로딩\n",
    "\n",
    "pos_review=(glob.glob(\"d:/data/imdb/train/pos/*.txt\"))[0:100]\n",
    "\n",
    "lines_pos=[]\n",
    "\n",
    "for i in pos_review:\n",
    "\n",
    "    try:\n",
    "\n",
    "        f = open(i, 'r')\n",
    "\n",
    "        temp = f.readlines()[0]\n",
    "\n",
    "        lines_pos.append(temp)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    except :\n",
    "\n",
    "        continue\n",
    "\n",
    "len(lines_pos)\n",
    "\n",
    "###############################################\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+') #알파벳, 숫자, _\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#동시출현 단어 계산\n",
    "\n",
    "count = {}   #동시출현 빈도가 저장될 dict\n",
    "\n",
    "for line in lines_pos:\n",
    "\n",
    "    words =  line.lower()\n",
    "\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "\n",
    "    #불용어 제거, 불용어에 br 추가\n",
    "\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if i not in stop_words+[\"br\"]]\n",
    "\n",
    "    #글자수가 1인 단어 제외\n",
    "\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "\n",
    "    for i, a in enumerate(stopped_tokens2):\n",
    "\n",
    "        for b in stopped_tokens2[i+1:]:\n",
    "\n",
    "            if a>b:\n",
    "\n",
    "                count[b, a] = count.get((b, a),0) + 1  \n",
    "\n",
    "            else :\n",
    "\n",
    "                count[a, b] = count.get((a, b),0) + 1    \n",
    "\n",
    "###############################################\n",
    "\n",
    "#딕셔너리로부터 데이터프레임 생성\n",
    "\n",
    "df=pd.DataFrame.from_dict(count, orient='index')\n",
    "\n",
    "#리스트 구성\n",
    "\n",
    "list1=[]\n",
    "\n",
    "for i in range(len(df)):\n",
    "\n",
    "    list1.append([df.index[i][0],df.index[i][1],df[0][i]])\n",
    "\n",
    "###############################################\n",
    "\n",
    "df2=pd.DataFrame(list1, columns=[\"term1\",\"term2\",\"freq\"])\n",
    "\n",
    "df3=df2.sort_values(by=['freq'],ascending=False)\n",
    "\n",
    "df3_pos=df3.reset_index(drop=True)\n",
    "\n",
    "#동시출현 단어 페어 빈도 상위 20개 출력\n",
    "\n",
    "df3_pos.head(20)\n",
    "\n",
    "#film과 story 총 41회 동시에 출현\n",
    "\n",
    "###############################################\n",
    "\n",
    "#부정 리뷰에 적용\n",
    "\n",
    "neg_review=(glob.glob(\"d:/data/imdb/train/neg/*.txt\"))[0:100]\n",
    "\n",
    "lines_neg=[]\n",
    "\n",
    "for i in neg_review:\n",
    "\n",
    "    try:\n",
    "\n",
    "        f = open(i, 'r')\n",
    "\n",
    "        temp = f.readlines()[0]\n",
    "\n",
    "        lines_neg.append(temp)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        continue\n",
    "\n",
    "len(lines_neg)\n",
    "\n",
    "###############################################\n",
    "\n",
    "count = {}   #동시출현 빈도가 저장될 dict\n",
    "\n",
    "for line in lines_neg:\n",
    "\n",
    "    words =  line.lower()\n",
    "\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if not i in stop_words+[\"br\"]]\n",
    "\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "\n",
    "    for i, a in enumerate(stopped_tokens2):\n",
    "\n",
    "        for b in stopped_tokens2[i+1:]:\n",
    "\n",
    "            if a>b:\n",
    "\n",
    "                count[b, a] = count.get((b, a),0) + 1  \n",
    "\n",
    "            else :\n",
    "\n",
    "                count[a, b] = count.get((a, b),0) + 1\n",
    "\n",
    "###############################################\n",
    "\n",
    "df=pd.DataFrame.from_dict(count, orient='index')\n",
    "\n",
    "list1=[]\n",
    "\n",
    "for i in range(len(df)):\n",
    "\n",
    "    list1.append([df.index[i][0],df.index[i][1],df[0][i]])\n",
    "\n",
    "df2=pd.DataFrame(list1, columns=[\"term1\",\"term2\",\"freq\"])\n",
    "\n",
    "df3=df2.sort_values(by=['freq'],ascending=False)\n",
    "\n",
    "df3_neg=df3.reset_index(drop=True)\n",
    "\n",
    "df3_neg.head(20)\n",
    "\n",
    "#film과 movie가 총 42회 동시에 출현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 통계적 가중치 기반\n",
    "통계적으로 가중치를 구한 후 두 단어 간의 유사도를 단어간의 연관도로 적용하는 방법\n",
    "\n",
    " 1.단어마다 가중치를 할당해야 함(출현빈도, tf-idf 등으로 계산)\n",
    "\n",
    " 2.단어간의 유사도 계산(cosine similarity 등의 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "#긍정리뷰 100개 불러오기\n",
    "\n",
    "pos_review=(glob.glob(\"d:/data/imdb/train/pos/*.txt\"))[0:100]\n",
    "\n",
    "lines_pos=[]\n",
    "\n",
    "for i in pos_review:\n",
    "\n",
    "    try:\n",
    "\n",
    "        f = open(i, 'r')\n",
    "\n",
    "        temp = f.readlines()[0]\n",
    "\n",
    "        lines_pos.append(temp)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    except :\n",
    "\n",
    "        continue\n",
    "\n",
    "len(lines_pos)\n",
    "\n",
    "###############################################\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#TF-IDF 가중치 할당\n",
    "\n",
    "vec = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "vector_lines_pos = vec.fit_transform(lines_pos)\n",
    "\n",
    "A=vector_lines_pos.toarray()\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "print(A)\n",
    "\n",
    "#x축 단어, y축 문서\n",
    "\n",
    "###############################################\n",
    "\n",
    "#현재 상태는 100개의 문서의 유사도\n",
    "\n",
    "#단어간의 유사도를 구하는 것이 목적이므로\n",
    "\n",
    "#단어-문서 행렬로 변경\n",
    "\n",
    "#x축 문서, y축 단어\n",
    "\n",
    "A=A.transpose()\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "print(A)\n",
    "\n",
    "###############################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "#밀집행렬(dense array)\n",
    "\n",
    "a=np.array([[0.5,0,0],[0,1,0],[0.7,0,1.5]])\n",
    "\n",
    "#밀집행렬을 희소행렬(sparse array)로 변환\n",
    "\n",
    "#밀집행렬의 단점: 0이 많을 경우 메모리 낭비가 될 수 있음\n",
    "\n",
    "#희소행렬은 0이 아닌 값들의 위치와 값만 기록하여 메모리를 절약하는 방식\n",
    "\n",
    "b=sparse.csr_matrix(a)  \n",
    "\n",
    "print(b)\n",
    "\n",
    "# (0,0) 0.5 => 인덱스 0,0에 값 0.5\n",
    "\n",
    "# (1,1) 1 => 인덱스 1,1에 값 1\n",
    "\n",
    "# (2,0) 0.7 => 인덱스 2,0에 값 0.7\n",
    "\n",
    "# (2,2) 1 => 인덱스 2,2에 값 1.5\n",
    "\n",
    "c=b.toarray() #희소행렬을 밀집행렬로 변환\n",
    "\n",
    "print(c)\n",
    "\n",
    "###############################################\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "print(A)\n",
    "\n",
    "#x축 단어, y축 문서\n",
    "\n",
    "###############################################\n",
    "\n",
    "#현재 상태는 100개의 문서의 유사도\n",
    "\n",
    "#단어간의 유사도를 구하는 것이 목적이므로\n",
    "\n",
    "#단어-문서 행렬로 변경\n",
    "\n",
    "#x축 문서, y축 단어\n",
    "\n",
    "A=A.transpose()\n",
    "\n",
    "print(A.shape)\n",
    "\n",
    "print(A)\n",
    "\n",
    "###############################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "#밀집행렬(dense array)\n",
    "\n",
    "a=np.array([[0.5,0,0],[0,1,0],[0.7,0,1.5]])\n",
    "\n",
    "#밀집행렬을 희소행렬(sparse array)로 변환\n",
    "\n",
    "#밀집행렬의 단점: 0이 많을 경우 메모리 낭비가 될 수 있음\n",
    "\n",
    "#희소행렬은 0이 아닌 값들의 위치와 값만 기록하여 메모리를 절약하는 방식\n",
    "\n",
    "b=sparse.csr_matrix(a)  \n",
    "\n",
    "print(b)\n",
    "\n",
    "# (0,0) 0.5 => 인덱스 0,0에 값 0.5\n",
    "\n",
    "# (1,1) 1 => 인덱스 1,1에 값 1\n",
    "\n",
    "# (2,0) 0.7 => 인덱스 2,0에 값 0.7\n",
    "\n",
    "# (2,2) 1 => 인덱스 2,2에 값 1.5\n",
    "\n",
    "c=b.toarray() #희소행렬을 밀집행렬로 변환\n",
    "\n",
    "print(c)\n",
    "\n",
    "###############################################\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A_sparse = sparse.csr_matrix(A)\n",
    "\n",
    "#코사인 유사도 계산\n",
    "\n",
    "similarities_sparse = cosine_similarity(A_sparse,dense_output=False)\n",
    "\n",
    "# todok() 행렬을 딕셔너리 형태로 변환\n",
    "\n",
    "list(similarities_sparse.todok().items())[35000:35010]\n",
    "\n",
    "#list(similarities_sparse.todok().items())[-10:]\n",
    "\n",
    "#단어 이름이 아닌 인덱스 형태로 출력됨\n",
    "\n",
    "###############################################\n",
    "\n",
    "# (1469,108), 0.37 1469 단어와 108 단어의 유사도는 37%\n",
    "\n",
    "vec.get_feature_names_out()[1469]\n",
    "\n",
    "###############################################\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#위의 결과값을 데이터프레임으로 출력\n",
    "\n",
    "df=pd.DataFrame(list(similarities_sparse.todok().items()),columns=[\"words\",\"weight\"])\n",
    "\n",
    "df2=df.sort_values(by=['weight'],ascending=False)\n",
    "\n",
    "df2=df2.reset_index(drop=True)\n",
    "\n",
    "#단어 자신끼리의 짝은 1이 되므로 1보다 작은 항목들만 출력시킴\n",
    "\n",
    "df3=df2.loc[np.round(df2['weight']) < 1]\n",
    "\n",
    "df3=df3.reset_index(drop=True)\n",
    "\n",
    "df3.head(10)\n",
    "\n",
    "###############################################\n",
    "\n",
    "for i,row in enumerate(df3.iterrows()):\n",
    "\n",
    "    a=vec.get_feature_names_out()[row[1][0][0]]\n",
    "\n",
    "    b=vec.get_feature_names_out()[row[1][0][1]]\n",
    "\n",
    "    print(f'{a},{b}=>{row[1][1]:.2f}')\n",
    "\n",
    "    if i>10:\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)  word2vec 기반\n",
    "단어의 의미는 그 단어 주변 단어의 분포로 이해될 수 있다\n",
    "\n",
    "단어의 의미는 단어 벡터 안에 인코딩될 수 있다.\n",
    "\n",
    "단순히 출현횟수만을 고려하는 것이 아닌 단어 위치, 순서도 고려하는 방법\n",
    "\n",
    "CBOW  주변 단어로 중심 단어를 예측하는 방법\n",
    "\n",
    "Skip-gram 중심 단어로 주변 단어를 예측하는 방법(더 많이 사용되는 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "pos_review=(glob.glob(\"D:/CLASS/train/pos/*.txt\"))[0:100]\n",
    "\n",
    "lines_pos=[]\n",
    "\n",
    "for i in pos_review:\n",
    "\n",
    "    try:\n",
    "\n",
    "        f = open(i, 'r')\n",
    "\n",
    "        temp = f.readlines()[0]\n",
    "\n",
    "        lines_pos.append(temp)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    except :\n",
    "\n",
    "        continue\n",
    "\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "#단어 추출\n",
    "\n",
    "text=[]\n",
    "\n",
    "for line in lines_pos:\n",
    "\n",
    "    words =  line.lower()\n",
    "\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if i not  in stop_words+[\"br\"]]\n",
    "\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "\n",
    "    text.append(stopped_tokens2)\n",
    "\n",
    "# word2vec 모형 생성 , sg=1 skip-gram을 적용, window=2 중심 단어로부터 좌우 2개의 단어까지 학습에 적용\n",
    "\n",
    "#min_count=3 최소 3회 이상 출현한 단어들을 대상으로 학습\n",
    "\n",
    "model = Word2Vec(text, vector_size=10, sg=1, window=2, min_count=3)\n",
    "\n",
    "# 두 단어의 유사도 계산\n",
    "\n",
    "model.wv.similarity('film', 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good과 가장 유사한 단어 5개\n",
    "\n",
    "model.wv.most_similar(\"good\",topn =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델에 저장된 단어의 갯수\n",
    "\n",
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델에 저장된 단어 텍스트\n",
    "\n",
    "model.wv.index_to_key[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어에 해당하는 벡터값\n",
    "\n",
    "model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 텍스트마이닝 실습(RNN)\n",
    "### 1) imdb 영화리뷰 분류\n",
    "* https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "* IMDB 데이터셋(케라스에 내장된 데이터셋, 숫자로 전처리되어 있음, 17MB 정도의 데이터셋)\n",
    "\n",
    "* 영화 리뷰 50,000개의 데이터셋\n",
    "\n",
    "* 학습용 데이터 25,000개와 검증용 데이터 25,000개, 각각 50%는 부정, 50%는 긍정 리뷰로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "top_words = 5000 #상위 단어 5000개\n",
    "\n",
    "#학습용,검증용 5:5\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500 #리뷰의 최대 길이를 500으로 설정\n",
    "\n",
    "# 길이가 짧으면 공백으로 채움\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# create the model\n",
    "\n",
    "embedding_vecor_length = 32 #벡터사이즈\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#임베딩 레이어 Embedding(단어의개수, 벡터크기, 입력사이즈(최대단어개수))\n",
    "\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "#문장을 단어들의 시퀀스로 간주하고 순환 레이어의 입력으로\n",
    "\n",
    "model.add(LSTM(100)) # Long Short-Term Memory\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=model.fit(X_train[:1000], y_train[:1000], epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate(X_train[:1000], y_train[:1000], verbose=1)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test[:1000], y_test[:1000], verbose=1)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 과정 표시\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.plot(hist.history['loss']) #손실\n",
    "\n",
    "plt.plot(hist.history['accuracy']) #정확도\n",
    "\n",
    "plt.legend(['loss','accuracy'])\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 한글텍스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('c:/data/text/ratings_train.csv',encoding='ms949')\n",
    "\n",
    "test_data = pd.read_csv('c:/data/text/ratings_test.csv',encoding='ms949')\n",
    "\n",
    "###############################################\n",
    "\n",
    "len(train_data)\n",
    "\n",
    "###############################################\n",
    "\n",
    "train_data[:3]\n",
    "\n",
    "###############################################\n",
    "\n",
    "len(test_data)\n",
    "\n",
    "###############################################\n",
    "\n",
    "test_data[:3]\n",
    "\n",
    "###############################################\n",
    "\n",
    "# 중복되지 않은 데이터 확인\n",
    "\n",
    "train_data['document'].nunique(), train_data['label'].nunique()\n",
    "\n",
    "# 중복값이 약 4000개\n",
    "\n",
    "###############################################\n",
    "\n",
    "#중복값 제거\n",
    "\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "len(train_data)\n",
    "\n",
    "###############################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data['label'].value_counts().plot(kind = 'bar')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "\n",
    "train_data.loc[train_data.document.isnull()]\n",
    "\n",
    "###############################################\n",
    "\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "\n",
    "train_data.isnull().values.any()\n",
    "\n",
    "###############################################\n",
    "\n",
    "#특수문자,기호 제거\n",
    "\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "train_data[:3]\n",
    "\n",
    "###############################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#공백 제거\n",
    "\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\")\n",
    "\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "\n",
    "train_data.isnull().sum()\n",
    "\n",
    "###############################################\n",
    "\n",
    "train_data.loc[train_data.document.isnull()][:3]\n",
    "\n",
    "###############################################\n",
    "\n",
    "#null 샘플 제거\n",
    "\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "len(train_data)\n",
    "\n",
    "###############################################\n",
    "\n",
    "# test_data에도 동일한 과정 적용\n",
    "\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True)\n",
    "\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\")\n",
    "\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "\n",
    "test_data = test_data.dropna(how='any')\n",
    "\n",
    "len(test_data)\n",
    "\n",
    "###############################################\n",
    "\n",
    "# 불용어 사전\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "###############################################\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "X_train = []  \n",
    "\n",
    "\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "train_data[:3]\n",
    "\n",
    "###############################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#공백 제거\n",
    "\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\")\n",
    "\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "\n",
    "train_data.isnull().sum()\n",
    "\n",
    "###############################################\n",
    "\n",
    "train_data.loc[train_data.document.isnull()][:3]\n",
    "\n",
    "###############################################\n",
    "\n",
    "#null 샘플 제거\n",
    "\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "len(train_data)\n",
    "\n",
    "###############################################\n",
    "\n",
    "# test_data에도 동일한 과정 적용\n",
    "\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True)\n",
    "\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\")\n",
    "\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "\n",
    "test_data = test_data.dropna(how='any')\n",
    "\n",
    "len(test_data)\n",
    "\n",
    "###############################################\n",
    "\n",
    "# 불용어 사전\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "###############################################\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "X_train = []  \n",
    "\n",
    "#형태소 분석\n",
    "\n",
    "#for sentence in train_data['document']:\n",
    "\n",
    "for sentence in train_data['document'][:10000]:    \n",
    "\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "###############################################\n",
    "\n",
    "print(X_train[:3])\n",
    "\n",
    "###############################################\n",
    "\n",
    "# 테스트 데이터에 대한 토큰화\n",
    "\n",
    "X_test = []\n",
    "\n",
    "#for sentence in test_data['document']:\n",
    "\n",
    "for sentence in test_data['document'][:10000]:    \n",
    "\n",
    "    temp_X = okt.morphs(sentence, stem=True)\n",
    "\n",
    "    temp_X = [word for word in temp_X if not word in stopwords]\n",
    "\n",
    "    X_test.append(temp_X)\n",
    "\n",
    "###############################################\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 정수 인코딩\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "#print(tokenizer.word_index)\n",
    "\n",
    "###############################################\n",
    "\n",
    "# 출현빈도가 3회 미만인 단어들\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "total_cnt = len(tokenizer.word_index) # 단어수\n",
    "\n",
    "rare_cnt = 0\n",
    "\n",
    "total_freq = 0\n",
    "\n",
    "rare_freq = 0\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    if(value < threshold):\n",
    "\n",
    "        rare_cnt = rare_cnt + 1\n",
    "\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print(total_cnt) #단어집합 크기\n",
    "\n",
    "print(rare_cnt) #희귀단어수\n",
    "\n",
    "###############################################\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "\n",
    "print('단어 집합의 크기 :',vocab_size)\n",
    "\n",
    "###############################################\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train) #텍스트를 숫자 시퀀스로 변환\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "###############################################\n",
    "\n",
    "import joblib  \n",
    "\n",
    "joblib.dump(tokenizer, 'tokenizer.h5')\n",
    "\n",
    "###############################################\n",
    "\n",
    "print(X_train[:3])\n",
    "\n",
    "###############################################\n",
    "\n",
    "y_train = np.array(train_data['label'][:10000])\n",
    "\n",
    "y_test = np.array(test_data['label'][:10000])\n",
    "\n",
    "###############################################\n",
    "\n",
    "max_len=max(len(l) for l in X_train) #리뷰의 최대 길이\n",
    "\n",
    "max_len\n",
    "\n",
    "###############################################\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for s in X_train:\n",
    "\n",
    "  if(len(s) <= 30):\n",
    "\n",
    "      cnt = cnt + 1\n",
    "\n",
    "#최대 길이 이하인 샘플의 비율\n",
    "\n",
    "(cnt / len(X_train))*100\n",
    "\n",
    "###############################################\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "###############################################\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "###############################################\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
    "\n",
    "mc = ModelCheckpoint('RNN_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size = 64, epochs=10, validation_split=0.2, callbacks=[es, mc])\n",
    "\n",
    "###############################################\n",
    "\n",
    "loaded_model = load_model('RNN_model.h5')\n",
    "\n",
    "loaded_model.evaluate(X_test, y_test)\n",
    "\n",
    "###############################################\n",
    "\n",
    "def review_predict(new_sentence):\n",
    "\n",
    "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "\n",
    "  score = float(model.predict(pad_new)) # 예측\n",
    "\n",
    "    if(score > 0.5):\n",
    "\n",
    "    print(f\"{score * 100:.2f}% 확률로 긍정 리뷰입니다.\\n\")\n",
    "\n",
    "  else:\n",
    "\n",
    "    print(f\"{(1 - score) * 100:.2f}% 확률로 부정 리뷰입니다.\\n\")\n",
    "\n",
    "###############################################\n",
    "\n",
    "review_predict('연기는 잔잔하게 볼 만 합니다')\n",
    "\n",
    "###############################################\n",
    "\n",
    "review_predict('영화의 주제가 뭔지 모르겠음')\n",
    "\n",
    "###############################################\n",
    "\n",
    "review_predict('익살스런 연기가 돋보였던 영화')\n",
    "\n",
    "###############################################\n",
    "\n",
    "review_predict('기대보다는 스토리가 큰 감흥은 없습니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 문장만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow-gpu==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "\n",
    "    try:\n",
    "\n",
    "        for gpu in gpus:\n",
    "\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('./DATA/ratings_all.csv',encoding='ms949')\n",
    "\n",
    "df=df[df['label'] == 1] # 긍정리뷰\n",
    "\n",
    "df=df.sample(frac=1) # 셔플 100%\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='\\n'.join(map(str,df['document'].values[10000:13000]))\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정수 인코딩, 단어에 번호\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t=Tokenizer()\n",
    "\n",
    "t.fit_on_texts([text])\n",
    "\n",
    "vocab_size=len(t.word_index)+1\n",
    "\n",
    "print(vocab_size) #단어 집합 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=list()\n",
    "\n",
    "for line in text.split('\\n'): #문장 나누기\n",
    "\n",
    "    encoded=t.texts_to_sequences([line])[0] #단어를 숫자로 변환\n",
    "\n",
    "    for i in range(1,len(encoded)):\n",
    "\n",
    "        sequence=encoded[:i+1]\n",
    "\n",
    "        sequences.append(sequence)\n",
    "\n",
    "print(len(sequences)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max(len(l) for l in sequences)\n",
    "\n",
    "print(max_len) #문장의 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 앞에 패딩을 한 이유: \n",
    "sequences=pad_sequences(sequences,maxlen=max_len,padding='pre')\n",
    "\n",
    "sequences #제로패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequences=np.array(sequences)\n",
    "\n",
    "X=sequences[:, :-1]\n",
    "\n",
    "y=sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "#원핫인코딩\n",
    "y=np_utils.to_categorical(y,num_classes=vocab_size) #vocab_size 단어수\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense,Embedding,LSTM\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,500,input_length=max_len-1)) #단어수,벡터사이즈,입력사이즈\n",
    "\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(vocab_size,activation='softmax'))\n",
    "\n",
    "adam = Adam(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리 부족으로 학습이 실패함\n",
    "\n",
    "#model.fit(X,y,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import glob\n",
    "\n",
    "[os.remove(f) for f in glob.glob('*.h5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "x_size=len(X)\n",
    "\n",
    "n=800\n",
    "\n",
    "for i in range(0,x_size,n):\n",
    "\n",
    "    print('step ',int(i/n)+1,'/',int(x_size/n))\n",
    "\n",
    "    X1=X[i:min(i + n, x_size)]\n",
    "\n",
    "    y1=y[i:min(i + n, x_size)]    \n",
    "\n",
    "    model.fit(X1,y1,batch_size=64,epochs=30, verbose=1)\n",
    "\n",
    "    model.save(f'sentence{int(i/n)}.h5')    \n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    del model    \n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    model = load_model(f'sentence{int(i/n)}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_generation(model,t,current_word,n):\n",
    "    init_word=current_word\n",
    "\n",
    "    sentence=''\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        encoded=t.texts_to_sequences([current_word])[0]\n",
    "\n",
    "        encoded=pad_sequences([encoded],maxlen=max_len-1,padding='pre')\n",
    "\n",
    "        pred=model.predict(encoded,verbose=0)\n",
    "\n",
    "        result=np.argmax(pred,axis=1)\n",
    "\n",
    "        for word,index in t.word_index.items():\n",
    "\n",
    "            if index==result:\n",
    "\n",
    "                break\n",
    "\n",
    "        current_word=current_word+' '+word\n",
    "\n",
    "        sentence=sentence +' '+word\n",
    "\n",
    "        \n",
    "\n",
    "    sentence=init_word+sentence\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_generation(model,t,'너무',5))\n",
    "\n",
    "print(sequence_generation(model,t,'진짜',5))\n",
    "\n",
    "print(sequence_generation(model,t,'역시',5))\n",
    "\n",
    "print(sequence_generation(model,t,'완전',5))\n",
    "\n",
    "print(sequence_generation(model,t,'왜케',5))\n",
    "\n",
    "print(sequence_generation(model,t,'오랜만에',5))\n",
    "\n",
    "print(sequence_generation(model,t,'큰',5))\n",
    "\n",
    "print(sequence_generation(model,t,'낭만적',5))\n",
    "\n",
    "print(sequence_generation(model,t,'정말',5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 텍스트마이닝(CNN)\n",
    "1) 기본예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합성곱\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "w = np.array([2, 1, 5, 3])\n",
    "x = np.array([2, 8, 3, 7, 1, 2, 0, 4, 5])\n",
    "\n",
    "#w 배열을 뒤집어서 출력\n",
    "w_r = np.flip(w)\n",
    "print(w_r)\n",
    "\n",
    "#합성곱 계산\n",
    "# w_r을 x의 왼쪽 자리에 맞추고 각 인덱스마다 곱한 후 더함\n",
    "# 2x3 + 8x5 + 3x1 + 7x2 = 63\n",
    "# w_r을 오른쪽으로 한자리 shift하여 곱셈\n",
    "\n",
    "for i in range(6):\n",
    "    print(np.dot(x[i:i+4], w_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사이파이에서 제공하는 합성곱 함수\n",
    "#w를 뒤집어서 곱하는 방식\n",
    "\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# valid - 원본 배열에 패딩을 추가하지 않는 방식\n",
    "# 원본 이미지가 4x4인 경우 결과물이 3x3으로 줄어드는 방식\n",
    "convolve(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합성곱 신경망에서는 w를 뒤집지 않고 그대로 곱하는 교차상관 방식을 사용함\n",
    "#초기 가중치값은 랜덤으로 만들어지므로 뒤집어서 곱하는 것과 뒤집지 않고 곱하는 것이 큰 의미가 없음\n",
    "#정확히 표현하면 교차상관이지만 합성곱 신경망이라는 이름을 관례적으로 사용하고 있음\n",
    "\n",
    "#교차상관 - w를 뒤집지 않고 곱하는 방식 --> 신경망의 convolution\n",
    "\n",
    "from scipy.signal import correlate\n",
    "\n",
    "correlate(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full 패딩 - 제로패딩을 한 후 연산을 하게 되면 원본 배열의 모든 원소가 연산에 동일하게 참여하게 됨\n",
    "correlate(x, w, mode='full')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#출력 배열의 길이가 원본 배열의 길이와 같아지도록 제로 패딩을 추가하는 방식\n",
    "\n",
    "#합성곱 신경망에서 많이 사용하는 방식\n",
    "\n",
    "correlate(x, w, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2차원 배열에 대한 합성곱 계산\n",
    "\n",
    "from scipy.signal import correlate2d\n",
    "\n",
    "x = np.array([[1, 2, 3],\n",
    "\n",
    "              [4, 5, 6],\n",
    "\n",
    "              [7, 8, 9]])\n",
    "\n",
    "w = np.array([[2, 0],\n",
    "\n",
    "              [0, 0]])\n",
    "\n",
    "correlate2d(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제로패딩을 하여 원본과 같은 사이즈로 출력되도록 함\n",
    "\n",
    "x = np.array([[1, 2, 3],\n",
    "\n",
    "              [4, 5, 6],\n",
    "\n",
    "              [7, 8, 9]])\n",
    "\n",
    "w = np.array([[2, 0],\n",
    "\n",
    "              [0, 0]])              \n",
    "\n",
    "correlate2d(x, w, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서플로에서 지원하는 합성곱 함수\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#4차원 배열을 사용해야 함\n",
    "\n",
    "x = np.array([[1, 2, 3],\n",
    "\n",
    "              [4, 5, 6],\n",
    "\n",
    "              [7, 8, 9]])\n",
    "\n",
    "# 채널: 데이터의 r,g,b \n",
    "# 가중치의 개수: 필터의 개수\n",
    "# 채널별로 필터의 개수\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    # 입력값: reshape(batch, height, width, channel)              \n",
    "    x_4d = x.astype(np.float32).reshape(1, 3, 3, 1) #실수형으로 입력해야 함\n",
    "\n",
    "    # 필터(가중치) reshape(height,width,channel,가중치의개수)\n",
    "    w_4d = w.reshape(2, 2, 1, 1)\n",
    "\n",
    "    #SAME 대문자로 작성해야 함\n",
    "    c_out = tf.nn.conv2d(x_4d, w_4d, strides=1, padding='SAME')\n",
    "\n",
    "    #  텐서를 넘파이 배열로 변환\n",
    "    print(c_out.numpy().reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#맥스풀링\n",
    "\n",
    "# 입력값: reshape(샘플수, height, width, channel)              \n",
    "\n",
    "x = np.array([[1, 2, 3, 4],\n",
    "\n",
    "              [5, 6, 7, 8],\n",
    "\n",
    "              [9, 10, 11, 12],\n",
    "              [13, 14, 15, 16]])\n",
    "\n",
    "x = x.reshape(1, 4, 4, 1)\n",
    "\n",
    "# ksize 커널사이즈 2x2, strides 이동간격\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    p_out=tf.nn.max_pool2d(x,ksize=2,strides=2,padding='SAME')\n",
    "\n",
    "    print(p_out.numpy().reshape(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 한글텍스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')  \n",
    "\n",
    "print(gpus)  \n",
    "\n",
    "if gpus:  \n",
    "\n",
    "    try:\n",
    "\n",
    "        for gpu in gpus:\n",
    "\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('./DATA/ratings_train.csv',encoding='ms949')\n",
    "\n",
    "test_data = pd.read_csv('./DATA/ratings_test.csv',encoding='ms949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 document  label\n",
       "0                                     굳 ㅋ      1\n",
       "1                    GDNTOPCLASSINTHECLUB      0\n",
       "2  뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146157, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복되지 않은 데이터 확인\n",
    "\n",
    "train_data['document'].nunique(), train_data['label'].nunique()\n",
    "\n",
    "# 중복값이 약 4000개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#중복값 제거\n",
    "\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data['label'].value_counts().plot(kind = 'bar')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25857</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      document  label\n",
       "25857      NaN      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data.document.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "\n",
    "train_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            document  label\n",
       "0                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2                  너무재밓었다그래서보는것을추천한다      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#특수문자,기호 제거\n",
    "\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#공백 제거\n",
    "\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\")\n",
    "\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "\n",
    "train_data.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [document, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data.document.isnull()][:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146157"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#null 샘플 제거\n",
    "\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49147"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_data에도 동일한 과정 적용\n",
    "\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True)\n",
    "\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\")\n",
    "\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "\n",
    "test_data = test_data.dropna(how='any')\n",
    "\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "X_train = []  \n",
    "\n",
    "#형태소 분석\n",
    "\n",
    "#for sentence in train_data['document']:\n",
    "\n",
    "for sentence in train_data['document'][:10000]:    \n",
    "\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['아', '더빙', '..', '진짜', '짜증나다', '목소리'], ['흠', '...', '포스터', '보고', '초딩', '영화', '줄', '....', '오버', '연기', '조차', '가볍다', '않다'], ['너', '무재', '밓었', '다그', '래서', '보다', '추천', '다']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 토큰화\n",
    "\n",
    "X_test = []\n",
    "\n",
    "#for sentence in test_data['document']:\n",
    "\n",
    "for sentence in test_data['document'][:10000]:    \n",
    "\n",
    "    temp_X = okt.morphs(sentence, stem=True)\n",
    "\n",
    "    temp_X = [word for word in temp_X if not word in stopwords]\n",
    "\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 정수 인코딩\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13345\n",
      "8891\n"
     ]
    }
   ],
   "source": [
    "# 출현빈도가 3회 미만인 단어들\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "total_cnt = len(tokenizer.word_index) # 단어수\n",
    "\n",
    "rare_cnt = 0\n",
    "\n",
    "total_freq = 0\n",
    "\n",
    "rare_freq = 0\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    if(value < threshold):\n",
    "\n",
    "        rare_cnt = rare_cnt + 1\n",
    "\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print(total_cnt) #단어집합 크기\n",
    "\n",
    "print(rare_cnt) #희귀단어수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 4455\n"
     ]
    }
   ],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size)\n",
    "\n",
    "tokenizer.fit_on_texts(X_train) #텍스트를 숫자 시퀀스로 변환\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59, 724, 4, 21, 259, 831], [725, 6, 378, 48, 678, 2, 178, 41, 1928, 29, 942, 769, 26], [373, 1613, 3462, 3, 249, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'][:10000])\n",
    "\n",
    "y_test = np.array(test_data['label'][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=max(len(l) for l in X_train) #리뷰의 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.58"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "\n",
    "for s in X_train:\n",
    "\n",
    "  if(len(s) <= 30):\n",
    "\n",
    "      cnt = cnt + 1\n",
    "\n",
    "#최대 길이 이하인 샘플의 비율\n",
    "\n",
    "(cnt / len(X_train))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Flatten, MaxPooling1D\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 100, input_length = max_len))\n",
    "\n",
    "model.add(Conv1D(filters = 64, kernel_size = 5, padding = 'same',activation = 'relu', strides = 1))\n",
    "\n",
    "model.add(Conv1D(filters = 32, kernel_size = 4, padding = 'same',activation = 'relu', strides = 1))\n",
    "\n",
    "model.add(Conv1D(filters = 16, kernel_size = 3, padding = 'same',activation = 'relu', strides = 1))\n",
    "\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
    "\n",
    "mc = ModelCheckpoint('CNN_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size = 64, epochs=10, validation_split=0.2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('CNN_model.h5')\n",
    "\n",
    "loaded_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_predict(new_sentence):\n",
    "\n",
    "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "\n",
    "  score = float(model.predict(pad_new)) # 예측\n",
    "\n",
    "  if(score > 0.5):\n",
    "\n",
    "    print(f\"{score * 100:.2f}% 확률로 긍정 리뷰입니다.\\n\")\n",
    "\n",
    "  else:\n",
    "\n",
    "    print(f\"{(1 - score) * 100:.2f}% 확률로 부정 리뷰입니다.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_predict('연기는 잔잔하게 볼 만 합니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_predict('영화의 주제가 뭔지 모르겠음')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_predict('익살스런 연기가 돋보였던 영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_predict('기대보다는 스토리가 큰 감흥은 없습니다')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
